"""
Shared Bench File Parser for DFT Analysis

This module provides a unified parser for .bench files that:
- Handles full-scan DFFs (Q output as PPI, D input as PPO)
- Tracks back edges for bidirectional graph traversal
- Provides a common data structure for both SAT and GNN models
"""

class BenchParser:
    """
    Unified parser for .bench format files with full-scan DFF support.
    
    For full-scan designs:
    - DFF outputs (Q) are treated as Pseudo Primary Inputs (PPIs)
    - DFF inputs (D) are treated as Pseudo Primary Outputs (PPOs)
    - The circuit is "broken" at flip-flops to eliminate cycles
    """
    
    def __init__(self, bench_file):
        self.bench_file = bench_file
        
        # Primary Inputs/Outputs
        self.inputs = []           # Primary Inputs (PIs)
        self.outputs = []          # Primary Outputs (POs)
        
        # Pseudo Inputs/Outputs (from DFFs)
        self.ppis = []             # Pseudo Primary Inputs (DFF Q outputs)
        self.ppos = []             # Pseudo Primary Outputs (DFF D inputs)
        
        # All inputs/outputs combined
        self.all_inputs = []       # PIs + PPIs
        self.all_outputs = []      # POs + PPOs
        
        # Gate structure
        self.gates = []            # List of (output, gate_type, inputs)
        self.gate_dict = {}        # Map: output_name -> (gate_type, inputs)
        
        # DFF tracking
        self.dffs = []             # List of (Q_output, D_input) tuples
        self.dff_map = {}          # Map: Q_output -> D_input
        
        # Back edges (for reverse traversal)
        self.back_edges = {}       # Map: input_wire -> [gates_it_drives]
        
        # Variable mapping (for SAT solver)
        self.var_map = {}          # Map: wire_name -> variable_id
        
        # Parse the file
        self._parse()
    
    def _parse(self):
        """Parse the bench file and populate all data structures."""
        with open(self.bench_file, 'r') as f:
            for line in f:
                line = line.strip()
                
                # Skip empty lines and comments
                if not line or line.startswith('#'):
                    continue
                
                # Parse INPUT declarations
                if line.startswith('INPUT'):
                    name = line[line.find('(')+1:line.find(')')]
                    self.inputs.append(name)
                    self.all_inputs.append(name)
                    
                # Parse OUTPUT declarations
                elif line.startswith('OUTPUT'):
                    name = line[line.find('(')+1:line.find(')')]
                    self.outputs.append(name)
                    self.all_outputs.append(name)
                    
                # Parse gate definitions
                elif '=' in line:
                    parts = line.split('=')
                    out = parts[0].strip()
                    rhs = parts[1].strip()
                    
                    # Extract gate type and inputs
                    g_type = rhs[:rhs.find('(')].strip().upper()
                    in_str = rhs[rhs.find('(')+1:-1]
                    inputs = [x.strip() for x in in_str.split(',')] if in_str else []
                    
                    # Handle DFFs specially (Full-Scan assumption)
                    if g_type == 'DFF':
                        # Q output (out) becomes a PPI
                        self.ppis.append(out)
                        self.all_inputs.append(out)
                        
                        # D input becomes a PPO
                        if len(inputs) > 0:
                            d_input = inputs[0]
                            self.ppos.append(d_input)
                            self.all_outputs.append(d_input)
                            
                            # Track the DFF relationship
                            self.dffs.append((out, d_input))
                            self.dff_map[out] = d_input
                        
                        # Note: DFFs are NOT added to self.gates
                        # This "breaks" the circuit at flip-flops
                    else:
                        # Regular combinational gate
                        self.gates.append((out, g_type, inputs))
                        self.gate_dict[out] = (g_type, inputs)
                        
                        # Build back edges for reverse traversal
                        for inp in inputs:
                            if inp not in self.back_edges:
                                self.back_edges[inp] = []
                            self.back_edges[inp].append(out)
        
        # Remove duplicates while preserving order
        self.all_inputs = list(dict.fromkeys(self.all_inputs))
        self.all_outputs = list(dict.fromkeys(self.all_outputs))
    
    def get_all_wires(self):
        """Get all wire names in the circuit (inputs, outputs, and internal)."""
        wires = set(self.all_inputs + self.all_outputs)
        for out, _, inputs in self.gates:
            wires.add(out)
            wires.update(inputs)
        return sorted(list(wires))
    
    def build_var_map(self):
        """Build variable mapping for SAT solver (1-indexed)."""
        if self.var_map:
            return self.var_map  # Already built
        
        next_var = 1
        for wire in self.get_all_wires():
            if wire not in self.var_map:
                self.var_map[wire] = next_var
                next_var += 1
        return self.var_map
    
    def get_fanout(self, wire_name):
        """Get all gates driven by a wire (forward edges)."""
        return self.back_edges.get(wire_name, [])
    
    def get_fanin(self, wire_name):
        """Get the gate driving a wire (backward edge)."""
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][1]  # Return inputs
        return []
    
    def is_pi(self, wire_name):
        """Check if wire is a Primary Input."""
        return wire_name in self.inputs
    
    def is_po(self, wire_name):
        """Check if wire is a Primary Output."""
        return wire_name in self.outputs
    
    def is_ppi(self, wire_name):
        """Check if wire is a Pseudo Primary Input (DFF Q)."""
        return wire_name in self.ppis
    
    def is_ppo(self, wire_name):
        """Check if wire is a Pseudo Primary Output (DFF D)."""
        return wire_name in self.ppos
    
    def is_dff_output(self, wire_name):
        """Check if wire is a DFF Q output."""
        return wire_name in self.dff_map
    
    def get_dff_input(self, q_output):
        """Get the D input for a DFF Q output."""
        return self.dff_map.get(q_output)
    
    def get_gate_type(self, wire_name):
        """Get the gate type that produces this wire."""
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][0]
        elif self.is_ppi(wire_name):
            return 'PPI'
        elif self.is_pi(wire_name):
            return 'INPUT'
        return None
    
    def __repr__(self):
        return (f"BenchParser({self.bench_file})\n"
                f"  PIs: {len(self.inputs)}, POs: {len(self.outputs)}\n"
                f"  PPIs: {len(self.ppis)}, PPOs: {len(self.ppos)}\n"
                f"  Gates: {len(self.gates)}, DFFs: {len(self.dffs)}")"""
Unified Circuit Parser

Auto-detects file format (.bench or .v) and uses appropriate parser.
Provides a single interface for both formats.
"""

import os
from BenchParser import BenchParser
from VerilogParser import VerilogParser

class UnifiedParser:
    """
    Wrapper that auto-detects and delegates to BenchParser or VerilogParser.
    Provides identical API regardless of input format.
    """
    
    def __init__(self, circuit_file):
        self.circuit_file = circuit_file
        
        # Detect format and create appropriate parser
        if circuit_file.endswith('.bench'):
            self.parser = BenchParser(circuit_file)
            self.format = 'bench'
        elif circuit_file.endswith('.v') or circuit_file.endswith('.verilog'):
            self.parser = VerilogParser(circuit_file)
            self.format = 'verilog'
        else:
            # Try to detect by content
            self.format = self._detect_format_by_content(circuit_file)
            if self.format == 'bench':
                self.parser = BenchParser(circuit_file)
            elif self.format == 'verilog':
                self.parser = VerilogParser(circuit_file)
            else:
                raise ValueError(f"Unknown circuit format: {circuit_file}")
    
    def _detect_format_by_content(self, filepath):
        """Detect format by examining file content."""
        with open(filepath, 'r') as f:
            content = f.read(1000)  # Read first 1KB
        
        # Check for Verilog keywords
        if 'module' in content and 'endmodule' in content:
            return 'verilog'
        
        # Check for BENCH keywords
        if 'INPUT(' in content or 'OUTPUT(' in content:
            return 'bench'
        
        return None
    
    # Delegate all methods to underlying parser
    def __getattr__(self, name):
        return getattr(self.parser, name)
    
    def __repr__(self):
        return f"UnifiedParser({self.format}): {self.parser}"


# =============================================================================
# Usage Example: Works with BOTH formats transparently
# =============================================================================

if __name__ == "__main__":
    # Works with .bench files
    bench_parser = UnifiedParser("c17.bench")
    print(bench_parser)
    print("Gates:", len(bench_parser.gates))
    
    # Works with .v files  
    verilog_parser = UnifiedParser("circuit.v")
    print(verilog_parser)
    print("Gates:", len(verilog_parser.gates))
    
    # Same API for both!
    for parser in [bench_parser, verilog_parser]:
        print(f"\nInputs: {parser.inputs}")
        print(f"Outputs: {parser.outputs}")
        print(f"Gate count: {len(parser.gates)}")
        var_map = parser.build_var_map()
        print(f"Variables: {len(var_map)}")r"""
VerilogParser - Enhanced for Yosys Output (FIXED: Consistent Wire Ordering)

CRITICAL FIX:
- get_all_wires() now matches BenchParser behavior exactly
- Only includes wires that are actually used in gates
- Ensures consistent node ordering between .bench and .v formats

Handles:
- Yosys header comments
- Escaped identifiers (\a[0], \b[1], etc.)
- Yosys internal gates (\$_AND_, \$_OR_, etc.)
- Both named and positional port connections
"""

import re

class VerilogParser:
    """Parser for gate-level Verilog with BenchParser-compatible API."""
    
    GATE_MAPPINGS = {
        # Standard gates (lowercase)
        'and': 'AND', 'or': 'OR', 'not': 'NOT', 'nand': 'NAND',
        'nor': 'NOR', 'xor': 'XOR', 'xnor': 'XNOR', 'buf': 'BUFF',
        'buffer': 'BUFF', 'dff': 'DFF', 'DFF': 'DFF',
        
        # Yosys internal gates (with $_..._  format)
        '$_and_': 'AND', '$_or_': 'OR', '$_not_': 'NOT', '$_nand_': 'NAND',
        '$_nor_': 'NOR', '$_xor_': 'XOR', '$_xnor_': 'XNOR', '$_buf_': 'BUFF',
        '$_dff_': 'DFF', '$_dffe_': 'DFF',
        '$_mux_': 'MUX', '$_nmux_': 'NMUX',
        '$_aoi3_': 'AOI3', '$_oai3_': 'OAI3',
        '$_aoi4_': 'AOI4', '$_oai4_': 'OAI4',
    }
    
    def __init__(self, verilog_file):
        self.verilog_file = verilog_file
        
        # Data structures
        self.inputs = []
        self.outputs = []
        self.ppis = []
        self.ppos = []
        self.all_inputs = []
        self.all_outputs = []
        self.gates = []
        self.gate_dict = {}
        self.dffs = []
        self.dff_map = {}
        self.back_edges = {}
        self.var_map = {}
        self.wires = []
        
        self._parse()
    
    def _remove_comments(self, content):
        """Remove ALL comments including Yosys headers."""
        # Remove multi-line comments
        content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
        # Remove single-line comments
        content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)
        return content
    
    def _normalize_identifier(self, name):
        """
        Normalize Verilog identifiers.
        Removes escaped identifier backslash and trailing whitespace.
        
        Examples:
            \\a[0]  -> a[0]
            \\b[1]  -> b[1]
            normal  -> normal
        """
        name = name.strip()
        # Remove leading backslash for escaped identifiers
        if name.startswith('\\'):
            name = name[1:].strip()
        return name
    
    def _parse(self):
        """Parse Verilog file."""
        with open(self.verilog_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Remove comments FIRST
        content = self._remove_comments(content)
        
        # Extract modules
        modules = self._extract_modules(content)
        
        if not modules:
            raise ValueError("No module found in Verilog file")
        
        # Process first module
        module_content = modules[0]
        
        self._parse_ports(module_content)
        self._parse_wires(module_content)
        self._parse_instances(module_content)
        
        # Build combined lists
        self.all_inputs = list(dict.fromkeys(self.inputs + self.ppis))
        self.all_outputs = list(dict.fromkeys(self.outputs + self.ppos))
    
    def _extract_modules(self, content):
        """Extract module definitions."""
        # Handle both escaped and normal identifiers in module name and ports
        pattern = r'module\s+(\S+)\s*\((.*?)\);(.*?)endmodule'
        matches = re.findall(pattern, content, re.DOTALL)
        return [match[2] for match in matches]  # Return module bodies
    
    def _parse_ports(self, content):
        """Parse input/output declarations."""
        # Updated patterns to handle escaped identifiers
        input_pattern = r'input\s+(?:\[.*?\]\s+)?([^;]+);'
        output_pattern = r'output\s+(?:\[.*?\]\s+)?([^;]+);'
        
        for match in re.finditer(input_pattern, content):
            ports = match.group(1).split(',')
            for port in ports:
                name = self._normalize_identifier(port)
                name = re.sub(r'\[.*?\]', '', name).strip()
                if name and name not in self.inputs:
                    self.inputs.append(name)
        
        for match in re.finditer(output_pattern, content):
            ports = match.group(1).split(',')
            for port in ports:
                name = self._normalize_identifier(port)
                name = re.sub(r'\[.*?\]', '', name).strip()
                if name and name not in self.outputs:
                    self.outputs.append(name)
    
    def _parse_wires(self, content):
        """Parse wire declarations."""
        wire_pattern = r'wire\s+(?:\[.*?\]\s+)?([^;]+);'
        
        for match in re.finditer(wire_pattern, content):
            wires = match.group(1).split(',')
            for wire in wires:
                name = self._normalize_identifier(wire)
                name = re.sub(r'\[.*?\]', '', name).strip()
                if name and name not in self.wires:
                    self.wires.append(name)
    
    def _parse_instances(self, content):
        """Parse gate instances (handles both ICCAD positional and Yosys named formats)."""
        # Pattern to match gate instances:
        # - ICCAD: gate_type ( ports );
        # - Yosys: \gate_type instance_name ( ports );
        # Matches escaped identifiers starting with \ or regular identifiers
        
        # Identifier can be:
        # - Escaped: \$_NAND_ or \a[0] (backslash followed by non-whitespace)
        # - Regular: and, or, _inst123, etc. (word characters)
        instance_pattern = r'(\\[^\s]+|\w+)\s+(?:(\\[^\s]+|\w+)\s+)?\(\s*(.*?)\s*\)\s*;'
        
        for match in re.finditer(instance_pattern, content, re.DOTALL):
            gate_type_raw = match.group(1)
            inst_name = match.group(2) if match.group(2) else 'unnamed'
            port_list = match.group(3)
            
            # Normalize gate type (remove backslash, convert to lowercase)
            gate_type_normalized = self._normalize_identifier(gate_type_raw).lower()
            
            if gate_type_normalized not in self.GATE_MAPPINGS:
                continue
            
            gate_type = self.GATE_MAPPINGS[gate_type_normalized]
            ports = self._parse_port_connections(port_list)
            
            if not ports or len(ports) < 1:
                continue
            
            output = ports[0]
            inputs = ports[1:] if len(ports) > 1 else []
            
            if gate_type == 'DFF':
                self.ppis.append(output)
                if inputs:
                    self.ppos.append(inputs[0])
                    self.dffs.append((output, inputs[0]))
                    self.dff_map[output] = inputs[0]
            else:
                self.gates.append((output, gate_type, inputs))
                self.gate_dict[output] = (gate_type, inputs)
                
                for inp in inputs:
                    if inp not in self.back_edges:
                        self.back_edges[inp] = []
                    self.back_edges[inp].append(output)
    
    def _parse_port_connections(self, port_list):
        """Parse port connections (positional or named)."""
        ports = []
        port_list = port_list.strip()
        
        if '.(' in port_list or ('.' in port_list and '(' in port_list):
            # Named connections - handle escaped identifiers
            # Pattern: .PORT_NAME(\wire_name) or .PORT_NAME(wire_name)
            named_pattern = r'\.(\w+)\s*\(\s*([^)]+)\s*\)'
            connections = {}
            
            for match in re.finditer(named_pattern, port_list):
                port_name = match.group(1)
                wire_name = self._normalize_identifier(match.group(2))
                connections[port_name] = wire_name
            
            # Extract output first
            output_names = ['Y', 'Q', 'OUT', 'Z', 'O']
            for name in output_names:
                if name in connections:
                    ports.append(connections[name])
                    break
            
            # Extract inputs
            skip_ports = ['CLK', 'CLOCK', 'RST', 'RESET', 'SET', 'CLEAR', 'EN', 'ENABLE']
            for port_name, wire_name in connections.items():
                if port_name not in output_names and port_name not in skip_ports:
                    if wire_name not in ports:
                        ports.append(wire_name)
        else:
            # Positional connections
            wires = [self._normalize_identifier(w) for w in port_list.split(',')]
            ports = [w for w in wires if w and w != '1\'b0' and w != '1\'b1']
        
        return ports
    
    # =========================================================================
    # BenchParser-Compatible API
    # =========================================================================
    
    def get_all_wires(self):
        """
        CRITICAL FIX: Match BenchParser behavior exactly!
        
        Only include wires that are ACTUALLY USED in the circuit:
        - Primary inputs/outputs (all_inputs, all_outputs)
        - Wires referenced in gates (outputs and inputs)
        
        DO NOT include declared-but-unused wires from self.wires!
        This ensures consistent ordering with BenchParser.
        """
        # Start with inputs and outputs (same as BenchParser)
        wires = set(self.all_inputs + self.all_outputs)
        
        # Add wires used in gates (same as BenchParser)
        for out, _, inputs in self.gates:
            wires.add(out)
            wires.update(inputs)
        
        # DO NOT add self.wires here - this was the bug!
        # self.wires may contain declared-but-unused wires
        
        return sorted(list(wires))
    
    def build_var_map(self):
        if self.var_map:
            return self.var_map
        next_var = 1
        for wire in self.get_all_wires():
            if wire not in self.var_map:
                self.var_map[wire] = next_var
                next_var += 1
        return self.var_map
    
    def get_fanout(self, wire_name):
        return self.back_edges.get(wire_name, [])
    
    def get_fanin(self, wire_name):
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][1]
        return []
    
    def is_pi(self, wire_name):
        return wire_name in self.inputs
    
    def is_po(self, wire_name):
        return wire_name in self.outputs
    
    def is_ppi(self, wire_name):
        return wire_name in self.ppis
    
    def is_ppo(self, wire_name):
        return wire_name in self.ppos
    
    def is_dff_output(self, wire_name):
        return wire_name in self.dff_map
    
    def get_dff_input(self, q_output):
        return self.dff_map.get(q_output)
    
    def get_gate_type(self, wire_name):
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][0]
        elif self.is_ppi(wire_name):
            return 'PPI'
        elif self.is_pi(wire_name):
            return 'INPUT'
        return None
    
    def __repr__(self):
        return (f"VerilogParser({self.verilog_file})\n"
                f"  PIs: {len(self.inputs)}, POs: {len(self.outputs)}\n"
                f"  PPIs: {len(self.ppis)}, PPOs: {len(self.ppos)}\n"
                f"  Gates: {len(self.gates)}, DFFs: {len(self.dffs)}")"""
WireFaultMiter with Complete ATPG Cone Extraction

Includes proper handling of:
1. Fan-in (activation)
2. Fan-out (propagation path)  
3. Side inputs (other logic affecting propagation)
"""

import os
import random
from collections import deque
from pysat.solvers import Glucose3, Minisat22

try:
    from UnifiedParser import UnifiedParser as Parser
except ImportError:
    from BenchParser import BenchParser as Parser

class WireFaultMiter:
    def __init__(self, circuit_file):
        """Initialize miter for fault detection."""
        self.circuit_file = circuit_file
        self.parser = Parser(circuit_file)
        
        self.inputs = self.parser.all_inputs
        self.outputs = self.parser.all_outputs
        self.gates = self.parser.gates
        self.var_map = self.parser.build_var_map()
        self.next_var = len(self.var_map) + 1
        self.faulty_map = {}
        self.scan_inputs = self.parser.ppis
        self.scan_outputs = self.parser.ppos

    def _get_var(self, name):
        if name not in self.var_map:
            self.var_map[name] = self.next_var
            self.next_var += 1
        return self.var_map[name]

    def get_reachable_outputs(self, gate_name):
        """Get all primary outputs reachable from a gate."""
        reachable = set()
        visited = set()
        queue = deque([gate_name])
        
        while queue:
            node = queue.popleft()
            if node in visited:
                continue
            visited.add(node)
            
            if node in self.outputs:
                reachable.add(node)
            
            fanout = self.parser.get_fanout(node)
            for next_gate in fanout:
                if next_gate not in visited:
                    queue.append(next_gate)
        
        return list(reachable)

    def get_complete_atpg_cone(self, gate_name, target_output):
        """
        Extract the COMPLETE ATPG cone including side inputs.
        
        This includes:
        1. Activation cone: inputs → fault (fan-in)
        2. Propagation cone: fault → output (fan-out path)
        3. Side input cones: other logic affecting propagation gates ⭐
        
        Args:
            gate_name: Fault site
            target_output: Target primary output (used to limit scope)
            
        Returns:
            Complete ATPG cone as list of (output, gate_type, inputs) tuples
        """
        # Step 1: Trace the propagation path (fault → target output)
        propagation_path = set()
        visited_prop = set()
        
        def trace_propagation(node, target):
            """Find all gates on path from fault to specific output"""
            if node in visited_prop:
                return
            visited_prop.add(node)
            
            # Stop if we reached the target
            if node == target:
                return
            
            # Add to path if it's a gate (not the fault itself yet)
            if node != gate_name and node in self.parser.gate_dict:
                propagation_path.add(node)
            
            # Continue tracing only if we haven't reached target
            if node != target:
                fanout = self.parser.get_fanout(node)
                for next_gate in fanout:
                    if next_gate not in visited_prop:
                        trace_propagation(next_gate, target)
        
        trace_propagation(gate_name, target_output)
        
        # Step 2: For EACH propagation gate, get its COMPLETE fan-in
        # This captures side inputs! ⭐
        side_logic = set()
        propagation_cone = []
        
        for prop_gate in propagation_path:
            # Add the propagation gate itself
            if prop_gate in self.parser.gate_dict:
                g_type, inputs = self.parser.gate_dict[prop_gate]
                propagation_cone.append((prop_gate, g_type, inputs))
                
                # Get ALL logic feeding this propagation gate
                # (excluding the fault gate to avoid double-counting)
                gate_fanin = self._get_fanin_recursive(prop_gate, stop_at=gate_name)
                for fanin_gate, _, _ in gate_fanin:
                    side_logic.add(fanin_gate)
        
        # Step 3: Get fault activation cone (fan-in to fault)
        activation_cone = self._get_fanin_recursive(gate_name, stop_at=None)
        
        # Step 4: Build side input cone (gates in side_logic but not activation)
        side_cone = []
        activation_gates = set([g[0] for g in activation_cone])
        
        for gate in side_logic:
            if gate not in activation_gates and gate in self.parser.gate_dict:
                g_type, inputs = self.parser.gate_dict[gate]
                side_cone.append((gate, g_type, inputs))
        
        # Step 5: Add the fault gate itself
        fault_gate = []
        if gate_name in self.parser.gate_dict:
            g_type, inputs = self.parser.gate_dict[gate_name]
            fault_gate = [(gate_name, g_type, inputs)]
        
        # Step 6: Combine all components and deduplicate
        all_gates = activation_cone + fault_gate + side_cone + propagation_cone
        seen = set()
        complete_cone = []
        
        for gate in all_gates:
            if gate[0] not in seen:
                seen.add(gate[0])
                complete_cone.append(gate)
        
        return complete_cone

    def _get_fanin_recursive(self, gate_name, stop_at=None):
        """
        Helper to get fan-in cone with optional stop gate.
        
        Args:
            gate_name: Gate to analyze
            stop_at: Gate to stop at (e.g., the fault gate), None to traverse fully
            
        Returns:
            List of (output, gate_type, inputs) tuples in fan-in cone
        """
        cone = []
        visited = set()
        
        def dfs_backward(node):
            if node in visited or node in self.inputs:
                return
            if stop_at is not None and node == stop_at:
                return
            visited.add(node)
            
            if node in self.parser.gate_dict:
                g_type, inputs = self.parser.gate_dict[node]
                cone.append((node, g_type, inputs))
                for inp in inputs:
                    dfs_backward(inp)
        
        dfs_backward(gate_name)
        return cone

    def get_cone_inputs(self, cone_gates):
        """Extract all primary inputs referenced by a cone."""
        cone_inputs = set()
        for _, _, inputs in cone_gates:
            for inp in inputs:
                if inp in self.inputs:
                    cone_inputs.add(inp)
        return cone_inputs

    def build_miter(self, fault_wire, fault_type=None, force_diff=1):
        """Build miter circuit for fault detection."""
        clauses = []
        
        # 1. Good Circuit
        for out, g_type, inputs in self.gates:
            self._add_gate_clauses(clauses, self.var_map[out], g_type, 
                                  [self.var_map[i] for i in inputs])
            
        # 2. Faulty Circuit
        self.faulty_map = {name: self.var_map[name] for name in self.inputs}
        for out, _, _ in self.gates:
            if out not in self.faulty_map:
                self.faulty_map[out] = self.next_var
                self.next_var += 1
                
        # Inject Fault
        if fault_wire in self.faulty_map and fault_type is not None:
            f_var = self.faulty_map[fault_wire]
            if fault_type == 1: 
                clauses.append([f_var])
            elif fault_type == 0: 
                clauses.append([-f_var])

        for out, g_type, inputs in self.gates:
            if out == fault_wire: 
                continue
            out_var = self.faulty_map[out]
            in_vars = [self.faulty_map.get(i) for i in inputs]
            if None in in_vars: 
                continue 
            self._add_gate_clauses(clauses, out_var, g_type, in_vars)

        # 3. Miter Comparator
        miter_out = self.next_var
        self.next_var += 1
        diff_vars = []
        
        for out in sorted(list(set(self.outputs))):
            if out not in self.var_map or out not in self.faulty_map: 
                continue
            good = self.var_map[out]
            bad = self.faulty_map[out]
            diff = self.next_var
            self.next_var += 1
            
            clauses.extend([
                [-good, -bad, -diff], 
                [good, bad, -diff], 
                [-good, bad, diff], 
                [good, -bad, diff]
            ])
            diff_vars.append(diff)
            
        big_or = [-miter_out]
        for d in diff_vars:
            clauses.append([-d, miter_out])
            big_or.append(d)
        clauses.append(big_or)
        clauses.append([miter_out])
        
        return clauses

    def _add_gate_clauses(self, clauses, out, g_type, inputs):
        """Add CNF clauses for a gate."""
        if g_type == 'AND':
            for i in inputs: 
                clauses.append([-out, i])
            clauses.append([out] + [-i for i in inputs])
        elif g_type == 'OR':
            for i in inputs: 
                clauses.append([out, -i])
            clauses.append([-out] + inputs)
        elif g_type == 'NOT':
            clauses.append([-out, -inputs[0]])
            clauses.append([out, inputs[0]])
        elif g_type == 'NAND':
            for i in inputs: 
                clauses.append([out, i])
            clauses.append([-out] + [-i for i in inputs])
        elif g_type == 'NOR':
            for i in inputs: 
                clauses.append([-out, -i])
            clauses.append([out] + inputs)
        elif g_type == 'XOR':
            if len(inputs) == 2:
                a, b = inputs
                clauses.append([-out, -a, -b])
                clauses.append([-out, a, b])
                clauses.append([out, -a, b])
                clauses.append([out, a, -b])
            else:
                prev = inputs[0]
                for inp in inputs[1:-1]:
                    temp = self.next_var
                    self.next_var += 1
                    clauses.extend([
                        [-temp, -prev, -inp],
                        [-temp, prev, inp],
                        [temp, -prev, inp],
                        [temp, prev, -inp]
                    ])
                    prev = temp
                a, b = prev, inputs[-1]
                clauses.extend([
                    [-out, -a, -b],
                    [-out, a, b],
                    [out, -a, b],
                    [out, a, -b]
                ])
        elif g_type == 'XNOR':
            if len(inputs) == 2:
                a, b = inputs
                clauses.append([out, -a, -b])
                clauses.append([out, a, b])
                clauses.append([-out, -a, b])
                clauses.append([-out, a, -b])
        elif g_type == 'BUFF':
            clauses.append([-out, inputs[0]])
            clauses.append([out, -inputs[0]])
        else:
            if len(inputs) > 0:
                clauses.append([-out, inputs[0]])
                clauses.append([out, -inputs[0]])"""
Benchmark GNN-Guided SAT Solving for Circuit Testability

Note: Only uses POLARITY predictions (importance head removed from usage)
"""

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'pysat'))

import time
import csv
import random
import numpy as np
import torch
from pysat.solvers import Minisat22
from WireFaultMiter import WireFaultMiter
from neuro_utils import VectorizedGraphExtractor
from train_model import CircuitGNN_DualTask

# =============================================================================
# CONFIGS
# =============================================================================
BENCHMARK_DIR = "../../hdl-benchmarks/iscas85/bench/"
MODEL_PATH = "gnn_model_dual_task_17feat_improved.pth"
RESULTS_PATH = "results_gnn_polarity.csv"
NUM_FAULTS_PER_CIRCUIT = 10
SEED = 42

def set_global_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

set_global_seed(SEED)

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def get_benchmark_files(benchmark_dir):
    """Get all .bench files from benchmark directory"""
    if not os.path.exists(benchmark_dir):
        return []
    
    files = []
    for root, dirs, filenames in os.walk(benchmark_dir):
        for f in filenames:
            if f.endswith(".bench"):
                full_path = os.path.join(root, f)
                rel_path = os.path.relpath(full_path, benchmark_dir)
                files.append(rel_path)
    
    return sorted(files)

# =============================================================================
# BENCHMARKING
# =============================================================================

def run_benchmark():
    """Main benchmarking function"""
    print("=" * 80)
    print("GNN-GUIDED SAT SOLVING BENCHMARK (Polarity Only)")
    print("=" * 80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # Load model
    if not os.path.exists(MODEL_PATH):
        print(f"ERROR: Model not found at {MODEL_PATH}")
        print("Please train the model first!")
        return
    
    print(f"Loading model from {MODEL_PATH}...")
    model = CircuitGNN_DualTask(num_node_features=17).to(device)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    print("Model loaded successfully")
    
    # Get benchmark files
    files = get_benchmark_files(BENCHMARK_DIR)
    if not files:
        print(f"ERROR: No benchmark files found in {BENCHMARK_DIR}")
        return
    
    print(f"Found {len(files)} benchmark circuits")
    print("=" * 80)
    
    results = []
    
    for filename in files:
        filepath = os.path.join(BENCHMARK_DIR, filename)
        print(f"\nProcessing {filename}...")
        
        try:
            miter = WireFaultMiter(filepath)
            num_gates = len(miter.gates)
            
            if num_gates == 0:
                print(f"  Skipping (no gates)")
                continue
            
            if num_gates > 10000:
                print(f"  Skipping (too large: {num_gates} gates)")
                continue
            
            print(f"  Gates: {num_gates}, Inputs: {len(miter.inputs)}")
            
            # Create extractor
            extractor = VectorizedGraphExtractor(filepath, var_map=miter.var_map, device=device.type)
            
            # Sample faults
            all_gates = sorted(miter.gates, key=lambda x: x[0])
            num_tests = min(NUM_FAULTS_PER_CIRCUIT, len(all_gates))
            
            for i in range(num_tests):
                target_gate = random.choice(all_gates)[0]
                fault_type = 1  # SA1
                
                # === BASELINE: Standard SAT solving ===
                clauses_std = miter.build_miter(target_gate, fault_type, 1)
                
                t_std_start = time.time()
                with Minisat22(bootstrap_with=clauses_std) as solver:
                    std_result = solver.solve()
                    std_conflicts = solver.accum_stats()['conflicts']
                std_time = time.time() - t_std_start
                
                # === GNN-GUIDED: With polarity hints ===
                t_gnn_start = time.time()
                
                # 1. Get GNN predictions
                data = extractor.get_data_for_fault(target_gate, fault_type=fault_type).to(device)
                
                with torch.no_grad():
                    _, pol_scores = model(data)  # Only use polarity
                
                # 2. Build phase hints (polarity only)
                hint_literals = []
                for idx, name in enumerate(data.node_names):
                    if name in miter.inputs:
                        prob = pol_scores[idx].item()
                        var_id = miter.var_map.get(name)
                        
                        if var_id:
                            # Positive if prob > 0.5, negative otherwise
                            signed_lit = var_id if prob > 0.5 else -var_id
                            hint_literals.append(signed_lit)
                
                # 3. Solve with phase hints
                clauses_gnn = miter.build_miter(target_gate, fault_type, 1)
                
                with Minisat22(bootstrap_with=clauses_gnn) as solver:
                    if hint_literals:
                        solver.set_phases(hint_literals)
                    
                    gnn_result = solver.solve()
                    gnn_conflicts = solver.accum_stats()['conflicts']
                
                gnn_time = time.time() - t_gnn_start
                
                # Calculate speedup
                speedup = std_conflicts / max(gnn_conflicts, 1)
                status = "SAT" if gnn_result else "UNSAT"
                
                print(f"  Fault {i+1}/{num_tests} ({target_gate}): "
                      f"{std_conflicts} → {gnn_conflicts} conflicts ({speedup:.2f}x speedup)")
                
                # Record results
                results.append({
                    "Circuit": filename,
                    "Fault": target_gate,
                    "Status": status,
                    "Std_Conflicts": std_conflicts,
                    "GNN_Conflicts": gnn_conflicts,
                    "Speedup": speedup,
                    "Std_Time_s": std_time,
                    "GNN_Time_s": gnn_time
                })
        
        except Exception as e:
            print(f"  Error: {e}")
            continue
    
    # Save results
    if results:
        print("\n" + "=" * 80)
        print("BENCHMARK COMPLETE")
        print("=" * 80)
        
        with open(RESULTS_PATH, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)
        
        # Print summary statistics
        total_tests = len(results)
        avg_speedup = sum(r['Speedup'] for r in results) / total_tests
        max_speedup = max(r['Speedup'] for r in results)
        min_speedup = min(r['Speedup'] for r in results)
        
        speedups = [r['Speedup'] for r in results]
        median_speedup = sorted(speedups)[len(speedups) // 2]
        
        # Count improvements
        improved = sum(1 for r in results if r['Speedup'] > 1.0)
        improved_pct = (improved / total_tests) * 100
        
        print(f"Total tests: {total_tests}")
        print(f"Tests improved: {improved} ({improved_pct:.1f}%)")
        print(f"\nSpeedup Statistics:")
        print(f"  Average:  {avg_speedup:.2f}x")
        print(f"  Median:   {median_speedup:.2f}x")
        print(f"  Min:      {min_speedup:.2f}x")
        print(f"  Max:      {max_speedup:.2f}x")
        print(f"\nResults saved to: {RESULTS_PATH}")
        print("=" * 80)
    else:
        print("\nNo results generated!")

if __name__ == "__main__":
    run_benchmark()"""
Improved data generation with critical input identification.
This filters training data to only include inputs that actually affect fault detection.
"""

from pysat.solvers import Glucose3, Minisat22
import torch
from torch_geometric.data import Data
import multiprocessing as mp
import random
import os
import pickle

# Assuming these imports work in your environment
from BenchParser import BenchParser
from VerilogParser import VerilogParser  
from WireFaultMiter import WireFaultMiter
from neuro_utils import VectorizedGraphExtractor

CONFLICT_BUDGET = 10000
CRITICAL_INPUT_TEST_BUDGET = 50  # Quick tests for critical input detection


def identify_critical_inputs(clauses, assignment, cone_inputs, var_map):
    """
    Identify which inputs are actually critical for detecting the fault.
    
    An input is critical if flipping its polarity makes the SAT instance UNSAT
    (i.e., the fault becomes undetectable).
    
    Args:
        clauses: CNF clauses for the miter
        assignment: SAT solution (set of positive literals)
        cone_inputs: List of input signal names in the fault cone
        var_map: Dict mapping signal names to variable IDs
    
    Returns:
        dict: {input_name: polarity} for only critical inputs
              polarity is 1.0 for True, 0.0 for False
    """
    critical_inputs = {}
    
    for inp in cone_inputs:
        if inp not in var_map:
            continue
            
        var_id = var_map[inp]
        
        # What's the current polarity in the solution?
        correct_polarity = var_id in assignment
        
        # Try to solve with the OPPOSITE polarity
        test_literal = -var_id if correct_polarity else var_id
        
        with Glucose3(bootstrap_with=clauses) as probe:
            probe.conf_budget(CRITICAL_INPUT_TEST_BUDGET)
            result = probe.solve(assumptions=[test_literal])
            
            if not result:  # UNSAT
                # This input is critical! Flipping it breaks fault detection
                critical_inputs[inp] = 1.0 if correct_polarity else 0.0
    
    return critical_inputs


def process_single_fault(args):
    """
    Process a single fault with critical input identification.
    
    Returns:
        Data object or None if fault is untestable or has no critical inputs
    """
    bench_file, fault_name, fault_type, extractor_data = args
    
    try:
        # Parse circuit
        if bench_file.endswith('.bench'):
            parser = BenchParser(bench_file)
        else:
            parser = VerilogParser(bench_file)
        
        # Create fault miter
        miter = WireFaultMiter(parser)
        fault_type_name = "SA1" if fault_type == 1 else "SA0"
        clauses = miter.create_wire_fault_miter(fault_name, fault_type_name)
        
        if not clauses:
            return None
        
        # Get complete ATPG cone
        complete_cone = miter.get_complete_cone(fault_name)
        if not complete_cone:
            return None
        
        # Solve to find if fault is testable
        with Glucose3(bootstrap_with=clauses) as solver:
            solver.conf_budget(CONFLICT_BUDGET)
            if not solver.solve():
                return None  # Untestable fault
            
            assignment = set(solver.get_model())
        
        # Get inputs in the cone
        cone_inputs = miter.get_cone_inputs(complete_cone)
        if not cone_inputs:
            return None
        
        # CRITICAL STEP: Identify which inputs are actually critical
        critical_inputs = identify_critical_inputs(
            clauses, assignment, cone_inputs, miter.var_map
        )
        
        # Only create training sample if we found critical inputs
        if len(critical_inputs) < 1:  # Need at least 1 critical input
            return None
        
        # Create graph data for this fault
        # Assuming extractor is a VectorizedGraphExtractor instance
        # You'll need to reconstruct it from extractor_data
        extractor = VectorizedGraphExtractor.from_saved_data(extractor_data)
        data = extractor.get_data_for_fault(fault_name, fault_type=fault_type)
        
        # Build labels - ONLY for critical inputs
        y_polarity = torch.zeros(len(data.node_names), 1)
        train_mask = torch.zeros(len(data.node_names), 1)
        importance = torch.zeros(len(data.node_names), 1)
        
        for k, node_name in enumerate(data.node_names):
            if node_name in critical_inputs:
                y_polarity[k] = critical_inputs[node_name]
                train_mask[k] = 1.0
                importance[k] = 1.0  # All critical inputs have equal importance
        
        # Attach to data object
        data.y_polarity = y_polarity
        data.train_mask = train_mask
        data.y_importance = importance
        
        # Store metadata
        data.fault_name = fault_name
        data.fault_type = fault_type
        data.num_critical_inputs = len(critical_inputs)
        
        return data
        
    except Exception as e:
        print(f"Error processing fault {fault_name}: {e}")
        return None


def generate_dataset_parallel(bench_file, output_dir, num_workers=4):
    """
    Generate training dataset with parallel processing.
    
    Args:
        bench_file: Path to .bench or .v file
        output_dir: Where to save the dataset
        num_workers: Number of parallel workers
    """
    
    # Parse circuit once
    if bench_file.endswith('.bench'):
        parser = BenchParser(bench_file)
    else:
        parser = VerilogParser(bench_file)
    
    # Get all gates for fault injection
    all_gates = list(parser.gates.keys())
    
    # Create extractor and save its data for worker processes
    extractor = VectorizedGraphExtractor(parser)
    extractor_data = extractor.save_data()
    
    # Generate fault list (both SA0 and SA1 for each gate)
    fault_list = []
    for gate in all_gates:
        fault_list.append((bench_file, gate, 0, extractor_data))  # SA0
        fault_list.append((bench_file, gate, 1, extractor_data))  # SA1
    
    print(f"Processing {len(fault_list)} faults using {num_workers} workers...")
    
    # Process in parallel
    dataset = []
    with mp.Pool(num_workers) as pool:
        results = pool.imap_unordered(process_single_fault, fault_list, chunksize=10)
        
        for i, data in enumerate(results):
            if data is not None:
                dataset.append(data)
            
            if (i + 1) % 100 == 0:
                print(f"Processed {i+1}/{len(fault_list)} faults, "
                      f"collected {len(dataset)} samples")
    
    print(f"\nDataset generation complete!")
    print(f"Total samples: {len(dataset)}")
    
    # Save dataset
    os.makedirs(output_dir, exist_ok=True)
    circuit_name = os.path.basename(bench_file).replace('.bench', '').replace('.v', '')
    save_path = os.path.join(output_dir, f'{circuit_name}_critical_inputs.pkl')
    
    with open(save_path, 'wb') as f:
        pickle.dump(dataset, f)
    
    print(f"Saved to {save_path}")
    
    # Print statistics
    if dataset:
        critical_counts = [d.num_critical_inputs for d in dataset]
        print(f"\nCritical input statistics:")
        print(f"  Min: {min(critical_counts)}")
        print(f"  Max: {max(critical_counts)}")
        print(f"  Avg: {sum(critical_counts) / len(critical_counts):.2f}")
    
    return dataset


def load_and_merge_datasets(data_dir):
    """
    Load and merge multiple dataset files.
    """
    dataset = []
    
    for filename in os.listdir(data_dir):
        if filename.endswith('_critical_inputs.pkl'):
            filepath = os.path.join(data_dir, filename)
            with open(filepath, 'rb') as f:
                data = pickle.load(f)
                dataset.extend(data)
                print(f"Loaded {len(data)} samples from {filename}")
    
    print(f"\nTotal dataset size: {len(dataset)}")
    return dataset


def generate_dataset_for_folder(bench_folder, output_dir, num_workers=4):
    """
    Generate training dataset for all circuits in a folder.
    
    Args:
        bench_folder: Path to folder containing .bench or .v files
        output_dir: Where to save the datasets
        num_workers: Number of parallel workers
    
    Returns:
        Total number of samples generated across all circuits
    """
    from pathlib import Path
    
    bench_folder = Path(bench_folder)
    bench_files = list(bench_folder.glob('*.bench')) + list(bench_folder.glob('*.v'))
    
    if not bench_files:
        print(f"No .bench or .v files found in {bench_folder}")
        return 0
    
    print(f"Found {len(bench_files)} circuits in {bench_folder}")
    bench_files = sorted(bench_files)
    
    total_samples = 0
    for i, bench_file in enumerate(bench_files):
        print(f"\n{'='*70}")
        print(f"[{i+1}/{len(bench_files)}] Processing {bench_file.name}...")
        print(f"{'='*70}")
        
        dataset = generate_dataset_parallel(str(bench_file), output_dir, num_workers)
        total_samples += len(dataset)
    
    print(f"\n{'='*70}")
    print(f"ALL CIRCUITS COMPLETE")
    print(f"{'='*70}")
    print(f"Total circuits processed: {len(bench_files)}")
    print(f"Total samples generated: {total_samples}")
    
    return total_samples


if __name__ == "__main__":
    import argparse
    from pathlib import Path
    
    parser = argparse.ArgumentParser(description='Generate training data with critical input filtering')
    parser.add_argument('--bench', type=str, required=True, 
                       help='Path to .bench/.v file or folder containing them')
    parser.add_argument('--output', type=str, default='./training_data_critical', 
                       help='Output directory')
    parser.add_argument('--workers', type=int, default=4, help='Number of parallel workers')
    
    args = parser.parse_args()
    
    # Check if bench is a file or folder
    bench_path = Path(args.bench)
    
    if bench_path.is_dir():
        # Process entire folder
        generate_dataset_for_folder(args.bench, args.output, args.workers)
    elif bench_path.is_file():
        # Process single file
        generate_dataset_parallel(args.bench, args.output, args.workers)
    else:
        print(f"Error: {args.bench} is not a valid file or directory")
        exit(1)"""
Demonstration: Old vs New Cone Extraction
Shows the difference between incomplete and complete cone extraction
"""

import sys
import os

# Mock classes for demonstration
class MockParser:
    def __init__(self):
        self.inputs = ['PI1', 'PI2', 'PI3', 'PI4']
        self.outputs = ['PO1']
        self.gates = [
            ('g1', 'AND', ['PI1', 'PI2']),
            ('g2', 'NOT', ['PI3']),
            ('g3', 'OR', ['g1', 'g2']),      # TARGET GATE
            ('g4', 'BUFF', ['PI4']),
            ('g5', 'AND', ['g3', 'g4']),     # Propagates to output
            ('PO1', 'BUFF', ['g5'])
        ]
        
        # Build gate_dict
        self.gate_dict = {out: (typ, inp) for out, typ, inp in self.gates}
        
        # Build fanout map
        self.fanout_map = {}
        for out, _, inputs in self.gates:
            for inp in inputs:
                if inp not in self.fanout_map:
                    self.fanout_map[inp] = []
                self.fanout_map[inp].append(out)
    
    def get_fanout(self, node):
        return self.fanout_map.get(node, [])

def visualize_circuit():
    """Print ASCII art of the example circuit"""
    print("""
Example Circuit:
================

    PI1 ──┐
          AND──g1──┐
    PI2 ──┘        │
                   OR──g3(TARGET)──┐
    PI3 ──NOT─g2───┘                │
                                    │
    PI4 ──BUFF──g4──────────────────┘
                                    │
                                   AND──g5──BUFF──PO1

Target Gate: g3 (fault site)
Target Output: PO1
""")

def old_cone_extraction(parser, target_gate, target_output):
    """OLD METHOD: Only backward from output to fault"""
    print("\n" + "="*80)
    print("OLD METHOD: get_logic_cone()")
    print("="*80)
    print(f"Extracting backward cone from {target_output} to {target_gate}")
    
    cone = []
    visited = set()
    
    def dfs(node):
        if node in visited:
            return
        if node == target_gate:
            print(f"  - Stopped at {node} (target gate)")
            return
        visited.add(node)
        
        if node in parser.gate_dict:
            g_type, inputs = parser.gate_dict[node]
            cone.append((node, g_type, inputs))
            print(f"  - Added {node} ({g_type} {inputs})")
            for inp in inputs:
                dfs(inp)
    
    dfs(target_output)
    
    print(f"\nRESULT: {len(cone)} gates in cone")
    print(f"Gates: {[g[0] for g in cone]}")
    
    # Analyze what's missing
    print("\nANALYSIS:")
    print("✓ Fan-out cone (g3 → PO1): [PO1, g5, g4]")
    print("✗ Fan-in cone (inputs → g3): MISSING [g1, g2]")
    print("✗ Target gate itself: MISSING [g3]")
    
    return cone

def new_cone_extraction(parser, target_gate, target_output):
    """NEW METHOD: Complete cone with fan-in + fan-out"""
    print("\n" + "="*80)
    print("NEW METHOD: get_full_cone()")
    print("="*80)
    
    # Step 1: Fan-in cone
    print(f"\nStep 1: Extract fan-in cone of {target_gate}")
    fanin = []
    visited_fanin = set()
    
    def dfs_backward(node):
        if node in visited_fanin:
            return
        if node in parser.inputs:
            print(f"  - Reached input {node}")
            return
        visited_fanin.add(node)
        
        if node in parser.gate_dict:
            g_type, inputs = parser.gate_dict[node]
            fanin.append((node, g_type, inputs))
            print(f"  - Added {node} ({g_type} {inputs})")
            for inp in inputs:
                dfs_backward(inp)
    
    dfs_backward(target_gate)
    print(f"Fan-in cone: {len(fanin)} gates")
    
    # Step 2: Fan-out cone
    print(f"\nStep 2: Extract fan-out cone from {target_gate} to {target_output}")
    fanout = []
    visited_fanout = set()
    
    def dfs_forward(node):
        if node in visited_fanout:
            return
        visited_fanout.add(node)
        
        if node != target_gate and node in parser.gate_dict:
            g_type, inputs = parser.gate_dict[node]
            fanout.append((node, g_type, inputs))
            print(f"  - Added {node} ({g_type} {inputs})")
        
        fanout_gates = parser.get_fanout(node)
        for next_gate in fanout_gates:
            if next_gate not in visited_fanout:
                dfs_forward(next_gate)
    
    dfs_forward(target_gate)
    print(f"Fan-out cone: {len(fanout)} gates")
    
    # Step 3: Add fault gate itself
    print(f"\nStep 3: Add target gate {target_gate}")
    fault_gate = []
    if target_gate in parser.gate_dict:
        g_type, inputs = parser.gate_dict[target_gate]
        fault_gate = [(target_gate, g_type, inputs)]
        print(f"  - Added {target_gate} ({g_type} {inputs})")
    
    # Step 4: Combine
    all_gates = fanin + fault_gate + fanout
    seen = set()
    unique_cone = []
    
    for gate in all_gates:
        if gate[0] not in seen:
            seen.add(gate[0])
            unique_cone.append(gate)
    
    print(f"\nRESULT: {len(unique_cone)} gates in FULL cone")
    print(f"Gates: {[g[0] for g in unique_cone]}")
    
    print("\nANALYSIS:")
    print("✓ Fan-in cone: [g1, g2]")
    print("✓ Target gate: [g3]")
    print("✓ Fan-out cone: [g4, g5, PO1]")
    print("✓ COMPLETE cone for ATPG!")
    
    return unique_cone

def extract_cone_inputs(parser, cone_gates):
    """Extract primary inputs used in the cone"""
    cone_inputs = set()
    for _, _, inputs in cone_gates:
        for inp in inputs:
            if inp in parser.inputs:
                cone_inputs.add(inp)
    return cone_inputs

def compare_methods():
    """Main comparison function"""
    parser = MockParser()
    target_gate = 'g3'
    target_output = 'PO1'
    
    visualize_circuit()
    
    # Old method
    old_cone = old_cone_extraction(parser, target_gate, target_output)
    old_inputs = extract_cone_inputs(parser, old_cone)
    
    # New method
    new_cone = new_cone_extraction(parser, target_gate, target_output)
    new_inputs = extract_cone_inputs(parser, new_cone)
    
    # Final comparison
    print("\n" + "="*80)
    print("FINAL COMPARISON")
    print("="*80)
    
    print(f"\nOLD METHOD:")
    print(f"  Gates in cone: {len(old_cone)}")
    print(f"  Gate names: {[g[0] for g in old_cone]}")
    print(f"  Cone inputs: {sorted(old_inputs)}")
    
    print(f"\nNEW METHOD:")
    print(f"  Gates in cone: {len(new_cone)}")
    print(f"  Gate names: {[g[0] for g in new_cone]}")
    print(f"  Cone inputs: {sorted(new_inputs)}")
    
    print(f"\nDIFFERENCE:")
    missing_gates = set([g[0] for g in new_cone]) - set([g[0] for g in old_cone])
    missing_inputs = new_inputs - old_inputs
    
    print(f"  Missing gates: {sorted(missing_gates)}")
    print(f"  Missing inputs: {sorted(missing_inputs)}")
    
    print("\n" + "="*80)
    print("WHY THIS MATTERS FOR ATPG:")
    print("="*80)
    print("""
1. OLD METHOD only gets fan-out (propagation path)
   - Probing PI4 ✓ (affects g4 → g5 → PO1)
   - Probing PI1 ✗ (in fan-in, but NOT measured!)
   - Probing PI2 ✗ (in fan-in, but NOT measured!)
   - Probing PI3 ✗ (in fan-in, but NOT measured!)
   
   Result: INCOMPLETE importance measurements!

2. NEW METHOD gets BOTH fan-in and fan-out
   - Probing PI1 ✓ (affects g1 → g3)
   - Probing PI2 ✓ (affects g1 → g3)
   - Probing PI3 ✓ (affects g2 → g3)
   - Probing PI4 ✓ (affects g4 → g5 → PO1)
   
   Result: COMPLETE importance measurements!

3. THE MITER XOR IS STILL CORRECT
   - We compare good_circuit vs faulty_circuit at outputs
   - XOR detects if outputs differ
   - This works for BOTH methods
   
   The issue was CONE EXTRACTION, not miter logic!
""")

if __name__ == "__main__":
    compare_methods()#!/usr/bin/env python3
"""
Recursive Verilog Gate-Level Netlist Generator (IMPROVED)

This version actually produces gate-level netlists with gate instances,
not just wire assignments.

Usage:
    python gate_level_synth.py <input_dir> <output_dir> [--liberty <lib_file>]
"""

import os
import sys
import re
import shutil
import subprocess
from pathlib import Path
from tqdm import tqdm
import multiprocessing as mp

def synthesize_to_gates(input_file, output_file, liberty_file=None):
    """
    Synthesize Verilog to ACTUAL gate-level netlist using Yosys.
    
    This version ensures you get gate instances, not just wire assignments.
    """
    
    if liberty_file and os.path.exists(liberty_file):
        # Technology-mapped gate-level synthesis with standard cells
        script = f"""
# Read input design
read_verilog {input_file}

# Elaborate design
hierarchy -check -auto-top

# Convert processes to netlists
proc

# Optimize
opt

# Map flip-flops to library cells
dfflibmap -liberty {liberty_file}

# Technology mapping with ABC
abc -liberty {liberty_file}

# Final cleanup
opt_clean

# Write gate-level netlist
write_verilog -noattr -noexpr {output_file}
"""
    else:
        # Generic gate-level synthesis using Yosys internal cells
        # This WILL produce actual gate instances
        script = f"""
# Read input design
read_verilog {input_file}

# Elaborate design
hierarchy -check -auto-top

# Convert to gate level (this is the key difference)
proc; opt; fsm; opt; memory; opt

# Flatten design
flatten

# Map to coarse cells
techmap; opt

# Technology mapping with ABC (uses generic gates)
abc -g AND,NAND,OR,NOR,XOR,XNOR,ANDNOT,ORNOT

# Map remaining cells to gates
techmap; opt

# Final optimization
opt_clean -purge

# Write gate-level netlist
write_verilog -noattr -noexpr {output_file}
"""
    
    script_file = output_file + ".ys"
    
    try:
        with open(script_file, 'w') as f:
            f.write(script)
        
        result = subprocess.run(
            ['yosys', '-s', script_file],
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if result.returncode == 0 and os.path.exists(output_file):
            # Validate output
            with open(output_file, 'r') as f:
                content = f.read()
            
            # Remove Yosys header
            if content.startswith('/* Generated by Yosys'):
                end = content.find('*/')
                if end != -1:
                    content = content[end + 2:].lstrip()
            
            # Check if file has actual module
            test = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
            test = re.sub(r'//.*?$', '', test, flags=re.MULTILINE)
            test = test.strip()
            
            if not test or 'module' not in test:
                # Empty file - delete and fail
                if os.path.exists(output_file):
                    os.remove(output_file)
                if os.path.exists(script_file):
                    os.remove(script_file)
                return False
            
            # Verify it's actually gate-level (has gate instances or assigns)
            # Gate-level should have either gate instances or structural assigns
            has_gates = bool(re.search(r'\s+(AND|OR|NOT|NAND|NOR|XOR|XNOR|BUF|MUX|DFF|DFFE)\s+', content, re.IGNORECASE))
            has_instances = bool(re.search(r'^\s*\w+\s+\w+\s*\(', content, re.MULTILINE))
            
            if not (has_gates or has_instances):
                # Might still be valid structural Verilog with assigns
                # Keep it anyway as it's at least flattened
                pass
            
            # Write cleaned content
            with open(output_file, 'w') as f:
                f.write(content)
            
            if os.path.exists(script_file):
                os.remove(script_file)
            return True
        else:
            if os.path.exists(script_file):
                os.remove(script_file)
            return False
            
    except (FileNotFoundError, subprocess.TimeoutExpired, Exception) as e:
        if os.path.exists(script_file):
            os.remove(script_file)
        return False


def find_verilog_files(root_dir):
    """Find ALL .v files recursively."""
    verilog_files = []
    root_path = Path(root_dir).resolve()
    
    for dirpath, _, filenames in os.walk(root_path):
        for filename in filenames:
            if filename.endswith(('.v', '.verilog', '.sv')):
                abs_path = Path(dirpath) / filename
                rel_path = abs_path.relative_to(root_path)
                verilog_files.append((str(rel_path), str(abs_path)))
    
    return sorted(verilog_files)


def process_single_file(args):
    """Worker function for parallel processing."""
    rel_path, input_path, output_dir, liberty_file = args
    
    output_path = Path(output_dir) / rel_path
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    success = synthesize_to_gates(input_path, str(output_path), liberty_file)
    
    return (rel_path, success)


def remove_empty_directories(root_dir):
    """Remove empty directories recursively from nested to parent."""
    removed_count = 0
    root_path = Path(root_dir).resolve()
    
    for dirpath, dirnames, filenames in os.walk(root_path, topdown=False):
        if Path(dirpath).resolve() == root_path:
            continue
            
        try:
            if not os.listdir(dirpath):
                os.rmdir(dirpath)
                removed_count += 1
        except OSError:
            pass
    
    return removed_count


def synthesize_directory_tree(input_dir, output_dir, liberty_file=None, num_workers=None):
    """Main function to recursively synthesize Verilog files to gate-level."""
    
    print("=" * 80)
    print("RECURSIVE VERILOG GATE-LEVEL SYNTHESIZER v2.0")
    print("=" * 80)
    print(f"Input:   {input_dir}")
    print(f"Output:  {output_dir}")
    if liberty_file:
        print(f"Library: {liberty_file}")
        if not os.path.exists(liberty_file):
            print(f"⚠️  WARNING: Liberty file not found! Using generic gates.")
            liberty_file = None
    else:
        print("Library: Generic gates (ABC with AND/OR/NOT/NAND/NOR/XOR/XNOR)")
    print("=" * 80)
    
    print("\n🔍 Scanning directory tree...")
    verilog_files = find_verilog_files(input_dir)
    
    if not verilog_files:
        print(f"❌ No Verilog files found in {input_dir}")
        return
    
    print(f"✅ Found {len(verilog_files)} Verilog files\n")
    
    os.makedirs(output_dir, exist_ok=True)
    
    if num_workers is None:
        num_workers = min(os.cpu_count() or 4, 8)
    
    print(f"🚀 Processing with {num_workers} parallel workers...\n")
    
    tasks = [(rel, abs_path, output_dir, liberty_file) 
             for rel, abs_path in verilog_files]
    
    successful = 0
    failed = 0
    failed_files = []
    
    try:
        mp.set_start_method('spawn', force=True)
    except:
        pass
    
    with mp.Pool(processes=num_workers) as pool:
        for rel_path, success in tqdm(
            pool.imap_unordered(process_single_file, tasks),
            total=len(tasks),
            desc="Synthesizing to gates"
        ):
            if success:
                successful += 1
            else:
                failed += 1
                failed_files.append(rel_path)
    
    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)
    print(f"Total files:      {len(verilog_files)}")
    print(f"✅ Successful:    {successful}")
    print(f"❌ Failed:        {failed}")
    print(f"Success rate:     {successful/len(verilog_files)*100:.1f}%")
    print("=" * 80)
    
    if failed > 0:
        print(f"\n⚠️  {failed} files failed")
        print("First 10 failed files:")
        for f in failed_files[:10]:
            print(f"  - {f}")
        if len(failed_files) > 10:
            print(f"  ... and {len(failed_files) - 10} more")
    
    # Clean up empty directories
    print("\n🧹 Cleaning up empty directories...")
    removed = remove_empty_directories(output_dir)
    if removed > 0:
        print(f"✅ Removed {removed} empty directories")
    else:
        print("✅ No empty directories to remove")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Recursively synthesize Verilog to ACTUAL gate-level netlists",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generic gate-level (AND, OR, NOT, NAND, NOR, XOR, XNOR gates)
  python gate_level_synth.py ./input ./output
  
  # Technology-mapped with standard cell library
  python gate_level_synth.py ./input ./output --liberty sky130.lib
  
  # Custom worker count
  python gate_level_synth.py ./input ./output --workers 16

Output will contain actual gate instances like:
  AND _001_ (.A(n1), .B(n2), .Y(n3));
  OR _002_ (.A(n3), .B(n4), .Y(n5));
  DFF _003_ (.D(n5), .Q(n6), .CLK(clk));
        """
    )
    
    parser.add_argument('input_dir', help='Input directory')
    parser.add_argument('output_dir', help='Output directory')
    parser.add_argument('--liberty', type=str, default=None,
                       help='Liberty (.lib) file for technology mapping')
    parser.add_argument('--workers', type=int, default=None,
                       help='Number of parallel workers')
    
    args = parser.parse_args()
    
    if not os.path.isdir(args.input_dir):
        print(f"❌ Error: {args.input_dir} does not exist")
        sys.exit(1)
    
    synthesize_directory_tree(
        args.input_dir,
        args.output_dir,
        liberty_file=args.liberty,
        num_workers=args.workers
    )


if __name__ == "__main__":
    main()"""
Main Pipeline for GNN-Guided SAT Solving

Usage:
    python main.py generate  # Generate training data
    python main.py train     # Train the model
    python main.py benchmark # Run benchmarks
    python main.py all       # Run complete pipeline
"""

import sys
import os

def print_usage():
    print(__doc__)

def main():
    if len(sys.argv) < 2:
        print_usage()
        return
    
    command = sys.argv[1].lower()
    
    if command == "generate":
        print("Starting data generation...")
        from data_generation_hybrid import generate_dataset
        generate_dataset()
        
    elif command == "train":
        print("Starting model training...")
        from train_model import train_model
        train_model()
        
    elif command == "benchmark":
        print("Starting benchmark...")
        from benchmark import run_benchmark
        run_benchmark()
        
    elif command == "all":
        print("Running complete pipeline...")
        print("\n" + "="*80)
        print("STEP 1: Data Generation")
        print("="*80)
        from data_generation_hybrid import generate_dataset
        generate_dataset()
        
        print("\n" + "="*80)
        print("STEP 2: Model Training")
        print("="*80)
        from train_model import train_model
        train_model()
        
        print("\n" + "="*80)
        print("STEP 3: Benchmarking")
        print("="*80)
        from benchmark import run_benchmark
        run_benchmark()
        
        print("\n" + "="*80)
        print("PIPELINE COMPLETE!")
        print("="*80)
        
    else:
        print(f"Unknown command: {command}")
        print_usage()

if __name__ == "__main__":
    main()import torch
import math
from torch_geometric.data import Data
from BenchParser import BenchParser

class VectorizedGraphExtractor:
    """
    High-Performance SCOAP Extractor using Vectorized Tensor Operations.
    Generates 17-dimensional feature vectors (16 Base + 1 Target Value).
    """
    
    TYPE_MAP = {
        'INPUT': 0, 'PPI': 0, 
        'BUFF': 1, 'NOT': 2,
        'AND': 3, 'NAND': 4,
        'OR': 5, 'NOR': 6,
        'XOR': 7, 'XNOR': 7
    }

    def __init__(self, bench_path, var_map=None, device='cpu'):
        self.parser = BenchParser(bench_path)
        self.device = device
        
        if var_map:
            self.var_map = var_map
        else:
            self.var_map = self.parser.build_var_map()
            
        self.ordered_names = sorted(self.var_map.keys(), key=lambda k: self.var_map[k])
        self.name_to_idx = {name: i for i, name in enumerate(self.ordered_names)}
        self.num_nodes = len(self.ordered_names)
        
        # Build structural tensors
        self.edges_list = []
        self.node_types = torch.zeros(self.num_nodes, dtype=torch.long, device=device)
        
        for name, g_type, _ in self.parser.gates:
            if name in self.name_to_idx:
                self.node_types[self.name_to_idx[name]] = self.TYPE_MAP.get(g_type, 1)
        
        for pi in self.parser.inputs:
            if pi in self.name_to_idx: 
                self.node_types[self.name_to_idx[pi]] = self.TYPE_MAP['INPUT']
        for ppi in self.parser.ppis:
            if ppi in self.name_to_idx: 
                self.node_types[self.name_to_idx[ppi]] = self.TYPE_MAP['INPUT']
        
        for out, _, inputs in self.parser.gates:
            if out in self.name_to_idx:
                dst = self.name_to_idx[out]
                for inp in inputs:
                    if inp in self.name_to_idx:
                        src = self.name_to_idx[inp]
                        self.edges_list.append([src, dst])
        
        if self.edges_list:
            self.edge_index = torch.tensor(self.edges_list, dtype=torch.long, device=device).t().contiguous()
        else:
            self.edge_index = torch.zeros((2, 0), dtype=torch.long, device=device)
            
        self.masks = {}
        for t_name, t_id in self.TYPE_MAP.items():
            self.masks[t_name] = (self.node_types == t_id)

        self.adj = [[] for _ in range(self.num_nodes)]
        self.parents = [[] for _ in range(self.num_nodes)]
        for src, dst in self.edges_list:
            self.adj[src].append(dst)
            self.parents[dst].append(src)
            
        self.cc0, self.cc1, self.co = self._compute_scoap_vectorized()
        self.x_base = self._build_base_features()

    def _compute_scoap_vectorized(self):
        """Vectorized SCOAP: Forward Controllability & Backward Observability"""
        num_nodes = self.num_nodes
        src_idx, dst_idx = self.edge_index
        
        cc0 = torch.ones(num_nodes, device=self.device)
        cc1 = torch.ones(num_nodes, device=self.device)
        
        mask_and = self.masks['AND'] | self.masks['NAND']
        mask_or  = self.masks['OR'] | self.masks['NOR']
        mask_inv = self.masks['NAND'] | self.masks['NOR'] | self.masks['NOT']
        mask_xor = self.masks['XOR']
        mask_buf_not = self.masks['BUFF'] | self.masks['NOT']
        
        for _ in range(50): 
            cc0_prev, cc1_prev = cc0.clone(), cc1.clone()
            edge_cc0 = cc0[src_idx]
            edge_cc1 = cc1[src_idx]
            
            min_cc0 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(
                0, dst_idx, edge_cc0, reduce='min', include_self=False)
            min_cc1 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(
                0, dst_idx, edge_cc1, reduce='min', include_self=False)
            sum_cc0 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc0)
            sum_cc1 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc1)
            
            cc0[mask_and] = min_cc0[mask_and] + 1
            cc1[mask_and] = sum_cc1[mask_and] + 1
            cc0[mask_or] = sum_cc0[mask_or] + 1
            cc1[mask_or] = min_cc1[mask_or] + 1
            cc0[mask_buf_not] = min_cc0[mask_buf_not] + 1
            cc1[mask_buf_not] = min_cc1[mask_buf_not] + 1
            cc0[mask_xor] = torch.minimum(sum_cc0[mask_xor], sum_cc1[mask_xor]) + 1
            cc1[mask_xor] = torch.maximum(min_cc0[mask_xor], min_cc1[mask_xor]) + 1
            
            temp_cc0 = cc0.clone()
            cc0[mask_inv] = cc1[mask_inv]
            cc1[mask_inv] = temp_cc0[mask_inv]
            
            mask_input = self.masks['INPUT']
            cc0[mask_input] = 1.0
            cc1[mask_input] = 1.0
            
            if torch.allclose(cc0, cc0_prev) and torch.allclose(cc1, cc1_prev):
                break

        co = torch.full((num_nodes,), 1e6, device=self.device)
        output_indices = [self.name_to_idx[n] for n in self.parser.all_outputs if n in self.name_to_idx]
        if output_indices:
            co[torch.tensor(output_indices, device=self.device)] = 0.0
        
        gate_cc0_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc0[src_idx])
        gate_cc1_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc1[src_idx])
        gate_min_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(
            0, dst_idx, torch.minimum(cc0[src_idx], cc1[src_idx]))

        for _ in range(50):
            co_prev = co.clone()
            co_dst = co[dst_idx]
            dst_types = self.node_types[dst_idx]
            side_costs = torch.zeros_like(co_dst)
            
            is_and = (dst_types == self.TYPE_MAP['AND']) | (dst_types == self.TYPE_MAP['NAND'])
            side_costs[is_and] = gate_cc1_sum[dst_idx][is_and] - cc1[src_idx][is_and]
            is_or = (dst_types == self.TYPE_MAP['OR']) | (dst_types == self.TYPE_MAP['NOR'])
            side_costs[is_or] = gate_cc0_sum[dst_idx][is_or] - cc0[src_idx][is_or]
            is_xor = (dst_types == self.TYPE_MAP['XOR'])
            side_costs[is_xor] = gate_min_sum[dst_idx][is_xor] - torch.minimum(cc0[src_idx], cc1[src_idx])[is_xor]
            
            path_costs = co_dst + side_costs + 1
            new_co = torch.zeros_like(co).scatter_reduce_(
                0, src_idx, path_costs, reduce='min', include_self=False)
            co = torch.minimum(co, new_co)
            if torch.allclose(co, co_prev):
                break
        
        return cc0, cc1, co

    def _compute_depth_fast(self, reverse=False):
        """Vectorized Topological Depth"""
        d_vals = torch.zeros(self.num_nodes, device=self.device)
        src_idx, dst_idx = self.edge_index
        prop_src = dst_idx if reverse else src_idx
        prop_dst = src_idx if reverse else dst_idx
        
        for _ in range(50):
            changed = False
            src_depths = d_vals[prop_src]
            new_depths = torch.zeros(self.num_nodes, device=self.device).scatter_reduce_(
                0, prop_dst, src_depths, reduce='amax', include_self=True)
            new_depths = new_depths + 1
            if not torch.allclose(d_vals, new_depths):
                d_vals = new_depths
                changed = True
            if not changed: 
                break
        
        max_d = d_vals.max() if d_vals.max() > 0 else 1.0
        return (d_vals / max_d).unsqueeze(1)

    def _build_base_features(self):
        """Builds 16 Base Features (without target value)"""
        x_type = torch.nn.functional.one_hot(self.node_types, num_classes=8).float()
        fwd_depth = self._compute_depth_fast(reverse=False)
        rev_depth = self._compute_depth_fast(reverse=True)
        
        f_cc0 = torch.log(self.cc0 + 1).unsqueeze(1) / 10.0
        f_cc1 = torch.log(self.cc1 + 1).unsqueeze(1) / 10.0
        f_co  = torch.log(self.co + 1).unsqueeze(1) / 10.0
        
        is_output = torch.zeros((self.num_nodes, 1), device=self.device)
        for name in self.parser.all_outputs:
            if name in self.name_to_idx: 
                is_output[self.name_to_idx[name]] = 1.0
        
        zeros = torch.zeros((self.num_nodes, 2), device=self.device)
        
        return torch.cat([x_type, fwd_depth, rev_depth, zeros, f_cc0, f_cc1, f_co, is_output], dim=1)

    def get_data_for_fault(self, fault_name, fault_type=1):
        """
        Generate Data object with 17 features.
        fault_type: 1 for SA1 (Target=0), 0 for SA0 (Target=1).
        """
        x = self.x_base.clone()
        tid = self.name_to_idx.get(fault_name)
        
        # 17th Feature: Target Value
        target_feat = torch.full((self.num_nodes, 1), 0.5, device=self.device)
        
        if tid is not None:
            x[tid, 10] = 1.0  # Fault location marker
            target_feat[tid] = 0.0 if fault_type == 1 else 1.0
            
            # BFS Distance
            dist = torch.full((self.num_nodes,), -1.0, device=self.device)
            dist[tid] = 0.0
            queue = [tid]
            visited = {tid: 0}
            idx = 0
            
            while idx < len(queue):
                u = queue[idx]
                idx += 1
                d = visited[u]
                if d >= 10: 
                    continue
                
                neighbors = self.adj[u] + self.parents[u]
                for v in neighbors:
                    if v not in visited:
                        visited[v] = d + 1
                        dist[v] = d + 1
                        queue.append(v)
            
            mask_visited = (dist != -1)
            if mask_visited.any():
                max_d = dist.max()
                if max_d == 0: 
                    max_d = 1.0
                x[mask_visited, 11] = 1.0 - (dist[mask_visited] / max_d)
        
        x = torch.cat([x, target_feat], dim=1)
        return Data(x=x, edge_index=self.edge_index, node_names=self.ordered_names)"""
Complete Model Training Script with Critical Input Support
===========================================================

This training script is optimized for the critical input filtered dataset.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATv2Conv, global_mean_pool
from torch_geometric.data import Data, DataLoader
import numpy as np
import os
import pickle
from pathlib import Path
import argparse
from datetime import datetime


# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================

class CircuitGNN_Polarity(torch.nn.Module):
    """
    GNN for predicting input polarities in circuit ATPG.
    
    Optimized for critical input learning with:
    - Deeper architecture (12 layers)
    - Residual connections
    - Better normalization
    """
    
    def __init__(self, num_node_features=17, num_layers=12, hidden_dim=64, dropout=0.1):
        super().__init__()
        
        self.num_layers = num_layers
        self.dropout = dropout
        
        # Input projection
        self.input_proj = nn.Sequential(
            nn.Linear(num_node_features, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ELU()
        )
        
        # GNN layers with residual connections
        self.convs = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()
        
        for i in range(num_layers):
            self.convs.append(
                GATv2Conv(hidden_dim, hidden_dim, heads=2, concat=False, dropout=dropout)
            )
            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        
        # Output head
        self.output_head = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ELU(),
            nn.Dropout(dropout),
            nn.Linear(32, 1)
        )
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        
        # Input projection
        x = self.input_proj(x)
        
        # GNN layers with residual connections
        for i in range(self.num_layers):
            identity = x
            x = self.convs[i](x, edge_index)
            x = self.bns[i](x)
            x = F.elu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = x + identity  # Residual connection
        
        # Output
        x = self.output_head(x)
        return torch.sigmoid(x)


# ============================================================================
# IMPROVED LOSS FUNCTION
# ============================================================================

def weighted_bce_loss(pred, target, mask, importance_weights=None):
    """
    Binary cross-entropy loss with optional importance weighting.
    
    Args:
        pred: Predicted polarities [N, 1]
        target: Ground truth polarities [N, 1]
        mask: Training mask [N, 1] (1.0 for labeled nodes, 0.0 otherwise)
        importance_weights: Optional [N, 1] weights for each node
    
    Returns:
        Weighted loss scalar
    """
    # Compute BCE for all nodes
    bce = F.binary_cross_entropy(pred, target, reduction='none')
    
    # Apply importance weights if provided
    if importance_weights is not None:
        bce = bce * importance_weights
    
    # Apply mask and average over labeled nodes
    masked_loss = (bce * mask).sum() / mask.sum().clamp(min=1)
    
    return masked_loss


def focal_loss(pred, target, mask, alpha=0.25, gamma=2.0):
    """
    Focal loss to handle class imbalance in critical inputs.
    
    Focuses training on hard examples (inputs with uncertain predictions).
    """
    # Compute BCE
    bce = F.binary_cross_entropy(pred, target, reduction='none')
    
    # Compute focal weight
    p_t = pred * target + (1 - pred) * (1 - target)
    focal_weight = (1 - p_t) ** gamma
    
    # Combine
    focal = alpha * focal_weight * bce
    
    # Apply mask
    masked_loss = (focal * mask).sum() / mask.sum().clamp(min=1)
    
    return masked_loss


# ============================================================================
# TRAINING FUNCTION
# ============================================================================

def train_epoch(model, loader, optimizer, device, use_focal=False, focal_alpha=0.25, focal_gamma=2.0):
    """Train for one epoch."""
    model.train()
    total_loss = 0
    num_batches = 0
    
    for batch in loader:
        batch = batch.to(device)
        optimizer.zero_grad()
        
        # Forward pass
        pred = model(batch)
        
        # Get mask and targets
        mask = batch.train_mask
        target = batch.y_polarity
        
        # Compute loss
        if use_focal:
            loss = focal_loss(pred, target, mask, focal_alpha, focal_gamma)
        else:
            # Use importance weights if available
            importance = batch.y_importance if hasattr(batch, 'y_importance') else None
            loss = weighted_bce_loss(pred, target, mask, importance)
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
    
    return total_loss / num_batches


def evaluate(model, loader, device):
    """Evaluate on validation/test set."""
    model.eval()
    total_loss = 0
    total_correct = 0
    total_labeled = 0
    num_batches = 0
    
    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            
            # Forward pass
            pred = model(batch)
            
            # Get mask and targets
            mask = batch.train_mask
            target = batch.y_polarity
            
            # Compute loss
            loss = weighted_bce_loss(pred, target, mask)
            total_loss += loss.item()
            
            # Compute accuracy on labeled nodes
            pred_binary = (pred > 0.5).float()
            correct = ((pred_binary == target) * mask).sum().item()
            labeled = mask.sum().item()
            
            total_correct += correct
            total_labeled += labeled
            num_batches += 1
    
    avg_loss = total_loss / num_batches
    accuracy = total_correct / total_labeled if total_labeled > 0 else 0.0
    
    return avg_loss, accuracy


# ============================================================================
# DATASET LOADING
# ============================================================================

def load_dataset(data_dir, train_ratio=0.8, val_ratio=0.1):
    """
    Load and split dataset.
    
    Args:
        data_dir: Directory containing .pkl files
        train_ratio: Fraction for training
        val_ratio: Fraction for validation (rest is test)
    
    Returns:
        train_dataset, val_dataset, test_dataset
    """
    all_data = []
    
    # Load all pickle files
    data_dir = Path(data_dir)
    for pkl_file in data_dir.glob('*.pkl'):
        print(f"Loading {pkl_file.name}...")
        with open(pkl_file, 'rb') as f:
            data = pickle.load(f)
            all_data.extend(data)
    
    print(f"Loaded {len(all_data)} samples total")
    
    # Shuffle
    np.random.shuffle(all_data)
    
    # Split
    n = len(all_data)
    n_train = int(n * train_ratio)
    n_val = int(n * val_ratio)
    
    train_dataset = all_data[:n_train]
    val_dataset = all_data[n_train:n_train + n_val]
    test_dataset = all_data[n_train + n_val:]
    
    print(f"Split: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test")
    
    return train_dataset, val_dataset, test_dataset


# ============================================================================
# MAIN TRAINING LOOP
# ============================================================================

def train_model(args):
    """Main training function."""
    
    # Set random seeds
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Load dataset
    print("Loading dataset...")
    train_dataset, val_dataset, test_dataset = load_dataset(
        args.data_dir, args.train_ratio, args.val_ratio
    )
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    # Get number of node features from first sample
    num_features = train_dataset[0].x.shape[1]
    print(f"Number of node features: {num_features}")
    
    # Create model
    print("Creating model...")
    model = CircuitGNN_Polarity(
        num_node_features=num_features,
        num_layers=args.num_layers,
        hidden_dim=args.hidden_dim,
        dropout=args.dropout
    ).to(device)
    
    # Count parameters
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model has {num_params:,} trainable parameters")
    
    # Optimizer
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    
    # Learning rate scheduler
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10, verbose=True
    )
    
    # Training loop
    print("\nStarting training...")
    best_val_loss = float('inf')
    best_val_acc = 0.0
    patience_counter = 0
    
    for epoch in range(args.epochs):
        # Train
        train_loss = train_epoch(
            model, train_loader, optimizer, device,
            use_focal=args.use_focal,
            focal_alpha=args.focal_alpha,
            focal_gamma=args.focal_gamma
        )
        
        # Validate
        val_loss, val_acc = evaluate(model, val_loader, device)
        
        # Update scheduler
        scheduler.step(val_loss)
        
        # Print progress
        print(f"Epoch {epoch+1}/{args.epochs}: "
              f"train_loss={train_loss:.4f}, "
              f"val_loss={val_loss:.4f}, "
              f"val_acc={val_acc:.4f}")
        
        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_val_acc = val_acc
            patience_counter = 0
            
            # Save checkpoint
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_acc': val_acc,
                'args': args
            }
            torch.save(checkpoint, args.save_path)
            print(f"  -> Saved best model (val_loss={val_loss:.4f})")
        else:
            patience_counter += 1
            
            # Early stopping
            if patience_counter >= args.patience:
                print(f"\nEarly stopping at epoch {epoch+1}")
                break
    
    # Load best model and evaluate on test set
    print("\nLoading best model for final evaluation...")
    checkpoint = torch.load(args.save_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    test_loss, test_acc = evaluate(model, test_loader, device)
    print(f"\nFinal Test Results:")
    print(f"  Loss: {test_loss:.4f}")
    print(f"  Accuracy: {test_acc:.4f}")
    print(f"\nBest Validation Results (epoch {checkpoint['epoch']+1}):")
    print(f"  Loss: {best_val_loss:.4f}")
    print(f"  Accuracy: {best_val_acc:.4f}")
    
    return model


# ============================================================================
# COMMAND LINE INTERFACE
# ============================================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train GNN for circuit polarity prediction')
    
    # Data
    parser.add_argument('--data_dir', type=str, required=True,
                       help='Directory containing training data (.pkl files)')
    parser.add_argument('--train_ratio', type=float, default=0.8,
                       help='Fraction of data for training')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                       help='Fraction of data for validation')
    
    # Model
    parser.add_argument('--num_layers', type=int, default=12,
                       help='Number of GNN layers')
    parser.add_argument('--hidden_dim', type=int, default=64,
                       help='Hidden dimension size')
    parser.add_argument('--dropout', type=float, default=0.1,
                       help='Dropout rate')
    
    # Training
    parser.add_argument('--epochs', type=int, default=200,
                       help='Maximum number of epochs')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='Batch size')
    parser.add_argument('--lr', type=float, default=0.001,
                       help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                       help='Weight decay')
    parser.add_argument('--patience', type=int, default=30,
                       help='Early stopping patience')
    
    # Loss function
    parser.add_argument('--use_focal', action='store_true',
                       help='Use focal loss instead of BCE')
    parser.add_argument('--focal_alpha', type=float, default=0.25,
                       help='Focal loss alpha parameter')
    parser.add_argument('--focal_gamma', type=float, default=2.0,
                       help='Focal loss gamma parameter')
    
    # Misc
    parser.add_argument('--save_path', type=str, default='best_model.pt',
                       help='Path to save best model')
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed')
    
    args = parser.parse_args()
    
    # Create save directory if needed
    save_dir = os.path.dirname(args.save_path)
    if save_dir and not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    # Train
    model = train_model(args)
    
    print(f"\nTraining complete! Model saved to {args.save_path}")"""
Validation Script: Verify Consistent Node Ordering Across Parsers

This script tests that BenchParser and VerilogParser produce IDENTICAL
node orderings for the same circuit in different formats.
"""

import tempfile
import os

# Import both parsers
import sys
sys.path.insert(0, '/home/claude')

print("=" * 80)
print("VALIDATING CONSISTENT NODE ORDERING ACROSS PARSERS")
print("=" * 80)

# Test circuit (simple example)
test_circuit_bench = """
# Simple test circuit
INPUT(a)
INPUT(b)
INPUT(c)
OUTPUT(out1)
OUTPUT(out2)

n1 = AND(a, b)
n2 = OR(b, c)
out1 = NAND(n1, n2)
out2 = NOT(n1)
"""

test_circuit_verilog = """
module test(a, b, c, out1, out2);
  input a, b, c;
  output out1, out2;
  wire n1, n2;
  wire unused_wire;  // This should NOT affect ordering!
  
  and (n1, a, b);
  or (n2, b, c);
  nand (out1, n1, n2);
  not (out2, n1);
endmodule
"""

print("\nTest Circuit (Bench format):")
print(test_circuit_bench)

print("\nTest Circuit (Verilog format):")
print(test_circuit_verilog)

# Create temporary files
bench_file = tempfile.NamedTemporaryFile(mode='w', suffix='.bench', delete=False)
verilog_file = tempfile.NamedTemporaryFile(mode='w', suffix='.v', delete=False)

bench_file.write(test_circuit_bench)
bench_file.close()

verilog_file.write(test_circuit_verilog)
verilog_file.close()

try:
    # Import parsers
    from BenchParser import BenchParser
    
    # Try old VerilogParser first to show the bug
    try:
        print("\n" + "=" * 80)
        print("TEST 1: OLD VerilogParser (WITH BUG)")
        print("=" * 80)
        
        # Temporarily rename the fixed version
        if os.path.exists('/home/claude/VerilogParser.py'):
            os.rename('/home/claude/VerilogParser.py', '/home/claude/VerilogParser_backup.py')
        
        # Load old version from documents
        old_verilog_code = '''
def get_all_wires(self):
    wires = set(self.all_inputs + self.all_outputs + self.wires)  # BUG: includes self.wires!
    for out, _, inputs in self.gates:
        wires.add(out)
        wires.update(inputs)
    return sorted(list(wires))
'''
        print("\nOld VerilogParser.get_all_wires():")
        print(old_verilog_code)
        print("\n⚠️  Note: Includes 'self.wires' which may contain unused wires!")
        
    except Exception as e:
        print(f"Could not load old parser: {e}")
    
    # Now test with fixed version
    print("\n" + "=" * 80)
    print("TEST 2: FIXED VerilogParser")
    print("=" * 80)
    
    # Copy fixed version to the right location
    if os.path.exists('/home/claude/VerilogParser_fixed.py'):
        with open('/home/claude/VerilogParser_fixed.py', 'r') as f:
            fixed_code = f.read()
        with open('/home/claude/VerilogParser.py', 'w') as f:
            f.write(fixed_code)
    
    from VerilogParser import VerilogParser
    
    # Parse both formats
    print("\nParsing Bench file...")
    bench_parser = BenchParser(bench_file.name)
    
    print("Parsing Verilog file...")
    verilog_parser = VerilogParser(verilog_file.name)
    
    # Get wire lists
    bench_wires = bench_parser.get_all_wires()
    verilog_wires = verilog_parser.get_all_wires()
    
    print("\n" + "=" * 80)
    print("WIRE LISTS")
    print("=" * 80)
    
    print(f"\nBenchParser wires ({len(bench_wires)}):")
    for i, wire in enumerate(bench_wires):
        print(f"  {i}: {wire}")
    
    print(f"\nVerilogParser wires ({len(verilog_wires)}):")
    for i, wire in enumerate(verilog_wires):
        print(f"  {i}: {wire}")
    
    # Check if they match
    print("\n" + "=" * 80)
    print("VALIDATION RESULTS")
    print("=" * 80)
    
    if bench_wires == verilog_wires:
        print("\n✅ SUCCESS! Wire lists are IDENTICAL")
        print("   Both parsers produce the same ordering")
        print("   Graph structures will match!")
    else:
        print("\n❌ FAILURE! Wire lists are DIFFERENT")
        print("\n   Differences:")
        
        bench_set = set(bench_wires)
        verilog_set = set(verilog_wires)
        
        only_bench = bench_set - verilog_set
        only_verilog = verilog_set - bench_set
        
        if only_bench:
            print(f"   Only in Bench: {only_bench}")
        if only_verilog:
            print(f"   Only in Verilog: {only_verilog}")
        
        if len(bench_wires) == len(verilog_wires):
            print("\n   Same count but different order:")
            for i, (b, v) in enumerate(zip(bench_wires, verilog_wires)):
                if b != v:
                    print(f"   Index {i}: Bench='{b}' vs Verilog='{v}'")
    
    # Build var maps
    print("\n" + "=" * 80)
    print("VARIABLE MAPPINGS")
    print("=" * 80)
    
    bench_var_map = bench_parser.build_var_map()
    verilog_var_map = verilog_parser.build_var_map()
    
    print("\nBenchParser var_map:")
    for wire in sorted(bench_var_map.keys()):
        print(f"  {wire} -> {bench_var_map[wire]}")
    
    print("\nVerilogParser var_map:")
    for wire in sorted(verilog_var_map.keys()):
        print(f"  {wire} -> {verilog_var_map[wire]}")
    
    # Check if var_maps match
    if bench_var_map == verilog_var_map:
        print("\n✅ SUCCESS! Variable mappings are IDENTICAL")
    else:
        print("\n❌ FAILURE! Variable mappings are DIFFERENT")
        
        all_wires = set(bench_var_map.keys()) | set(verilog_var_map.keys())
        for wire in sorted(all_wires):
            bench_id = bench_var_map.get(wire, 'N/A')
            verilog_id = verilog_var_map.get(wire, 'N/A')
            if bench_id != verilog_id:
                print(f"  {wire}: Bench={bench_id} vs Verilog={verilog_id}")
    
    # Test graph structure
    print("\n" + "=" * 80)
    print("GRAPH STRUCTURE (Edge Index)")
    print("=" * 80)
    
    # Simulate what VectorizedGraphExtractor does
    def build_edge_list(parser):
        edges = []
        var_map = parser.build_var_map()
        wires = parser.get_all_wires()
        name_to_idx = {name: i for i, name in enumerate(wires)}
        
        for out, _, inputs in parser.gates:
            if out in name_to_idx:
                dst = name_to_idx[out]
                for inp in inputs:
                    if inp in name_to_idx:
                        src = name_to_idx[inp]
                        edges.append([src, dst])
        
        return edges, name_to_idx
    
    bench_edges, bench_name_to_idx = build_edge_list(bench_parser)
    verilog_edges, verilog_name_to_idx = build_edge_list(verilog_parser)
    
    print("\nBenchParser edges:")
    for src, dst in bench_edges:
        src_name = bench_wires[src]
        dst_name = bench_wires[dst]
        print(f"  {src} → {dst}  ({src_name} → {dst_name})")
    
    print("\nVerilogParser edges:")
    for src, dst in verilog_edges:
        src_name = verilog_wires[src]
        dst_name = verilog_wires[dst]
        print(f"  {src} → {dst}  ({src_name} → {dst_name})")
    
    if bench_edges == verilog_edges:
        print("\n✅ SUCCESS! Edge lists are IDENTICAL")
        print("   Graph structures match perfectly!")
    else:
        print("\n❌ FAILURE! Edge lists are DIFFERENT")
        print(f"   Bench has {len(bench_edges)} edges")
        print(f"   Verilog has {len(verilog_edges)} edges")
    
    print("\n" + "=" * 80)
    print("FINAL VERDICT")
    print("=" * 80)
    
    if (bench_wires == verilog_wires and 
        bench_var_map == verilog_var_map and 
        bench_edges == verilog_edges):
        print("\n🎉 ALL TESTS PASSED! 🎉")
        print("\nThe fixed VerilogParser produces IDENTICAL graph structures")
        print("to BenchParser. Training and inference will use consistent")
        print("node orderings regardless of file format.")
    else:
        print("\n❌ TESTS FAILED!")
        print("\nParsers produce different structures.")
        print("This will cause graph mismatch between training and inference.")

finally:
    # Cleanup
    os.unlink(bench_file.name)
    os.unlink(verilog_file.name)
    
    # Restore backup if it exists
    if os.path.exists('/home/claude/VerilogParser_backup.py'):
        os.rename('/home/claude/VerilogParser_backup.py', '/home/claude/VerilogParser_old.py')

print("\n" + "=" * 80)