
===== BenchParser.py =====
"""
Shared Bench File Parser for DFT Analysis

This module provides a unified parser for .bench files that:
- Handles full-scan DFFs (Q output as PPI, D input as PPO)
- Tracks back edges for bidirectional graph traversal
- Provides a common data structure for both SAT and GNN models
"""

class BenchParser:
    """
    Unified parser for .bench format files with full-scan DFF support.
    
    For full-scan designs:
    - DFF outputs (Q) are treated as Pseudo Primary Inputs (PPIs)
    - DFF inputs (D) are treated as Pseudo Primary Outputs (PPOs)
    - The circuit is "broken" at flip-flops to eliminate cycles
    """
    
    def __init__(self, bench_file):
        self.bench_file = bench_file
        
        # Primary Inputs/Outputs
        self.inputs = []           # Primary Inputs (PIs)
        self.outputs = []          # Primary Outputs (POs)
        
        # Pseudo Inputs/Outputs (from DFFs)
        self.ppis = []             # Pseudo Primary Inputs (DFF Q outputs)
        self.ppos = []             # Pseudo Primary Outputs (DFF D inputs)
        
        # All inputs/outputs combined
        self.all_inputs = []       # PIs + PPIs
        self.all_outputs = []      # POs + PPOs
        
        # Gate structure
        self.gates = []            # List of (output, gate_type, inputs)
        self.gate_dict = {}        # Map: output_name -> (gate_type, inputs)
        
        # DFF tracking
        self.dffs = []             # List of (Q_output, D_input) tuples
        self.dff_map = {}          # Map: Q_output -> D_input
        
        # Back edges (for reverse traversal)
        self.back_edges = {}       # Map: input_wire -> [gates_it_drives]
        
        # Variable mapping (for SAT solver)
        self.var_map = {}          # Map: wire_name -> variable_id
        
        # Parse the file
        self._parse()
    
    def _parse(self):
        """Parse the bench file and populate all data structures."""
        with open(self.bench_file, 'r') as f:
            for line in f:
                line = line.strip()
                
                # Skip empty lines and comments
                if not line or line.startswith('#'):
                    continue
                
                # Parse INPUT declarations
                if line.startswith('INPUT'):
                    name = line[line.find('(')+1:line.find(')')]
                    self.inputs.append(name)
                    self.all_inputs.append(name)
                    
                # Parse OUTPUT declarations
                elif line.startswith('OUTPUT'):
                    name = line[line.find('(')+1:line.find(')')]
                    self.outputs.append(name)
                    self.all_outputs.append(name)
                    
                # Parse gate definitions
                elif '=' in line:
                    parts = line.split('=')
                    out = parts[0].strip()
                    rhs = parts[1].strip()
                    
                    # Extract gate type and inputs
                    g_type = rhs[:rhs.find('(')].strip().upper()
                    in_str = rhs[rhs.find('(')+1:-1]
                    inputs = [x.strip() for x in in_str.split(',')] if in_str else []
                    
                    # Handle DFFs specially (Full-Scan assumption)
                    if g_type == 'DFF':
                        # Q output (out) becomes a PPI
                        self.ppis.append(out)
                        self.all_inputs.append(out)
                        
                        # D input becomes a PPO
                        if len(inputs) > 0:
                            d_input = inputs[0]
                            self.ppos.append(d_input)
                            self.all_outputs.append(d_input)
                            
                            # Track the DFF relationship
                            self.dffs.append((out, d_input))
                            self.dff_map[out] = d_input
                        
                        # Note: DFFs are NOT added to self.gates
                        # This "breaks" the circuit at flip-flops
                    else:
                        # Regular combinational gate
                        self.gates.append((out, g_type, inputs))
                        self.gate_dict[out] = (g_type, inputs)
                        
                        # Build back edges for reverse traversal
                        for inp in inputs:
                            if inp not in self.back_edges:
                                self.back_edges[inp] = []
                            self.back_edges[inp].append(out)
        
        # Remove duplicates while preserving order
        self.all_inputs = list(dict.fromkeys(self.all_inputs))
        self.all_outputs = list(dict.fromkeys(self.all_outputs))
    
    def get_all_wires(self):
        """Get all wire names in the circuit (inputs, outputs, and internal)."""
        wires = set(self.all_inputs + self.all_outputs)
        for out, _, inputs in self.gates:
            wires.add(out)
            wires.update(inputs)
        return sorted(list(wires))
    
    def build_var_map(self):
        """Build variable mapping for SAT solver (1-indexed)."""
        if self.var_map:
            return self.var_map  # Already built
        
        next_var = 1
        for wire in self.get_all_wires():
            if wire not in self.var_map:
                self.var_map[wire] = next_var
                next_var += 1
        return self.var_map
    
    def get_fanout(self, wire_name):
        """Get all gates driven by a wire (forward edges)."""
        return self.back_edges.get(wire_name, [])
    
    def get_fanin(self, wire_name):
        """Get the gate driving a wire (backward edge)."""
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][1]  # Return inputs
        return []
    
    def is_pi(self, wire_name):
        """Check if wire is a Primary Input."""
        return wire_name in self.inputs
    
    def is_po(self, wire_name):
        """Check if wire is a Primary Output."""
        return wire_name in self.outputs
    
    def is_ppi(self, wire_name):
        """Check if wire is a Pseudo Primary Input (DFF Q)."""
        return wire_name in self.ppis
    
    def is_ppo(self, wire_name):
        """Check if wire is a Pseudo Primary Output (DFF D)."""
        return wire_name in self.ppos
    
    def is_dff_output(self, wire_name):
        """Check if wire is a DFF Q output."""
        return wire_name in self.dff_map
    
    def get_dff_input(self, q_output):
        """Get the D input for a DFF Q output."""
        return self.dff_map.get(q_output)
    
    def get_gate_type(self, wire_name):
        """Get the gate type that produces this wire."""
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][0]
        elif self.is_ppi(wire_name):
            return 'PPI'
        elif self.is_pi(wire_name):
            return 'INPUT'
        return None
    
    def __repr__(self):
        return (f"BenchParser({self.bench_file})\n"
                f"  PIs: {len(self.inputs)}, POs: {len(self.outputs)}\n"
                f"  PPIs: {len(self.ppis)}, PPOs: {len(self.ppos)}\n"
                f"  Gates: {len(self.gates)}, DFFs: {len(self.dffs)}")
===== UnifiedParser.py =====
"""
Unified Circuit Parser

Auto-detects file format (.bench or .v) and uses appropriate parser.
Provides a single interface for both formats.
"""

import os
from BenchParser import BenchParser
from VerilogParser import VerilogParser

class UnifiedParser:
    """
    Wrapper that auto-detects and delegates to BenchParser or VerilogParser.
    Provides identical API regardless of input format.
    """
    
    def __init__(self, circuit_file):
        self.circuit_file = circuit_file
        
        # Detect format and create appropriate parser
        if circuit_file.endswith('.bench'):
            self.parser = BenchParser(circuit_file)
            self.format = 'bench'
        elif circuit_file.endswith('.v') or circuit_file.endswith('.verilog'):
            self.parser = VerilogParser(circuit_file)
            self.format = 'verilog'
        else:
            # Try to detect by content
            self.format = self._detect_format_by_content(circuit_file)
            if self.format == 'bench':
                self.parser = BenchParser(circuit_file)
            elif self.format == 'verilog':
                self.parser = VerilogParser(circuit_file)
            else:
                raise ValueError(f"Unknown circuit format: {circuit_file}")
    
    def _detect_format_by_content(self, filepath):
        """Detect format by examining file content."""
        with open(filepath, 'r') as f:
            content = f.read(1000)  # Read first 1KB
        
        # Check for Verilog keywords
        if 'module' in content and 'endmodule' in content:
            return 'verilog'
        
        # Check for BENCH keywords
        if 'INPUT(' in content or 'OUTPUT(' in content:
            return 'bench'
        
        return None
    
    # Delegate all methods to underlying parser
    def __getattr__(self, name):
        return getattr(self.parser, name)
    
    def __repr__(self):
        return f"UnifiedParser({self.format}): {self.parser}"


# =============================================================================
# Usage Example: Works with BOTH formats transparently
# =============================================================================

if __name__ == "__main__":
    # Works with .bench files
    bench_parser = UnifiedParser("c17.bench")
    print(bench_parser)
    print("Gates:", len(bench_parser.gates))
    
    # Works with .v files  
    verilog_parser = UnifiedParser("circuit.v")
    print(verilog_parser)
    print("Gates:", len(verilog_parser.gates))
    
    # Same API for both!
    for parser in [bench_parser, verilog_parser]:
        print(f"\nInputs: {parser.inputs}")
        print(f"Outputs: {parser.outputs}")
        print(f"Gate count: {len(parser.gates)}")
        var_map = parser.build_var_map()
        print(f"Variables: {len(var_map)}")
===== VerilogParser.py =====
r"""
VerilogParser - Enhanced for Yosys Output

Handles:
- Yosys header comments
- Escaped identifiers (\a[0], \b[1], etc.)
- Yosys internal gates (\$_AND_, \$_OR_, etc.)
- Both named and positional port connections
"""

import re

class VerilogParser:
    """Parser for gate-level Verilog with BenchParser-compatible API."""
    
    GATE_MAPPINGS = {
        # Standard gates (lowercase)
        'and': 'AND', 'or': 'OR', 'not': 'NOT', 'nand': 'NAND',
        'nor': 'NOR', 'xor': 'XOR', 'xnor': 'XNOR', 'buf': 'BUFF',
        'buffer': 'BUFF', 'dff': 'DFF', 'DFF': 'DFF',
        
        # Yosys internal gates (with $_..._  format)
        '$_and_': 'AND', '$_or_': 'OR', '$_not_': 'NOT', '$_nand_': 'NAND',
        '$_nor_': 'NOR', '$_xor_': 'XOR', '$_xnor_': 'XNOR', '$_buf_': 'BUFF',
        '$_dff_': 'DFF', '$_dffe_': 'DFF',
        '$_mux_': 'MUX', '$_nmux_': 'NMUX',
        '$_aoi3_': 'AOI3', '$_oai3_': 'OAI3',
        '$_aoi4_': 'AOI4', '$_oai4_': 'OAI4',
    }
    
    def __init__(self, verilog_file):
        self.verilog_file = verilog_file
        
        # Data structures
        self.inputs = []
        self.outputs = []
        self.ppis = []
        self.ppos = []
        self.all_inputs = []
        self.all_outputs = []
        self.gates = []
        self.gate_dict = {}
        self.dffs = []
        self.dff_map = {}
        self.back_edges = {}
        self.var_map = {}
        self.wires = []
        
        self._parse()
    
    def _remove_comments(self, content):
        """Remove ALL comments including Yosys headers."""
        # Remove multi-line comments
        content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
        # Remove single-line comments
        content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)
        return content
    
    def _normalize_identifier(self, name):
        """
        Normalize Verilog identifiers.
        Removes escaped identifier backslash and trailing whitespace.
        
        Examples:
            \\a[0]  -> a[0]
            \\b[1]  -> b[1]
            normal  -> normal
        """
        name = name.strip()
        # Remove leading backslash for escaped identifiers
        if name.startswith('\\'):
            name = name[1:].strip()
        return name
    
    def _parse(self):
        """Parse Verilog file."""
        with open(self.verilog_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Remove comments FIRST
        content = self._remove_comments(content)
        
        # Extract modules
        modules = self._extract_modules(content)
        
        if not modules:
            raise ValueError("No module found in Verilog file")
        
        # Process first module
        module_content = modules[0]
        
        self._parse_ports(module_content)
        self._parse_wires(module_content)
        self._parse_instances(module_content)
        
        # Build combined lists
        self.all_inputs = list(dict.fromkeys(self.inputs + self.ppis))
        self.all_outputs = list(dict.fromkeys(self.outputs + self.ppos))
    
    def _extract_modules(self, content):
        """Extract module definitions."""
        # Handle both escaped and normal identifiers in module name and ports
        pattern = r'module\s+(\S+)\s*\((.*?)\);(.*?)endmodule'
        matches = re.findall(pattern, content, re.DOTALL)
        return [match[2] for match in matches]  # Return module bodies
    
    def _parse_ports(self, content):
        """Parse input/output declarations."""
        # Updated patterns to handle escaped identifiers
        input_pattern = r'input\s+(?:\[.*?\]\s+)?([^;]+);'
        output_pattern = r'output\s+(?:\[.*?\]\s+)?([^;]+);'
        
        for match in re.finditer(input_pattern, content):
            ports = match.group(1).split(',')
            for port in ports:
                name = self._normalize_identifier(port)
                name = re.sub(r'\[.*?\]', '', name).strip()
                if name and name not in self.inputs:
                    self.inputs.append(name)
        
        for match in re.finditer(output_pattern, content):
            ports = match.group(1).split(',')
            for port in ports:
                name = self._normalize_identifier(port)
                name = re.sub(r'\[.*?\]', '', name).strip()
                if name and name not in self.outputs:
                    self.outputs.append(name)
    
    def _parse_wires(self, content):
        """Parse wire declarations."""
        wire_pattern = r'wire\s+(?:\[.*?\]\s+)?([^;]+);'
        
        for match in re.finditer(wire_pattern, content):
            wires = match.group(1).split(',')
            for wire in wires:
                name = self._normalize_identifier(wire)
                name = re.sub(r'\[.*?\]', '', name).strip()
                if name and name not in self.wires:
                    self.wires.append(name)
    
    def _parse_instances(self, content):
        """Parse gate instances (handles both ICCAD positional and Yosys named formats)."""
        # Pattern to match gate instances:
        # - ICCAD: gate_type ( ports );
        # - Yosys: \gate_type instance_name ( ports );
        # Matches escaped identifiers starting with \ or regular identifiers
        
        # Identifier can be:
        # - Escaped: \$_NAND_ or \a[0] (backslash followed by non-whitespace)
        # - Regular: and, or, _inst123, etc. (word characters)
        instance_pattern = r'(\\[^\s]+|\w+)\s+(?:(\\[^\s]+|\w+)\s+)?\(\s*(.*?)\s*\)\s*;'
        
        for match in re.finditer(instance_pattern, content, re.DOTALL):
            gate_type_raw = match.group(1)
            inst_name = match.group(2) if match.group(2) else 'unnamed'
            port_list = match.group(3)
            
            # Normalize gate type (remove backslash, convert to lowercase)
            gate_type_normalized = self._normalize_identifier(gate_type_raw).lower()
            
            if gate_type_normalized not in self.GATE_MAPPINGS:
                continue
            
            gate_type = self.GATE_MAPPINGS[gate_type_normalized]
            ports = self._parse_port_connections(port_list)
            
            if not ports or len(ports) < 1:
                continue
            
            output = ports[0]
            inputs = ports[1:] if len(ports) > 1 else []
            
            if gate_type == 'DFF':
                self.ppis.append(output)
                if inputs:
                    self.ppos.append(inputs[0])
                    self.dffs.append((output, inputs[0]))
                    self.dff_map[output] = inputs[0]
            else:
                self.gates.append((output, gate_type, inputs))
                self.gate_dict[output] = (gate_type, inputs)
                
                for inp in inputs:
                    if inp not in self.back_edges:
                        self.back_edges[inp] = []
                    self.back_edges[inp].append(output)
    
    def _parse_port_connections(self, port_list):
        """Parse port connections (positional or named)."""
        ports = []
        port_list = port_list.strip()
        
        if '.(' in port_list or ('.' in port_list and '(' in port_list):
            # Named connections - handle escaped identifiers
            # Pattern: .PORT_NAME(\wire_name) or .PORT_NAME(wire_name)
            named_pattern = r'\.(\w+)\s*\(\s*([^)]+)\s*\)'
            connections = {}
            
            for match in re.finditer(named_pattern, port_list):
                port_name = match.group(1)
                wire_name = self._normalize_identifier(match.group(2))
                connections[port_name] = wire_name
            
            # Extract output first
            output_names = ['Y', 'Q', 'OUT', 'Z', 'O']
            for name in output_names:
                if name in connections:
                    ports.append(connections[name])
                    break
            
            # Extract inputs
            skip_ports = ['CLK', 'CLOCK', 'RST', 'RESET', 'SET', 'CLEAR', 'EN', 'ENABLE']
            for port_name, wire_name in connections.items():
                if port_name not in output_names and port_name not in skip_ports:
                    if wire_name not in ports:
                        ports.append(wire_name)
        else:
            # Positional connections
            wires = [self._normalize_identifier(w) for w in port_list.split(',')]
            ports = [w for w in wires if w and w != '1\'b0' and w != '1\'b1']
        
        return ports
    
    # =========================================================================
    # BenchParser-Compatible API
    # =========================================================================
    
    def get_all_wires(self):
        wires = set(self.all_inputs + self.all_outputs + self.wires)
        for out, _, inputs in self.gates:
            wires.add(out)
            wires.update(inputs)
        return sorted(list(wires))
    
    def build_var_map(self):
        if self.var_map:
            return self.var_map
        next_var = 1
        for wire in self.get_all_wires():
            if wire not in self.var_map:
                self.var_map[wire] = next_var
                next_var += 1
        return self.var_map
    
    def get_fanout(self, wire_name):
        return self.back_edges.get(wire_name, [])
    
    def get_fanin(self, wire_name):
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][1]
        return []
    
    def is_pi(self, wire_name):
        return wire_name in self.inputs
    
    def is_po(self, wire_name):
        return wire_name in self.outputs
    
    def is_ppi(self, wire_name):
        return wire_name in self.ppis
    
    def is_ppo(self, wire_name):
        return wire_name in self.ppos
    
    def is_dff_output(self, wire_name):
        return wire_name in self.dff_map
    
    def get_dff_input(self, q_output):
        return self.dff_map.get(q_output)
    
    def get_gate_type(self, wire_name):
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][0]
        elif self.is_ppi(wire_name):
            return 'PPI'
        elif self.is_pi(wire_name):
            return 'INPUT'
        return None
    
    def __repr__(self):
        return (f"VerilogParser({self.verilog_file})\n"
                f"  PIs: {len(self.inputs)}, POs: {len(self.outputs)}\n"
                f"  PPIs: {len(self.ppis)}, PPOs: {len(self.ppos)}\n"
                f"  Gates: {len(self.gates)}, DFFs: {len(self.dffs)}")
===== WireFaultMiter.py =====
import os
import random
from collections import deque
from pysat.solvers import Glucose3, Minisat22

# Support both formats
try:
    from UnifiedParser import UnifiedParser as Parser
except ImportError:
    from BenchParser import BenchParser as Parser

class WireFaultMiter:
    def __init__(self, circuit_file):
        """
        Initialize miter for fault detection.
        
        Args:
            circuit_file: Path to .bench or .v file (auto-detected)
        """
        self.circuit_file = circuit_file
        self.parser = Parser(circuit_file)  # Auto-detects format!
        
        self.inputs = self.parser.all_inputs
        self.outputs = self.parser.all_outputs
        self.gates = self.parser.gates
        self.var_map = self.parser.build_var_map()
        self.next_var = len(self.var_map) + 1
        self.faulty_map = {}
        self.scan_inputs = self.parser.ppis
        self.scan_outputs = self.parser.ppos

    def _get_var(self, name):
        if name not in self.var_map:
            self.var_map[name] = self.next_var
            self.next_var += 1
        return self.var_map[name]

    def build_miter(self, fault_wire, fault_type=None, force_diff=1):
        """
        Build miter circuit for fault detection.
        fault_type: 0 for SA0, 1 for SA1, None for no fault injection
        """
        clauses = []
        
        # --- 1. Good Circuit ---
        for out, g_type, inputs in self.gates:
            self._add_gate_clauses(clauses, self.var_map[out], g_type, [self.var_map[i] for i in inputs])
            
        # --- 2. Faulty Circuit ---
        self.faulty_map = {name: self.var_map[name] for name in self.inputs}
        for out, _, _ in self.gates:
            if out not in self.faulty_map:
                self.faulty_map[out] = self.next_var
                self.next_var += 1
                
        # Inject Fault
        if fault_wire in self.faulty_map and fault_type is not None:
            f_var = self.faulty_map[fault_wire]
            if fault_type == 1: 
                clauses.append([f_var])   # SA1
            elif fault_type == 0: 
                clauses.append([-f_var])  # SA0

        for out, g_type, inputs in self.gates:
            if out == fault_wire: 
                continue
            out_var = self.faulty_map[out]
            in_vars = [self.faulty_map.get(i) for i in inputs]
            if None in in_vars: 
                continue 
            self._add_gate_clauses(clauses, out_var, g_type, in_vars)

        # --- 3. Miter Comparator ---
        miter_out = self.next_var
        self.next_var += 1
        diff_vars = []
        unique_outputs = sorted(list(set(self.outputs)))
        
        for out in unique_outputs:
            if out not in self.var_map or out not in self.faulty_map: 
                continue
            good = self.var_map[out]
            bad = self.faulty_map[out]
            diff = self.next_var
            self.next_var += 1
            
            # XOR logic
            clauses.extend([
                [-good, -bad, -diff], 
                [good, bad, -diff], 
                [-good, bad, diff], 
                [good, -bad, diff]
            ])
            diff_vars.append(diff)
            
        # OR gate
        big_or = [-miter_out]
        for d in diff_vars:
            clauses.append([-d, miter_out])
            big_or.append(d)
        clauses.append(big_or)
        clauses.append([miter_out])  # Force miter output = 1
        
        return clauses

    def _add_gate_clauses(self, clauses, out, g_type, inputs):
        """Add CNF clauses for a gate."""
        if g_type == 'AND':
            for i in inputs: 
                clauses.append([-out, i])
            clauses.append([out] + [-i for i in inputs])
        elif g_type == 'OR':
            for i in inputs: 
                clauses.append([out, -i])
            clauses.append([-out] + inputs)
        elif g_type == 'NOT':
            clauses.append([-out, -inputs[0]])
            clauses.append([out, inputs[0]])
        elif g_type == 'NAND':
            for i in inputs: 
                clauses.append([out, i])
            clauses.append([-out] + [-i for i in inputs])
        elif g_type == 'NOR':
            for i in inputs: 
                clauses.append([-out, -i])
            clauses.append([out] + inputs)
        elif g_type == 'XOR':
            if len(inputs) == 2:
                a, b = inputs
                clauses.append([-out, -a, -b])
                clauses.append([-out, a, b])
                clauses.append([out, -a, b])
                clauses.append([out, a, -b])
            else:
                # Multi-input XOR (cascade pairwise)
                prev = inputs[0]
                for inp in inputs[1:-1]:
                    temp = self.next_var
                    self.next_var += 1
                    # temp = prev XOR inp
                    clauses.extend([
                        [-temp, -prev, -inp],
                        [-temp, prev, inp],
                        [temp, -prev, inp],
                        [temp, prev, -inp]
                    ])
                    prev = temp
                # Final XOR
                a, b = prev, inputs[-1]
                clauses.extend([
                    [-out, -a, -b],
                    [-out, a, b],
                    [out, -a, b],
                    [out, a, -b]
                ])
        elif g_type == 'XNOR':
            # XNOR = NOT(XOR)
            if len(inputs) == 2:
                a, b = inputs
                clauses.append([out, -a, -b])
                clauses.append([out, a, b])
                clauses.append([-out, -a, b])
                clauses.append([-out, a, -b])
        elif g_type == 'BUFF':
            clauses.append([-out, inputs[0]])
            clauses.append([out, -inputs[0]])
        else:
            # Unknown gate type - treat as buffer
            if len(inputs) > 0:
                clauses.append([-out, inputs[0]])
                clauses.append([out, -inputs[0]])
===== benchmark.py =====
"""
Benchmark GNN-Guided SAT Solving for Circuit Testability
"""

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'pysat'))

import time
import csv
import random
import numpy as np
import torch
from pysat.solvers import Minisat22, Glucose3, Cadical195, Glucose42, Gluecard4
from WireFaultMiter import WireFaultMiter
from neuro_utils import VectorizedGraphExtractor

# Import model from training script
from train_model import CircuitGNN_DualTask

# =============================================================================
# CONFIGS
# =============================================================================
BENCHMARK_DIR = "../hdl-benchmarks/iscas85/bench/"
MODEL_PATH = "gnn_model_dual_task_17feat.pth"
RESULTS_PATH = "results_gnn_guided.csv"
NUM_FAULTS_PER_CIRCUIT = 10
SEED = 42

def set_global_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

set_global_seed(SEED)

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def get_benchmark_files(benchmark_dir):
    """Get all .bench files from benchmark directory"""
    if not os.path.exists(benchmark_dir):
        return []
    
    files = []
    for root, dirs, filenames in os.walk(benchmark_dir):
        for f in filenames:
            if f.endswith(".bench"):
                full_path = os.path.join(root, f)
                rel_path = os.path.relpath(full_path, benchmark_dir)
                files.append(rel_path)
    
    return sorted(files)

# =============================================================================
# BENCHMARKING
# =============================================================================

def run_benchmark():
    """Main benchmarking function"""
    print("=" * 80)
    print("GNN-GUIDED SAT SOLVING BENCHMARK")
    print("=" * 80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # Load model
    if not os.path.exists(MODEL_PATH):
        print(f"ERROR: Model not found at {MODEL_PATH}")
        print("Please train the model first!")
        return
    
    print(f"Loading model from {MODEL_PATH}...")
    model = CircuitGNN_DualTask(num_node_features=17).to(device)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    print("Model loaded successfully")
    
    # Get benchmark files
    files = get_benchmark_files(BENCHMARK_DIR)
    if not files:
        print(f"ERROR: No benchmark files found in {BENCHMARK_DIR}")
        return
    
    print(f"Found {len(files)} benchmark circuits")
    print("=" * 80)
    
    results = []
    
    for filename in files:
        filepath = os.path.join(BENCHMARK_DIR, filename)
        print(f"\nProcessing {filename}...")
        
        try:
            miter = WireFaultMiter(filepath)
            num_gates = len(miter.gates)
            
            if num_gates == 0:
                print(f"  Skipping (no gates)")
                continue
            
            if num_gates > 10000:
                print(f"  Skipping (too large: {num_gates} gates)")
                continue
            
            print(f"  Gates: {num_gates}, Inputs: {len(miter.inputs)}")
            
            # Create extractor
            extractor = VectorizedGraphExtractor(filepath, var_map=miter.var_map, device=device.type)
            
            # Sample faults
            all_gates = sorted(miter.gates, key=lambda x: x[0])
            num_tests = min(NUM_FAULTS_PER_CIRCUIT, len(all_gates))
            
            for i in range(num_tests):
                target_gate = random.choice(all_gates)[0]
                fault_type = 1  # SA1
                
                # === BASELINE: Standard SAT solving ===
                clauses_std = miter.build_miter(target_gate, fault_type, 1)
                
                t_std_start = time.time()
                with Minisat22(bootstrap_with=clauses_std) as solver:
                    std_result = solver.solve()
                    std_conflicts = solver.accum_stats()['conflicts']
                std_time = time.time() - t_std_start
                
                # === GNN-GUIDED: With hints ===
                t_gnn_start = time.time()
                
                # 1. Get GNN predictions
                data = extractor.get_data_for_fault(target_gate, fault_type=fault_type).to(device)
                
                with torch.no_grad():
                    imp_scores, pol_scores = model(data)
                
                # 2. Build phase hints (sorted by importance)
                hints = []
                for idx, name in enumerate(data.node_names):
                    if name in miter.inputs:
                        imp = imp_scores[idx].item()
                        prob = pol_scores[idx].item()
                        var_id = miter.var_map.get(name)
                        
                        if var_id:
                            signed_lit = var_id if prob > 0.5 else -var_id
                            hints.append((signed_lit, imp, var_id))
                
                # Sort by importance (descending), then by var_id (for determinism)
                hints.sort(key=lambda x: (-x[1], x[2]))
                hint_literals = [h[0] for h in hints]
                
                # 3. Solve with phase hints
                clauses_gnn = miter.build_miter(target_gate, fault_type, 1)
                
                with Minisat22(bootstrap_with=clauses_gnn) as solver:
                    if hint_literals:
                        solver.set_phases(hint_literals)
                    
                    gnn_result = solver.solve()
                    gnn_conflicts = solver.accum_stats()['conflicts']
                
                gnn_time = time.time() - t_gnn_start
                
                # Calculate speedup
                speedup = std_conflicts / max(gnn_conflicts, 1)
                status = "SAT" if gnn_result else "UNSAT"
                
                print(f"  Fault {i+1}/{num_tests} ({target_gate}): "
                      f"{std_conflicts} â†’ {gnn_conflicts} conflicts ({speedup:.2f}x speedup)")
                
                # Record results
                results.append({
                    "Circuit": filename,
                    "Fault": target_gate,
                    "Status": status,
                    "Std_Conflicts": std_conflicts,
                    "GNN_Conflicts": gnn_conflicts,
                    "Speedup": speedup,
                    "Std_Time_s": std_time,
                    "GNN_Time_s": gnn_time
                })
        
        except Exception as e:
            print(f"  Error: {e}")
            continue
    
    # Save results
    if results:
        print("\n" + "=" * 80)
        print("BENCHMARK COMPLETE")
        print("=" * 80)
        
        with open(RESULTS_PATH, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)
        
        # Print summary statistics
        total_tests = len(results)
        avg_speedup = sum(r['Speedup'] for r in results) / total_tests
        max_speedup = max(r['Speedup'] for r in results)
        min_speedup = min(r['Speedup'] for r in results)
        
        speedups = [r['Speedup'] for r in results]
        median_speedup = sorted(speedups)[len(speedups) // 2]
        
        # Count improvements
        improved = sum(1 for r in results if r['Speedup'] > 1.0)
        improved_pct = (improved / total_tests) * 100
        
        print(f"Total tests: {total_tests}")
        print(f"Tests improved: {improved} ({improved_pct:.1f}%)")
        print(f"\nSpeedup Statistics:")
        print(f"  Average:  {avg_speedup:.2f}x")
        print(f"  Median:   {median_speedup:.2f}x")
        print(f"  Min:      {min_speedup:.2f}x")
        print(f"  Max:      {max_speedup:.2f}x")
        print(f"\nResults saved to: {RESULTS_PATH}")
        print("=" * 80)
    else:
        print("\nNo results generated!")

if __name__ == "__main__":
    run_benchmark()
===== data_generation_hybrid.py =====
"""
HYBRID DATA GENERATION: Structural + Selective SAT Probing
5-10x faster than full probing with 85% quality
"""

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'pysat'))

import time
import random
import numpy as np
import torch
import torch.multiprocessing as mp
from collections import deque
from pysat.solvers import Glucose3
from tqdm import tqdm
from WireFaultMiter import WireFaultMiter
from neuro_utils import VectorizedGraphExtractor

# =============================================================================
# CONFIGS
# =============================================================================
GENERATE_TRAIN_DATA_DIR = "../output"
DATASET_PATH = "dataset_hybrid_17feat.pt"
SAMPLES_PER_FILE = 20
MAX_PROBES = 5  # Only probe top-5 inputs
CONFLICT_BUDGET = 2000
SKIP_SIZE_THRESHOLD = 10000  # Skip circuits larger than this
NUM_WORKERS = 8
SEED = 42

def set_global_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)

set_global_seed(SEED)

# =============================================================================
# HYBRID IMPORTANCE COMPUTATION
# =============================================================================

def compute_hybrid_importance(miter, target_gate, assignment, max_probes=5):
    """
    Hybrid approach: Structural ranking + selective SAT probing
    
    Returns:
        importance: dict {input_name: importance_score}
        polarity: dict {input_name: polarity_value}
    """
    parser = miter.parser
    
    # Step 1: Fast structural ranking (BFS from fault)
    structural_scores = {}
    distances = {}
    queue = deque([target_gate])
    distances[target_gate] = 0
    visited = {target_gate}
    
    while queue:
        node = queue.popleft()
        dist = distances[node]
        
        if node in parser.gate_dict:
            _, inputs = parser.gate_dict[node]
            for inp in inputs:
                if inp not in visited:
                    visited.add(inp)
                    distances[inp] = dist + 1
                    queue.append(inp)
    
    # Rank inputs by distance (closer = more important)
    for inp in miter.inputs:
        if inp in distances:
            structural_scores[inp] = 1.0 / (distances[inp] + 1)
        else:
            structural_scores[inp] = 0.0
    
    # Step 2: Select top-K inputs for SAT probing
    sorted_inputs = sorted(structural_scores.items(), key=lambda x: -x[1])
    probe_candidates = [inp for inp, _ in sorted_inputs[:max_probes]]
    
    # Step 3: Probe selected inputs
    sat_importance = {}
    polarity = {}
    
    clauses = miter.build_miter(target_gate, 1, 1)  # SA1 fault
    
    with Glucose3(bootstrap_with=clauses) as probe_solver:
        base_conflicts = probe_solver.accum_stats()['conflicts']
        
        for inp in probe_candidates:
            if inp not in miter.var_map:
                continue
            
            var_id = miter.var_map[inp]
            correct_val = var_id if var_id in assignment else -var_id
            
            # Try flipping the input
            probe_solver.conf_budget(500)
            result = probe_solver.solve(assumptions=[-correct_val])
            
            new_conflicts = probe_solver.accum_stats()['conflicts']
            delta = new_conflicts - base_conflicts
            base_conflicts = new_conflicts
            
            if result:
                sat_importance[inp] = delta
            else:
                sat_importance[inp] = 5000  # UNSAT = Critical input
            
            polarity[inp] = 1.0 if var_id in assignment else 0.0
    
    # Step 4: Calibrate structural scores using SAT results
    if sat_importance:
        sat_values = [sat_importance[inp] for inp in probe_candidates if inp in sat_importance]
        struct_values = [structural_scores[inp] for inp in probe_candidates if inp in sat_importance]
        
        if sat_values and struct_values:
            # Compute scaling factor
            sat_max = max(sat_values)
            struct_max = max(struct_values)
            scale_factor = sat_max / max(struct_max, 0.001)
            
            # Extrapolate to all inputs
            importance = {}
            for inp in miter.inputs:
                if inp in sat_importance:
                    # Use actual SAT value
                    importance[inp] = sat_importance[inp]
                else:
                    # Estimate from structural score
                    importance[inp] = structural_scores[inp] * scale_factor
                
                # Set polarity
                if inp not in polarity:
                    if inp in miter.var_map:
                        var_id = miter.var_map[inp]
                        polarity[inp] = 1.0 if var_id in assignment else 0.0
                    else:
                        polarity[inp] = 0.5
            
            return importance, polarity
    
    # Fallback: Use pure structural scores
    importance = structural_scores
    polarity = {}
    for inp in miter.inputs:
        if inp in miter.var_map:
            var_id = miter.var_map[inp]
            polarity[inp] = 1.0 if var_id in assignment else 0.0
        else:
            polarity[inp] = 0.5
    
    return importance, polarity

# =============================================================================
# CIRCUIT PROCESSING
# =============================================================================

def process_circuit_hybrid(filename):
    """Process one circuit with hybrid importance computation"""
    set_global_seed(SEED + hash(filename) % 10000)
    
    filepath = os.path.join(GENERATE_TRAIN_DATA_DIR, filename)
    local_dataset = []
    
    try:
        miter = WireFaultMiter(filepath)
        num_gates = len(miter.gates)
        
        if num_gates == 0:
            return []
        
        # Skip very large circuits
        if num_gates > SKIP_SIZE_THRESHOLD:
            print(f"[{filename}] SKIP: {num_gates} gates (too large)", flush=True)
            return []
        
        # Adaptive sampling
        if num_gates > 5000:
            samples = 5
            probes = 3
        elif num_gates > 1000:
            samples = 10
            probes = 5
        else:
            samples = SAMPLES_PER_FILE
            probes = MAX_PROBES
        
        extractor = VectorizedGraphExtractor(filepath, var_map=miter.var_map, device='cpu')
        
        print(f"[{filename}] Processing {samples} faults (probing top-{probes})...", flush=True)
        
        # Sample faults
        if len(miter.gates) <= samples:
            sampled_faults = miter.gates
        else:
            sampled_faults = random.sample(miter.gates, samples)
        
        successful = 0
        
        for target_gate, _, _ in sampled_faults:
            fault_type = 1  # SA1
            
            # Build and solve miter
            clauses = miter.build_miter(target_gate, fault_type, 1)
            
            with Glucose3(bootstrap_with=clauses) as solver:
                solver.conf_budget(CONFLICT_BUDGET)
                if not solver.solve():
                    continue
                assignment = solver.get_model()
                if not assignment:
                    continue
            
            # HYBRID: Structural + selective probing
            importance, polarity = compute_hybrid_importance(
                miter, target_gate, set(assignment), max_probes=probes
            )
            
            if not importance:
                continue
            
            # Build training sample
            data = extractor.get_data_for_fault(target_gate, fault_type=fault_type)
            max_imp = max(importance.values()) if importance else 1.0
            
            y_polarity = torch.zeros(len(data.node_names), 1)
            y_importance = torch.zeros(len(data.node_names), 1)
            train_mask = torch.zeros(len(data.node_names), 1)
            
            for k, node_name in enumerate(data.node_names):
                if node_name in importance:
                    y_polarity[k] = polarity[node_name]
                    y_importance[k] = importance[node_name] / max(max_imp, 1.0)
                    train_mask[k] = 1.0
            
            data.y_polarity = y_polarity
            data.y_importance = y_importance
            data.train_mask = train_mask
            local_dataset.append(data)
            successful += 1
        
        print(f"[{filename}] Done: {successful}/{samples} successful", flush=True)
    
    except Exception as e:
        print(f"[{filename}] Error: {e}", flush=True)
    
    return local_dataset

# =============================================================================
# DATASET GENERATION
# =============================================================================

def get_target_files(DIR):
    """Get all .bench files recursively"""
    if not os.path.exists(DIR): 
        return []
    
    file_list = []
    for root, dirs, files in os.walk(DIR):
        for f in files:
            if f.endswith(".bench") or f.endswith(".v"):
                full_path = os.path.join(root, f)
                rel_path = os.path.relpath(full_path, DIR)
                file_list.append(rel_path)
    
    # Sort by size (smaller first for better load balancing)
    return sorted(file_list, key=lambda x: os.path.getsize(os.path.join(DIR, x)))

def generate_dataset():
    """Main dataset generation function"""
    print("=" * 80)
    print("HYBRID DATA GENERATION (Structural + Selective Probing)")
    print("=" * 80)
    print(f"Directory: {GENERATE_TRAIN_DATA_DIR}")
    print(f"Max probes per fault: {MAX_PROBES}")
    print(f"Samples per file: {SAMPLES_PER_FILE}")
    print(f"Workers: {NUM_WORKERS}")
    print("=" * 80)
    
    files = get_target_files(GENERATE_TRAIN_DATA_DIR)
    
    if not files:
        print(f"ERROR: No .bench or .v files found in {GENERATE_TRAIN_DATA_DIR}")
        return
    
    print(f"Found {len(files)} benchmark files")
    
    dataset = []
    
    # Setup multiprocessing
    try:
        mp.set_start_method('spawn', force=True)
    except:
        pass
    
    # Parallel processing
    start_time = time.time()
    
    with mp.Pool(processes=NUM_WORKERS) as pool:
        results = list(tqdm(
            pool.imap_unordered(process_circuit_hybrid, files),
            total=len(files),
            desc="Processing circuits"
        ))
        
        for res in results:
            dataset.extend(res)
    
    elapsed = time.time() - start_time
    
    print("\n" + "=" * 80)
    print("GENERATION COMPLETE")
    print("=" * 80)
    print(f"Total samples: {len(dataset)}")
    print(f"Time elapsed: {elapsed/60:.2f} minutes")
    print(f"Samples per minute: {len(dataset)/(elapsed/60):.1f}")
    print("=" * 80)
    
    if dataset:
        torch.save(dataset, DATASET_PATH)
        print(f"Saved to: {DATASET_PATH}")
    else:
        print("WARNING: No samples generated!")

if __name__ == "__main__":
    generate_dataset()
===== data_train_bench_mem_efficient.py =====
"""
Complete pipeline for Dual-Task GNN training with Parallel Generation & Polarity Guidance.
Uses Cone Splitting and Branch-less Stem Optimization for Large Circuits.
"""

import os
import sys

# Add local PySAT if needed
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'pysat'))

import time
import csv
import math
import torch
import torch.nn as nn
from torch_geometric.nn import GATv2Conv
import torch.optim as optim
import random
import numpy as np
from pysat.solvers import Glucose3, Minisat22
from pysat.formula import CNF
from tqdm import tqdm
from torch_geometric.loader import DataLoader
from torch_geometric.data import Dataset
from WireFaultMiter import WireFaultMiter
from BenchParser import BenchParser
from torch_geometric.data import Data
import torch.multiprocessing as mp

# IMPORT THE EXTRACTOR
from neuro_utils import VectorizedGraphExtractor

# =============================================================================
# CONFIGS
# =============================================================================
BENCHMARK_DIR = "../hdl-benchmarks/iscas85/bench/"
DATASET_PATH = "dataset_oracle_dual_17feat.pt"
SAMPLES_PER_FILE = 50
MODEL_PATH = "gnn_model_dual_task_17feat.pth"
EPOCHS = 20
BATCH_SIZE = 32
GENERATE_TRAIN_DATA_DIR = "../I99T"
SEED = 42

# =============================================================================
# 0. DETERMINISM SETUP
# =============================================================================
def set_global_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)

set_global_seed(SEED)

# =============================================================================
# PART 1: OPTIMIZED PARALLEL DATA GENERATION
# =============================================================================

def get_target_files(DIR):
    if not os.path.exists(DIR): return []
    file_list = []
    for root, dirs, files in os.walk(DIR):
        for f in files:
            if f.endswith(".bench"):
                full_path = os.path.join(root, f)
                rel_path = os.path.relpath(full_path, DIR)
                file_list.append(rel_path)
    return sorted(file_list, key=lambda x: os.path.getsize(os.path.join(DIR, x)), reverse=True)

def process_single_circuit(filename):
    """Worker passing fault_type=1 to extractor"""
    set_global_seed(SEED + len(filename)) 
    filepath = os.path.join(GENERATE_TRAIN_DATA_DIR, filename)
    local_dataset = []
    try:
        miter = WireFaultMiter(filepath)
        if not miter.gates: return []
        local_samples = 10 if len(miter.gates) > 5000 else SAMPLES_PER_FILE
        print(f"[{filename}] Mining {local_samples} samples (Cone Optimized)...", flush=True)
        extractor = VectorizedGraphExtractor(filepath, var_map=miter.parser.build_var_map(), device='cpu')
        
        for i in range(local_samples):
            # 1. Pick a Fault
            target_gate = random.choice(miter.gates)[0]
            outs = miter.get_reachable_outputs(target_gate)
            if not outs: continue
            
            # --- FIX FOR HANG ON B17/B19 ---
            # Don't check all outputs. Sample 20 random ones.
            sample_outs = random.sample(outs, min(20, len(outs)))
            candidates = [(len(miter.get_logic_cone([o], target_gate)), o) for o in sample_outs]
            candidates.sort()
            target_out = candidates[0][1]
            
            cone_gates = miter.get_logic_cone([target_out], target_gate)
            
            orig_gates = miter.gates; miter.gates = cone_gates
            
            # --- FAULT TYPE IS 1 (SA1) ---
            FAULT_TYPE = 1 
            clauses = miter.build_miter(target_gate, None, FAULT_TYPE) 
            miter.gates = orig_gates
            
            with Glucose3(bootstrap_with=clauses) as solver:
                solver.conf_budget(5000)
                if solver.solve():
                    model = solver.get_model()
                    cone_inputs = set([i for _,_,inps in cone_gates for i in inps if i in miter.inputs])
                    probe_list = list(cone_inputs)[:50]
                    
                    with Glucose3(bootstrap_with=clauses) as probe:
                        base = probe.accum_stats()['conflicts']
                        input_importance = {}
                        input_polarity = {} 
                        for inp in probe_list:
                            if inp not in miter.var_map: continue
                            vid = miter.var_map[inp]
                            val = vid if vid in model else -vid
                            probe.conf_budget(500)
                            res = probe.solve(assumptions=[-val])
                            new_c = probe.accum_stats()['conflicts']
                            input_importance[inp] = (new_c - base) if res else 2000
                            input_polarity[inp] = 1.0 if vid in model else 0.0
                            base = new_c
                    
                    if input_importance:
                        # --- PASS FAULT TYPE HERE ---
                        data = extractor.get_data_for_fault(target_gate, fault_type=FAULT_TYPE)
                        max_imp = max(input_importance.values())
                        y_pol = torch.zeros(len(data.node_names), 1)
                        y_imp = torch.zeros(len(data.node_names), 1)
                        mask = torch.zeros(len(data.node_names), 1)
                        for k, n in enumerate(data.node_names):
                            if n in input_importance:
                                y_pol[k] = input_polarity[n]
                                y_imp[k] = input_importance[n] / max(max_imp, 1)
                                mask[k] = 1.0
                        data.y_polarity = y_pol
                        data.y_importance = y_imp
                        data.train_mask = mask
                        local_dataset.append(data)
                        
                        # Progress print
                        if i % 2 == 0: 
                            print(f"[{filename}] {i+1}/{local_samples} done.", flush=True)

    except Exception as e:
        print(f"[{filename}] Error: {e}")
    return local_dataset

def generate_dataset():
    files = get_target_files(GENERATE_TRAIN_DATA_DIR)
    dataset = []
    try: mp.set_start_method('spawn', force=True)
    except: pass
    with mp.Pool(min(4, os.cpu_count())) as pool:
        results = list(tqdm(pool.imap_unordered(process_single_circuit, files), total=len(files)))
        for res in results: dataset.extend(res)
    torch.save(dataset, DATASET_PATH)

# =============================================================================
# PART 2 & 3: MODEL AND TRAINING
# =============================================================================

class CircuitGNN_DualTask(torch.nn.Module):
    # --- UPDATED INPUT DIM TO 17 ---
    def __init__(self, num_node_features=17, num_layers=20, hidden_dim=64, dropout=0.2):
        super(CircuitGNN_DualTask, self).__init__()
        self.convs = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()
        self.convs.append(GATv2Conv(num_node_features, hidden_dim, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        for _ in range(num_layers - 2):
            self.convs.append(GATv2Conv(hidden_dim, hidden_dim, heads=2, concat=False))
            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        self.convs.append(GATv2Conv(hidden_dim, 32, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(32))
        self.importance_head = torch.nn.Linear(32, 1)
        self.polarity_head = torch.nn.Linear(32, 1)
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = torch.relu(self.bns[0](self.convs[0](x, edge_index)))
        for i in range(1, len(self.convs) - 1):
            x = x + torch.relu(self.bns[i](self.convs[i](x, edge_index)))
        x = torch.relu(self.bns[-1](self.convs[-1](x, edge_index)))
        return self.importance_head(x), torch.sigmoid(self.polarity_head(x))

def train_model():
    print("--- Training Dual-Task GNN ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    if not os.path.exists(DATASET_PATH):
        generate_dataset()

    dataset = torch.load(DATASET_PATH, weights_only=False)
    loader = DataLoader(dataset[:int(len(dataset)*0.8)],
                        batch_size=BATCH_SIZE,
                        shuffle=True)

    model = CircuitGNN_DualTask(num_node_features=17).to(device)

    # LOAD MODEL IF IT EXISTS
    if os.path.exists(MODEL_PATH):
        print(f"Loading existing model from {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    else:
        print("No existing model found. Training from scratch.")

    optimizer = optim.Adam(model.parameters(), lr=0.001)
    crit_imp = nn.MSELoss(reduction='none')
    crit_pol = nn.BCELoss(reduction='none')

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0.0

        for batch in loader:
            batch = batch.to(device)
            optimizer.zero_grad()

            p_imp, p_pol = model(batch)
            mask = batch.train_mask
            mask_sum = mask.sum().clamp(min=1)

            l_imp = (crit_imp(p_imp, batch.y_importance) * mask).sum() / mask_sum
            l_pol = (crit_pol(p_pol, batch.y_polarity) * mask).sum() / mask_sum

            (l_imp + l_pol).backward()
            optimizer.step()
            total_loss += (l_imp + l_pol).item()

        print(f"Epoch {epoch+1}: {total_loss/len(loader):.4f}")

    torch.save(model.state_dict(), MODEL_PATH)
    print(f"Model saved to {MODEL_PATH}")


# =============================================================================
# PART 4: DETERMINISTIC BENCHMARKING (FIXED WITH OPTIMIZATION)
# =============================================================================

def run_benchmark():
    print(f"--- BENCHMARKING (Cone Split + Branchless + GNN) ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # --- UPDATE MODEL CALL TO 17 ---
    model = CircuitGNN_DualTask(num_node_features=17).to(device)
    if os.path.exists(MODEL_PATH): model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    
    results = []
    files = get_target_files(BENCHMARK_DIR)
    random.seed(SEED) 
    
    for filename in files:
        filepath = os.path.join(BENCHMARK_DIR, filename)
        print(f"\nProcessing {filename}...")
        try:
            miter = WireFaultMiter(filepath)
            if not miter.gates: continue
            extractor = VectorizedGraphExtractor(filepath, var_map=miter.var_map, device=device.type)
            
            all_gates = sorted(miter.gates, key=lambda x: x[0])
            for i in range(10): 
                target_gate = random.choice(all_gates)[0]
                FAULT_TYPE = 1 # SA1
                
                # --- GNN INFERENCE ---
                t0 = time.time()
                # Pass fault type 1
                data = extractor.get_data_for_fault(target_gate, fault_type=FAULT_TYPE).to(device)
                with torch.no_grad(): imp, pol = model(data)
                
                hints = {}
                for idx, name in enumerate(data.node_names):
                    if name in miter.inputs: hints[name] = pol[idx].item()
                
                # --- 1. BASELINE RUN (No Hints) ---
                _, conf_std = miter.solve_fault_specific_cones(target_gate, FAULT_TYPE, gnn_hints=None)
                
                # --- 2. GNN RUN (With Hints) ---
                assign_gnn, conf_gnn = miter.solve_fault_specific_cones(target_gate, FAULT_TYPE, gnn_hints=hints)
                dur = time.time() - t0
                
                spd = conf_std / max(conf_gnn, 1)
                status = "DETECTED" if assign_gnn else "UNSAT"
                
                if assign_gnn:
                    vec_str = "".join([str(assign_gnn.get(k, 'X')) for k in sorted(miter.inputs)])
                    print(f"  {target_gate}: {status} | Conf: {conf_std} -> {conf_gnn} ({spd:.2f}x)")
                    print(f"  Vector: {vec_str[:30]}...")
                else:
                    vec_str = "UNSAT"
                    print(f"  {target_gate}: {status} | Conf: {conf_std} -> {conf_gnn} ({spd:.2f}x)")
                
                results.append({
                    "Circuit": filename, "Fault": target_gate, "Speedup": spd,
                    "Std_Conf": conf_std, "GNN_Conf": conf_gnn, "Status": status,
                    "Vector": vec_str
                })
        except Exception as e:
            print(f"Error: {e}")

    if results:
        with open("results_optimized.csv", 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)

if __name__ == "__main__":
    if len(sys.argv) < 2: print("Usage: python data_train_bench_mem_efficient.py [generate|train|benchmark]")
    else:
        cmd = sys.argv[1]
        if cmd == "generate": generate_dataset()
        elif cmd == "train": train_model()
        elif cmd == "benchmark": run_benchmark()
===== flatten_verilog.py =====
#!/usr/bin/env python3
"""
Recursive Verilog Gate-Level Netlist Generator (IMPROVED)

This version actually produces gate-level netlists with gate instances,
not just wire assignments.

Usage:
    python gate_level_synth.py <input_dir> <output_dir> [--liberty <lib_file>]
"""

import os
import sys
import re
import shutil
import subprocess
from pathlib import Path
from tqdm import tqdm
import multiprocessing as mp

def synthesize_to_gates(input_file, output_file, liberty_file=None):
    """
    Synthesize Verilog to ACTUAL gate-level netlist using Yosys.
    
    This version ensures you get gate instances, not just wire assignments.
    """
    
    if liberty_file and os.path.exists(liberty_file):
        # Technology-mapped gate-level synthesis with standard cells
        script = f"""
# Read input design
read_verilog {input_file}

# Elaborate design
hierarchy -check -auto-top

# Convert processes to netlists
proc

# Optimize
opt

# Map flip-flops to library cells
dfflibmap -liberty {liberty_file}

# Technology mapping with ABC
abc -liberty {liberty_file}

# Final cleanup
opt_clean

# Write gate-level netlist
write_verilog -noattr -noexpr {output_file}
"""
    else:
        # Generic gate-level synthesis using Yosys internal cells
        # This WILL produce actual gate instances
        script = f"""
# Read input design
read_verilog {input_file}

# Elaborate design
hierarchy -check -auto-top

# Convert to gate level (this is the key difference)
proc; opt; fsm; opt; memory; opt

# Flatten design
flatten

# Map to coarse cells
techmap; opt

# Technology mapping with ABC (uses generic gates)
abc -g AND,NAND,OR,NOR,XOR,XNOR,ANDNOT,ORNOT

# Map remaining cells to gates
techmap; opt

# Final optimization
opt_clean -purge

# Write gate-level netlist
write_verilog -noattr -noexpr {output_file}
"""
    
    script_file = output_file + ".ys"
    
    try:
        with open(script_file, 'w') as f:
            f.write(script)
        
        result = subprocess.run(
            ['yosys', '-s', script_file],
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if result.returncode == 0 and os.path.exists(output_file):
            # Validate output
            with open(output_file, 'r') as f:
                content = f.read()
            
            # Remove Yosys header
            if content.startswith('/* Generated by Yosys'):
                end = content.find('*/')
                if end != -1:
                    content = content[end + 2:].lstrip()
            
            # Check if file has actual module
            test = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
            test = re.sub(r'//.*?$', '', test, flags=re.MULTILINE)
            test = test.strip()
            
            if not test or 'module' not in test:
                # Empty file - delete and fail
                if os.path.exists(output_file):
                    os.remove(output_file)
                if os.path.exists(script_file):
                    os.remove(script_file)
                return False
            
            # Verify it's actually gate-level (has gate instances or assigns)
            # Gate-level should have either gate instances or structural assigns
            has_gates = bool(re.search(r'\s+(AND|OR|NOT|NAND|NOR|XOR|XNOR|BUF|MUX|DFF|DFFE)\s+', content, re.IGNORECASE))
            has_instances = bool(re.search(r'^\s*\w+\s+\w+\s*\(', content, re.MULTILINE))
            
            if not (has_gates or has_instances):
                # Might still be valid structural Verilog with assigns
                # Keep it anyway as it's at least flattened
                pass
            
            # Write cleaned content
            with open(output_file, 'w') as f:
                f.write(content)
            
            if os.path.exists(script_file):
                os.remove(script_file)
            return True
        else:
            if os.path.exists(script_file):
                os.remove(script_file)
            return False
            
    except (FileNotFoundError, subprocess.TimeoutExpired, Exception) as e:
        if os.path.exists(script_file):
            os.remove(script_file)
        return False


def find_verilog_files(root_dir):
    """Find ALL .v files recursively."""
    verilog_files = []
    root_path = Path(root_dir).resolve()
    
    for dirpath, _, filenames in os.walk(root_path):
        for filename in filenames:
            if filename.endswith(('.v', '.verilog', '.sv')):
                abs_path = Path(dirpath) / filename
                rel_path = abs_path.relative_to(root_path)
                verilog_files.append((str(rel_path), str(abs_path)))
    
    return sorted(verilog_files)


def process_single_file(args):
    """Worker function for parallel processing."""
    rel_path, input_path, output_dir, liberty_file = args
    
    output_path = Path(output_dir) / rel_path
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    success = synthesize_to_gates(input_path, str(output_path), liberty_file)
    
    return (rel_path, success)


def remove_empty_directories(root_dir):
    """Remove empty directories recursively from nested to parent."""
    removed_count = 0
    root_path = Path(root_dir).resolve()
    
    for dirpath, dirnames, filenames in os.walk(root_path, topdown=False):
        if Path(dirpath).resolve() == root_path:
            continue
            
        try:
            if not os.listdir(dirpath):
                os.rmdir(dirpath)
                removed_count += 1
        except OSError:
            pass
    
    return removed_count


def synthesize_directory_tree(input_dir, output_dir, liberty_file=None, num_workers=None):
    """Main function to recursively synthesize Verilog files to gate-level."""
    
    print("=" * 80)
    print("RECURSIVE VERILOG GATE-LEVEL SYNTHESIZER v2.0")
    print("=" * 80)
    print(f"Input:   {input_dir}")
    print(f"Output:  {output_dir}")
    if liberty_file:
        print(f"Library: {liberty_file}")
        if not os.path.exists(liberty_file):
            print(f"âš ï¸  WARNING: Liberty file not found! Using generic gates.")
            liberty_file = None
    else:
        print("Library: Generic gates (ABC with AND/OR/NOT/NAND/NOR/XOR/XNOR)")
    print("=" * 80)
    
    print("\nðŸ” Scanning directory tree...")
    verilog_files = find_verilog_files(input_dir)
    
    if not verilog_files:
        print(f"âŒ No Verilog files found in {input_dir}")
        return
    
    print(f"âœ… Found {len(verilog_files)} Verilog files\n")
    
    os.makedirs(output_dir, exist_ok=True)
    
    if num_workers is None:
        num_workers = min(os.cpu_count() or 4, 8)
    
    print(f"ðŸš€ Processing with {num_workers} parallel workers...\n")
    
    tasks = [(rel, abs_path, output_dir, liberty_file) 
             for rel, abs_path in verilog_files]
    
    successful = 0
    failed = 0
    failed_files = []
    
    try:
        mp.set_start_method('spawn', force=True)
    except:
        pass
    
    with mp.Pool(processes=num_workers) as pool:
        for rel_path, success in tqdm(
            pool.imap_unordered(process_single_file, tasks),
            total=len(tasks),
            desc="Synthesizing to gates"
        ):
            if success:
                successful += 1
            else:
                failed += 1
                failed_files.append(rel_path)
    
    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)
    print(f"Total files:      {len(verilog_files)}")
    print(f"âœ… Successful:    {successful}")
    print(f"âŒ Failed:        {failed}")
    print(f"Success rate:     {successful/len(verilog_files)*100:.1f}%")
    print("=" * 80)
    
    if failed > 0:
        print(f"\nâš ï¸  {failed} files failed")
        print("First 10 failed files:")
        for f in failed_files[:10]:
            print(f"  - {f}")
        if len(failed_files) > 10:
            print(f"  ... and {len(failed_files) - 10} more")
    
    # Clean up empty directories
    print("\nðŸ§¹ Cleaning up empty directories...")
    removed = remove_empty_directories(output_dir)
    if removed > 0:
        print(f"âœ… Removed {removed} empty directories")
    else:
        print("âœ… No empty directories to remove")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Recursively synthesize Verilog to ACTUAL gate-level netlists",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generic gate-level (AND, OR, NOT, NAND, NOR, XOR, XNOR gates)
  python gate_level_synth.py ./input ./output
  
  # Technology-mapped with standard cell library
  python gate_level_synth.py ./input ./output --liberty sky130.lib
  
  # Custom worker count
  python gate_level_synth.py ./input ./output --workers 16

Output will contain actual gate instances like:
  AND _001_ (.A(n1), .B(n2), .Y(n3));
  OR _002_ (.A(n3), .B(n4), .Y(n5));
  DFF _003_ (.D(n5), .Q(n6), .CLK(clk));
        """
    )
    
    parser.add_argument('input_dir', help='Input directory')
    parser.add_argument('output_dir', help='Output directory')
    parser.add_argument('--liberty', type=str, default=None,
                       help='Liberty (.lib) file for technology mapping')
    parser.add_argument('--workers', type=int, default=None,
                       help='Number of parallel workers')
    
    args = parser.parse_args()
    
    if not os.path.isdir(args.input_dir):
        print(f"âŒ Error: {args.input_dir} does not exist")
        sys.exit(1)
    
    synthesize_directory_tree(
        args.input_dir,
        args.output_dir,
        liberty_file=args.liberty,
        num_workers=args.workers
    )


if __name__ == "__main__":
    main()
===== main.py =====
"""
Main Pipeline for GNN-Guided SAT Solving

Usage:
    python main.py generate  # Generate training data
    python main.py train     # Train the model
    python main.py benchmark # Run benchmarks
    python main.py all       # Run complete pipeline
"""

import sys
import os

def print_usage():
    print(__doc__)

def main():
    if len(sys.argv) < 2:
        print_usage()
        return
    
    command = sys.argv[1].lower()
    
    if command == "generate":
        print("Starting data generation...")
        from data_generation_hybrid import generate_dataset
        generate_dataset()
        
    elif command == "train":
        print("Starting model training...")
        from train_model import train_model
        train_model()
        
    elif command == "benchmark":
        print("Starting benchmark...")
        from benchmark import run_benchmark
        run_benchmark()
        
    elif command == "all":
        print("Running complete pipeline...")
        print("\n" + "="*80)
        print("STEP 1: Data Generation")
        print("="*80)
        from data_generation_hybrid import generate_dataset
        generate_dataset()
        
        print("\n" + "="*80)
        print("STEP 2: Model Training")
        print("="*80)
        from train_model import train_model
        train_model()
        
        print("\n" + "="*80)
        print("STEP 3: Benchmarking")
        print("="*80)
        from benchmark import run_benchmark
        run_benchmark()
        
        print("\n" + "="*80)
        print("PIPELINE COMPLETE!")
        print("="*80)
        
    else:
        print(f"Unknown command: {command}")
        print_usage()

if __name__ == "__main__":
    main()
===== neuro_utils.py =====
import torch
import math
from torch_geometric.data import Data
from BenchParser import BenchParser

class VectorizedGraphExtractor:
    """
    High-Performance SCOAP Extractor using Vectorized Tensor Operations.
    Generates 17-dimensional feature vectors (16 Base + 1 Target Value).
    """
    
    TYPE_MAP = {
        'INPUT': 0, 'PPI': 0, 
        'BUFF': 1, 'NOT': 2,
        'AND': 3, 'NAND': 4,
        'OR': 5, 'NOR': 6,
        'XOR': 7, 'XNOR': 7
    }

    def __init__(self, bench_path, var_map=None, device='cpu'):
        self.parser = BenchParser(bench_path)
        self.device = device
        
        if var_map:
            self.var_map = var_map
        else:
            self.var_map = self.parser.build_var_map()
            
        self.ordered_names = sorted(self.var_map.keys(), key=lambda k: self.var_map[k])
        self.name_to_idx = {name: i for i, name in enumerate(self.ordered_names)}
        self.num_nodes = len(self.ordered_names)
        
        # Build structural tensors
        self.edges_list = []
        self.node_types = torch.zeros(self.num_nodes, dtype=torch.long, device=device)
        
        for name, g_type, _ in self.parser.gates:
            if name in self.name_to_idx:
                self.node_types[self.name_to_idx[name]] = self.TYPE_MAP.get(g_type, 1)
        
        for pi in self.parser.inputs:
            if pi in self.name_to_idx: 
                self.node_types[self.name_to_idx[pi]] = self.TYPE_MAP['INPUT']
        for ppi in self.parser.ppis:
            if ppi in self.name_to_idx: 
                self.node_types[self.name_to_idx[ppi]] = self.TYPE_MAP['INPUT']
        
        for out, _, inputs in self.parser.gates:
            if out in self.name_to_idx:
                dst = self.name_to_idx[out]
                for inp in inputs:
                    if inp in self.name_to_idx:
                        src = self.name_to_idx[inp]
                        self.edges_list.append([src, dst])
        
        if self.edges_list:
            self.edge_index = torch.tensor(self.edges_list, dtype=torch.long, device=device).t().contiguous()
        else:
            self.edge_index = torch.zeros((2, 0), dtype=torch.long, device=device)
            
        self.masks = {}
        for t_name, t_id in self.TYPE_MAP.items():
            self.masks[t_name] = (self.node_types == t_id)

        self.adj = [[] for _ in range(self.num_nodes)]
        self.parents = [[] for _ in range(self.num_nodes)]
        for src, dst in self.edges_list:
            self.adj[src].append(dst)
            self.parents[dst].append(src)
            
        self.cc0, self.cc1, self.co = self._compute_scoap_vectorized()
        self.x_base = self._build_base_features()

    def _compute_scoap_vectorized(self):
        """Vectorized SCOAP: Forward Controllability & Backward Observability"""
        num_nodes = self.num_nodes
        src_idx, dst_idx = self.edge_index
        
        cc0 = torch.ones(num_nodes, device=self.device)
        cc1 = torch.ones(num_nodes, device=self.device)
        
        mask_and = self.masks['AND'] | self.masks['NAND']
        mask_or  = self.masks['OR'] | self.masks['NOR']
        mask_inv = self.masks['NAND'] | self.masks['NOR'] | self.masks['NOT']
        mask_xor = self.masks['XOR']
        mask_buf_not = self.masks['BUFF'] | self.masks['NOT']
        
        for _ in range(50): 
            cc0_prev, cc1_prev = cc0.clone(), cc1.clone()
            edge_cc0 = cc0[src_idx]
            edge_cc1 = cc1[src_idx]
            
            min_cc0 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(
                0, dst_idx, edge_cc0, reduce='min', include_self=False)
            min_cc1 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(
                0, dst_idx, edge_cc1, reduce='min', include_self=False)
            sum_cc0 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc0)
            sum_cc1 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc1)
            
            cc0[mask_and] = min_cc0[mask_and] + 1
            cc1[mask_and] = sum_cc1[mask_and] + 1
            cc0[mask_or] = sum_cc0[mask_or] + 1
            cc1[mask_or] = min_cc1[mask_or] + 1
            cc0[mask_buf_not] = min_cc0[mask_buf_not] + 1
            cc1[mask_buf_not] = min_cc1[mask_buf_not] + 1
            cc0[mask_xor] = torch.minimum(sum_cc0[mask_xor], sum_cc1[mask_xor]) + 1
            cc1[mask_xor] = torch.maximum(min_cc0[mask_xor], min_cc1[mask_xor]) + 1
            
            temp_cc0 = cc0.clone()
            cc0[mask_inv] = cc1[mask_inv]
            cc1[mask_inv] = temp_cc0[mask_inv]
            
            mask_input = self.masks['INPUT']
            cc0[mask_input] = 1.0
            cc1[mask_input] = 1.0
            
            if torch.allclose(cc0, cc0_prev) and torch.allclose(cc1, cc1_prev):
                break

        co = torch.full((num_nodes,), 1e6, device=self.device)
        output_indices = [self.name_to_idx[n] for n in self.parser.all_outputs if n in self.name_to_idx]
        if output_indices:
            co[torch.tensor(output_indices, device=self.device)] = 0.0
        
        gate_cc0_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc0[src_idx])
        gate_cc1_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc1[src_idx])
        gate_min_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(
            0, dst_idx, torch.minimum(cc0[src_idx], cc1[src_idx]))

        for _ in range(50):
            co_prev = co.clone()
            co_dst = co[dst_idx]
            dst_types = self.node_types[dst_idx]
            side_costs = torch.zeros_like(co_dst)
            
            is_and = (dst_types == self.TYPE_MAP['AND']) | (dst_types == self.TYPE_MAP['NAND'])
            side_costs[is_and] = gate_cc1_sum[dst_idx][is_and] - cc1[src_idx][is_and]
            is_or = (dst_types == self.TYPE_MAP['OR']) | (dst_types == self.TYPE_MAP['NOR'])
            side_costs[is_or] = gate_cc0_sum[dst_idx][is_or] - cc0[src_idx][is_or]
            is_xor = (dst_types == self.TYPE_MAP['XOR'])
            side_costs[is_xor] = gate_min_sum[dst_idx][is_xor] - torch.minimum(cc0[src_idx], cc1[src_idx])[is_xor]
            
            path_costs = co_dst + side_costs + 1
            new_co = torch.zeros_like(co).scatter_reduce_(
                0, src_idx, path_costs, reduce='min', include_self=False)
            co = torch.minimum(co, new_co)
            if torch.allclose(co, co_prev):
                break
        
        return cc0, cc1, co

    def _compute_depth_fast(self, reverse=False):
        """Vectorized Topological Depth"""
        d_vals = torch.zeros(self.num_nodes, device=self.device)
        src_idx, dst_idx = self.edge_index
        prop_src = dst_idx if reverse else src_idx
        prop_dst = src_idx if reverse else dst_idx
        
        for _ in range(50):
            changed = False
            src_depths = d_vals[prop_src]
            new_depths = torch.zeros(self.num_nodes, device=self.device).scatter_reduce_(
                0, prop_dst, src_depths, reduce='amax', include_self=True)
            new_depths = new_depths + 1
            if not torch.allclose(d_vals, new_depths):
                d_vals = new_depths
                changed = True
            if not changed: 
                break
        
        max_d = d_vals.max() if d_vals.max() > 0 else 1.0
        return (d_vals / max_d).unsqueeze(1)

    def _build_base_features(self):
        """Builds 16 Base Features (without target value)"""
        x_type = torch.nn.functional.one_hot(self.node_types, num_classes=8).float()
        fwd_depth = self._compute_depth_fast(reverse=False)
        rev_depth = self._compute_depth_fast(reverse=True)
        
        f_cc0 = torch.log(self.cc0 + 1).unsqueeze(1) / 10.0
        f_cc1 = torch.log(self.cc1 + 1).unsqueeze(1) / 10.0
        f_co  = torch.log(self.co + 1).unsqueeze(1) / 10.0
        
        is_output = torch.zeros((self.num_nodes, 1), device=self.device)
        for name in self.parser.all_outputs:
            if name in self.name_to_idx: 
                is_output[self.name_to_idx[name]] = 1.0
        
        zeros = torch.zeros((self.num_nodes, 2), device=self.device)
        
        return torch.cat([x_type, fwd_depth, rev_depth, zeros, f_cc0, f_cc1, f_co, is_output], dim=1)

    def get_data_for_fault(self, fault_name, fault_type=1):
        """
        Generate Data object with 17 features.
        fault_type: 1 for SA1 (Target=0), 0 for SA0 (Target=1).
        """
        x = self.x_base.clone()
        tid = self.name_to_idx.get(fault_name)
        
        # 17th Feature: Target Value
        target_feat = torch.full((self.num_nodes, 1), 0.5, device=self.device)
        
        if tid is not None:
            x[tid, 10] = 1.0  # Fault location marker
            target_feat[tid] = 0.0 if fault_type == 1 else 1.0
            
            # BFS Distance
            dist = torch.full((self.num_nodes,), -1.0, device=self.device)
            dist[tid] = 0.0
            queue = [tid]
            visited = {tid: 0}
            idx = 0
            
            while idx < len(queue):
                u = queue[idx]
                idx += 1
                d = visited[u]
                if d >= 10: 
                    continue
                
                neighbors = self.adj[u] + self.parents[u]
                for v in neighbors:
                    if v not in visited:
                        visited[v] = d + 1
                        dist[v] = d + 1
                        queue.append(v)
            
            mask_visited = (dist != -1)
            if mask_visited.any():
                max_d = dist.max()
                if max_d == 0: 
                    max_d = 1.0
                x[mask_visited, 11] = 1.0 - (dist[mask_visited] / max_d)
        
        x = torch.cat([x, target_feat], dim=1)
        return Data(x=x, edge_index=self.edge_index, node_names=self.ordered_names)
===== pipeline.py =====
"""
Complete pipeline for Dual-Task GNN training with Parallel Generation & Polarity Guidance.
Uses Cone Splitting and Branch-less Stem Optimization for Large Circuits.
"""

import os
import sys

# Add local PySAT if needed
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'pysat'))

import time
import csv
import math
import torch
import torch.nn as nn
from torch_geometric.nn import GATv2Conv
import torch.optim as optim
import random
import numpy as np
from pysat.solvers import Glucose3, Minisat22
from pysat.formula import CNF
from tqdm import tqdm
from torch_geometric.loader import DataLoader
from torch_geometric.data import Dataset
from WireFaultMiter import WireFaultMiter
from BenchParser import BenchParser
from torch_geometric.data import Data
import torch.multiprocessing as mp

# =============================================================================
# EXTRACTOR (Defined here to ensure completeness)
# =============================================================================
class VectorizedGraphExtractor:
    """
    High-Performance SCOAP Extractor using Vectorized Tensor Operations.
    Generates 16-dimensional feature vectors including Observability.
    """
    
    # Gate Type Mapping
    TYPE_MAP = {
        'INPUT': 0, 'PPI': 0, 
        'BUFF': 1, 'NOT': 2,
        'AND': 3, 'NAND': 4,
        'OR': 5, 'NOR': 6,
        'XOR': 7, 'XNOR': 7
    }

    def __init__(self, bench_path, var_map=None, device='cpu'):
        self.parser = BenchParser(bench_path)
        self.device = device
        
        # 1. Build Name Mappings (Sync with Miter if var_map provided)
        if var_map:
            self.var_map = var_map
        else:
            self.var_map = self.parser.build_var_map()
            
        self.ordered_names = sorted(self.var_map.keys(), key=lambda k: self.var_map[k])
        self.name_to_idx = {name: i for i, name in enumerate(self.ordered_names)}
        self.num_nodes = len(self.ordered_names)
        
        # 2. Build Structural Tensors
        self.edges_list = []
        self.node_types = torch.zeros(self.num_nodes, dtype=torch.long, device=device)
        
        # Assign Gate Types
        for name, g_type, _ in self.parser.gates:
            if name in self.name_to_idx:
                idx = self.name_to_idx[name]
                self.node_types[idx] = self.TYPE_MAP.get(g_type, 1) # Default to BUFF
        
        # Overwrite Types for Inputs/PPIs
        for pi in self.parser.inputs:
            if pi in self.name_to_idx:
                self.node_types[self.name_to_idx[pi]] = self.TYPE_MAP['INPUT']
        for ppi in self.parser.ppis:
            if ppi in self.name_to_idx:
                self.node_types[self.name_to_idx[ppi]] = self.TYPE_MAP['INPUT']
        
        # Build Edge List (Source -> Dest)
        for out, _, inputs in self.parser.gates:
            if out in self.name_to_idx:
                dst = self.name_to_idx[out]
                for inp in inputs:
                    if inp in self.name_to_idx:
                        src = self.name_to_idx[inp]
                        self.edges_list.append([src, dst])
        
        # Create Edge Index Tensor
        if self.edges_list:
            self.edge_index = torch.tensor(self.edges_list, dtype=torch.long, device=device).t().contiguous()
        else:
            self.edge_index = torch.zeros((2, 0), dtype=torch.long, device=device)
            
        # Create Boolean Masks for Vectorized Logic
        self.masks = {}
        for t_name, t_id in self.TYPE_MAP.items():
            self.masks[t_name] = (self.node_types == t_id)

        # Pre-build Python adjacency for BFS traversals (Distance calculation)
        self.adj = [[] for _ in range(self.num_nodes)]       # Forward: src -> [dst]
        self.parents = [[] for _ in range(self.num_nodes)]  # Backward: dst -> [src]
        
        for src, dst in self.edges_list:
            self.adj[src].append(dst)
            self.parents[dst].append(src)
            
        # 3. Compute Metrics Immediately
        self.cc0, self.cc1, self.co = self._compute_scoap_vectorized()
        self.x_base = self._build_base_features()

    def _compute_scoap_vectorized(self):
        """Vectorized SCOAP: Forward Controllability & Backward Observability"""
        num_nodes = self.num_nodes
        src_idx, dst_idx = self.edge_index
        
        # --- Part A: Controllability (Forward) ---
        cc0 = torch.ones(num_nodes, device=self.device)
        cc1 = torch.ones(num_nodes, device=self.device)
        
        mask_and = self.masks['AND'] | self.masks['NAND']
        mask_or  = self.masks['OR'] | self.masks['NOR']
        mask_inv = self.masks['NAND'] | self.masks['NOR'] | self.masks['NOT']
        mask_xor = self.masks['XOR']
        mask_buf_not = self.masks['BUFF'] | self.masks['NOT']
        
        for _ in range(50): 
            cc0_prev, cc1_prev = cc0.clone(), cc1.clone()
            
            edge_cc0 = cc0[src_idx]
            edge_cc1 = cc1[src_idx]
            
            # Aggregate per Gate (Destination)
            min_cc0 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(
                0, dst_idx, edge_cc0, reduce='min', include_self=False)
            min_cc1 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(
                0, dst_idx, edge_cc1, reduce='min', include_self=False)
            
            sum_cc0 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc0)
            sum_cc1 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc1)
            
            # Apply Logic
            cc0[mask_and] = min_cc0[mask_and] + 1
            cc1[mask_and] = sum_cc1[mask_and] + 1
            
            cc0[mask_or] = sum_cc0[mask_or] + 1
            cc1[mask_or] = min_cc1[mask_or] + 1
            
            cc0[mask_buf_not] = min_cc0[mask_buf_not] + 1
            cc1[mask_buf_not] = min_cc1[mask_buf_not] + 1
            
            cc0[mask_xor] = torch.minimum(sum_cc0[mask_xor], sum_cc1[mask_xor]) + 1
            cc1[mask_xor] = torch.maximum(min_cc0[mask_xor], min_cc1[mask_xor]) + 1

            # Inversions
            temp_cc0 = cc0.clone()
            cc0[mask_inv] = cc1[mask_inv]
            cc1[mask_inv] = temp_cc0[mask_inv]
            
            # Reset Inputs
            mask_input = self.masks['INPUT']
            cc0[mask_input] = 1.0
            cc1[mask_input] = 1.0
            
            if torch.allclose(cc0, cc0_prev) and torch.allclose(cc1, cc1_prev):
                break

        # --- Part B: Observability (Backward) ---
        co = torch.full((num_nodes,), 1e6, device=self.device)
        
        output_indices = [self.name_to_idx[n] for n in self.parser.all_outputs if n in self.name_to_idx]
        if output_indices:
            co[torch.tensor(output_indices, device=self.device)] = 0.0

        gate_cc0_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc0[src_idx])
        gate_cc1_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc1[src_idx])
        gate_min_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(
            0, dst_idx, torch.minimum(cc0[src_idx], cc1[src_idx]))

        for _ in range(50):
            co_prev = co.clone()
            
            co_dst = co[dst_idx]
            dst_types = self.node_types[dst_idx]
            side_costs = torch.zeros_like(co_dst)
            
            # Side input logic
            is_and = (dst_types == self.TYPE_MAP['AND']) | (dst_types == self.TYPE_MAP['NAND'])
            side_costs[is_and] = gate_cc1_sum[dst_idx][is_and] - cc1[src_idx][is_and]
            
            is_or = (dst_types == self.TYPE_MAP['OR']) | (dst_types == self.TYPE_MAP['NOR'])
            side_costs[is_or] = gate_cc0_sum[dst_idx][is_or] - cc0[src_idx][is_or]
            
            is_xor = (dst_types == self.TYPE_MAP['XOR'])
            side_costs[is_xor] = gate_min_sum[dst_idx][is_xor] - torch.minimum(cc0[src_idx], cc1[src_idx])[is_xor]
            
            path_costs = co_dst + side_costs + 1
            
            new_co = torch.zeros_like(co).scatter_reduce_(
                0, src_idx, path_costs, reduce='min', include_self=False
            )
            
            co = torch.minimum(co, new_co)
            
            if torch.allclose(co, co_prev):
                break
                
        return cc0, cc1, co

    def _compute_depth_fast(self, reverse=False):
        """Vectorized Topological Depth"""
        d_vals = torch.zeros(self.num_nodes, device=self.device)
        src_idx, dst_idx = self.edge_index
        prop_src = dst_idx if reverse else src_idx
        prop_dst = src_idx if reverse else dst_idx
        
        for _ in range(50):
            changed = False
            src_depths = d_vals[prop_src]
            new_depths = torch.zeros(self.num_nodes, device=self.device).scatter_reduce_(
                0, prop_dst, src_depths, reduce='amax', include_self=True
            )
            new_depths = new_depths + 1
            if not torch.allclose(d_vals, new_depths):
                d_vals = new_depths
                changed = True
            if not changed: break
            
        max_d = d_vals.max() if d_vals.max() > 0 else 1.0
        return (d_vals / max_d).unsqueeze(1)

    def _build_base_features(self):
        """
        Builds 16-dimensional feature matrix
        [0-7]: Type, [8-9]: Depth, [10-11]: Fault, [12-14]: SCOAP, [15]: Output
        """
        x_type = torch.nn.functional.one_hot(self.node_types, num_classes=8).float()
        fwd_depth = self._compute_depth_fast(reverse=False)
        rev_depth = self._compute_depth_fast(reverse=True)
        
        f_cc0 = torch.log(self.cc0 + 1).unsqueeze(1) / 10.0
        f_cc1 = torch.log(self.cc1 + 1).unsqueeze(1) / 10.0
        f_co  = torch.log(self.co + 1).unsqueeze(1) / 10.0
        
        is_output = torch.zeros((self.num_nodes, 1), device=self.device)
        for name in self.parser.all_outputs:
            if name in self.name_to_idx:
                is_output[self.name_to_idx[name]] = 1.0
                
        zeros = torch.zeros((self.num_nodes, 2), device=self.device)
        
        return torch.cat([x_type, fwd_depth, rev_depth, zeros, f_cc0, f_cc1, f_co, is_output], dim=1)

    def get_data_for_fault(self, fault_name):
        """Generate Data object for a specific fault"""
        x = self.x_base.clone()
        tid = self.name_to_idx.get(fault_name)
        
        if tid is not None:
            x[tid, 10] = 1.0 # Mark target
            
            # BFS for Distance (Index 11)
            dist = torch.full((self.num_nodes,), -1.0, device=self.device)
            dist[tid] = 0.0
            queue = [tid]
            visited = {tid: 0}
            idx = 0
            
            while idx < len(queue):
                u = queue[idx]
                idx += 1
                d = visited[u]
                if d >= 10: continue
                
                neighbors = self.adj[u] + self.parents[u]
                for v in neighbors:
                    if v not in visited:
                        visited[v] = d + 1
                        dist[v] = d + 1
                        queue.append(v)
            
            mask_visited = (dist != -1)
            if mask_visited.any():
                max_d = dist.max()
                if max_d == 0: max_d = 1.0
                x[mask_visited, 11] = 1.0 - (dist[mask_visited] / max_d)
                
        return Data(x=x, edge_index=self.edge_index, node_names=self.ordered_names)

# =============================================================================
# CONFIGS
# =============================================================================
BENCHMARK_DIR = "../hdl-benchmarks/iscas85/bench/"
DATASET_PATH = "dataset_oracle_dual_16feat.pt"
SAMPLES_PER_FILE = 50
MODEL_PATH = "gnn_model_dual_task_16feat.pth"
EPOCHS = 20
BATCH_SIZE = 32
GENERATE_TRAIN_DATA_DIR = "../I99T"
SEED = 42

# =============================================================================
# 0. DETERMINISM SETUP
# =============================================================================
def set_global_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)

set_global_seed(SEED)

# =============================================================================
# PART 1: OPTIMIZED PARALLEL DATA GENERATION (CONE BASED)
# =============================================================================

def get_target_files(DIR):
    if not os.path.exists(DIR): return []
    file_list = []
    for root, dirs, files in os.walk(DIR):
        for f in files:
            if f.endswith(".bench"):
                full_path = os.path.join(root, f)
                rel_path = os.path.relpath(full_path, DIR)
                file_list.append(rel_path)
    # Sort largest first for better scheduling
    return sorted(file_list, key=lambda x: os.path.getsize(os.path.join(DIR, x)), reverse=True)

def process_single_circuit(filename):
    """Worker using CONE SPLITTING to handle Large Circuits."""
    set_global_seed(SEED + len(filename)) 
    filepath = os.path.join(GENERATE_TRAIN_DATA_DIR, filename)
    local_dataset = []

    try:
        miter = WireFaultMiter(filepath)
        if not miter.gates: return []
        
        # Adjust samples based on size (Giant circuits get fewer samples, but higher quality)
        num_gates = len(miter.gates)
        local_samples = 10 if num_gates > 5000 else SAMPLES_PER_FILE
        
        extractor = VectorizedGraphExtractor(filepath, var_map=miter.parser.build_var_map(), device='cpu')
        
        print(f"[{filename}] Mining {local_samples} samples (Cone Optimized)...", flush=True)

        for i in range(local_samples):
            # 1. Pick a Fault
            target_gate = random.choice(miter.gates)[0]
            
            # 2. Extract a Valid Cone (Instead of solving whole circuit)
            outs = miter.get_reachable_outputs(target_gate)
            if not outs: continue
            
            target_out = random.choice(outs)
            cone_gates = miter.get_logic_cone([target_out], target_gate)
            
            # 3. Swap Gates & Build Miter
            orig_gates = miter.gates
            miter.gates = cone_gates
            
            clauses = miter.build_miter(target_gate, None, 1) # SA1
            miter.gates = orig_gates # Restore immediately
            
            with Glucose3(bootstrap_with=clauses) as solver:
                solver.conf_budget(5000)
                if not solver.solve(): continue
                
                model = solver.get_model()
                if not model: continue
                
                # 4. Probe Inputs (Only those in the Cone!)
                cone_inputs = set()
                for _, _, inps in cone_gates:
                    for inp in inps:
                        if inp in miter.inputs: cone_inputs.add(inp)
                
                probe_list = list(cone_inputs)
                if len(probe_list) > 50: probe_list = random.sample(probe_list, 50)
                
                with Glucose3(bootstrap_with=clauses) as probe:
                    base_conf = probe.accum_stats()['conflicts']
                    input_importance = {}
                    input_polarity = {}
                    
                    for input_name in probe_list:
                        if input_name not in miter.var_map: continue
                        var_id = miter.var_map[input_name]
                        
                        correct = var_id if var_id in model else -var_id
                        probe.conf_budget(500)
                        res = probe.solve(assumptions=[-correct])
                        new_conf = probe.accum_stats()['conflicts']
                        
                        imp = new_conf - base_conf
                        base_conf = new_conf
                        
                        input_importance[input_name] = imp if res else 2000
                        input_polarity[input_name] = 1.0 if var_id in model else 0.0

                    # 5. Build Data Object
                    if input_importance:
                        data = extractor.get_data_for_fault(target_gate)
                        max_imp = max(input_importance.values())
                        
                        y_pol = torch.zeros(len(data.node_names), 1)
                        y_imp = torch.zeros(len(data.node_names), 1)
                        mask = torch.zeros(len(data.node_names), 1)
                        
                        for k, node in enumerate(data.node_names):
                            if node in input_importance:
                                y_pol[k] = input_polarity[node]
                                y_imp[k] = input_importance[node] / max(max_imp, 1)
                                mask[k] = 1.0
                        
                        data.y_polarity = y_pol
                        data.y_importance = y_imp
                        data.train_mask = mask
                        local_dataset.append(data)

    except Exception as e:
        print(f"[{filename}] Error: {e}", flush=True)
        return []

    return local_dataset

def generate_dataset():
    print(f"--- MINING DATA (CONE OPTIMIZED) ---")
    if not os.path.exists(GENERATE_TRAIN_DATA_DIR): return

    files = get_target_files(GENERATE_TRAIN_DATA_DIR)
    dataset = []
    
    try: mp.set_start_method('spawn', force=True)
    except: pass

    with mp.Pool(min(4, os.cpu_count())) as pool:
        results = list(tqdm(pool.imap_unordered(process_single_circuit, files), total=len(files)))
        for res in results:
            dataset.extend(res)

    torch.save(dataset, DATASET_PATH)


# =============================================================================
# PART 2 & 3: MODEL AND TRAINING (UNCHANGED)
# =============================================================================

class CircuitGNN_DualTask(torch.nn.Module):
    def __init__(self, num_node_features=16, num_layers=20, hidden_dim=64, dropout=0.2):
        super(CircuitGNN_DualTask, self).__init__()
        self.dropout = dropout
        self.num_layers = num_layers
        self.convs = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()
        self.convs.append(GATv2Conv(num_node_features, hidden_dim, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        for _ in range(num_layers - 2):
            self.convs.append(GATv2Conv(hidden_dim, hidden_dim, heads=2, concat=False))
            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        self.convs.append(GATv2Conv(hidden_dim, 32, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(32))
        self.importance_head = torch.nn.Linear(32, 1)
        self.polarity_head = torch.nn.Linear(32, 1)
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.convs[0](x, edge_index)
        x = self.bns[0](x)
        x = torch.nn.functional.elu(x)
        for i in range(1, self.num_layers - 1):
            identity = x
            x = self.convs[i](x, edge_index)
            x = self.bns[i](x)
            x = torch.nn.functional.elu(x)
            x = x + identity
        x = self.convs[-1](x, edge_index)
        x = self.bns[-1](x)
        x = torch.nn.functional.elu(x)
        return self.importance_head(x), torch.sigmoid(self.polarity_head(x))

def train_model():
    print("--- Training Dual-Task GNN ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if not os.path.exists(DATASET_PATH): 
        print("Dataset not found. Generating...")
        generate_dataset()
    
    dataset = torch.load(DATASET_PATH, weights_only=False)
    train_loader = DataLoader(dataset[:int(len(dataset)*0.8)], batch_size=BATCH_SIZE, shuffle=True)
    
    model = CircuitGNN_DualTask(num_node_features=16).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    crit_imp = nn.MSELoss(reduction='none')
    crit_pol = nn.BCELoss(reduction='none')
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            p_imp, p_pol = model(batch)
            mask = batch.train_mask
            mask_sum = mask.sum().clamp(min=1)
            l_imp = (crit_imp(p_imp, batch.y_importance) * mask).sum() / mask_sum
            l_pol = (crit_pol(p_pol, batch.y_polarity) * mask).sum() / mask_sum
            (l_imp + l_pol).backward()
            optimizer.step()
            total_loss += (l_imp + l_pol).item()
        print(f"Epoch {epoch+1}/{EPOCHS}: Loss={total_loss/len(train_loader):.4f}")
    
    torch.save(model.state_dict(), MODEL_PATH)


# =============================================================================
# PART 4: DETERMINISTIC BENCHMARKING (OPTIMIZED)
# =============================================================================

def run_benchmark():
    print(f"--- BENCHMARKING (GNN + CONE SPLIT + BRANCHLESS) ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model = CircuitGNN_DualTask(num_node_features=16).to(device)
    if not os.path.exists(MODEL_PATH):
        print("Train model first.")
        return
        
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    
    results = []
    files = get_target_files(BENCHMARK_DIR)
    
    random.seed(SEED) 
    
    for filename in files:
        filepath = os.path.join(BENCHMARK_DIR, filename)
        print(f"\nProcessing {filename}...")
        
        try:
            miter = WireFaultMiter(filepath)
            if not miter.gates: continue
            
            extractor = VectorizedGraphExtractor(filepath, var_map=miter.var_map, device=device.type)
            all_gates = sorted(miter.gates, key=lambda x: x[0])
            
            # Run 10 faults
            for i in range(10): 
                target_gate = random.choice(all_gates)[0]
                
                # 1. Inference
                t_gnn_start = time.time()
                data = extractor.get_data_for_fault(target_gate)
                data = data.to(device)
                
                with torch.no_grad():
                    imp_scores, pol_scores = model(data)
                
                # 2. Extract Hints
                hints = []
                for idx, name in enumerate(data.node_names):
                    if name in miter.inputs:
                        imp = imp_scores[idx].item()
                        prob = pol_scores[idx].item()
                        var_id = miter.var_map.get(name)
                        if var_id:
                            val = 1 if prob > 0.5 else 0
                            # Map 0 -> -Lit, 1 -> Lit
                            hints.append((var_id if val else -var_id, imp))
                            
                hints.sort(key=lambda x: -x[1])
                hint_lits = [h[0] for h in hints]
                
                # 3. Solve using Optimized Cone Strategy with Hints
                assignment = miter.solve_fault_specific_cones(target_gate, gnn_hints=hint_lits)
                
                dur = time.time() - t_gnn_start
                status = "DETECTED" if assignment else "UNSAT/UNDETECTED"
                print(f"  Fault {target_gate}: {status} in {dur:.4f}s")
                if assignment:
                    # Print Compact Vector
                    vec = "".join([str(assignment.get(k, 'X')) for k in sorted(miter.inputs)])
                    print(f"  Vector: {vec[:50]}...")
                
                results.append({
                    "Circuit": filename,
                    "Fault": target_gate,
                    "Status": status,
                    "Time": dur
                })
                
        except Exception as e:
            print(f"Error: {e}")

    if results:
        with open("results_optimized.csv", 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)
        print("Saved to results_optimized.csv")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python data_train_bench_mem_efficient.py [generate|train|benchmark]")
    else:
        cmd = sys.argv[1]
        if cmd == "generate": generate_dataset()
        elif cmd == "train": train_model()
        elif cmd == "benchmark": run_benchmark()
===== test_verilog_parser.py =====
#!/usr/bin/env python3
"""
Comprehensive Test: Input/Output Argument Positions
in ICCAD vs Yosys Formats

This shows WHERE inputs and outputs are in gate instantiations
and tests if VerilogParser handles both correctly.
"""

# import sys
import tempfile
import os

# sys.path.insert(0, '/home/claude')
from VerilogParser import VerilogParser

print("=" * 80)
print("UNDERSTANDING INPUT/OUTPUT POSITIONS IN GATE INSTANTIATIONS")
print("=" * 80)

# ============================================================================
# FORMAT 1: ICCAD-2015 POSITIONAL FORMAT
# ============================================================================

iccad_format = """
module iccad_example(a, b, c, out1, out2);
  input a, b, c;
  output out1, out2;
  wire n1, n2, n3;
  
  // ICCAD Positional Format: gate_type ( output , input1 , input2 , ... );
  //                                      ^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^
  //                                      1st arg   2nd+ args are inputs
  
  buf ( n1 , a );              // Buffer:  output=n1,  input=a
  not ( n2 , b );              // NOT:     output=n2,  input=b
  and ( n3 , a , b );          // AND:     output=n3,  inputs=a,b
  nand ( out1 , n3 , c );      // NAND:    output=out1, inputs=n3,c
  or ( out2 , n1 , n2 );       // OR:      output=out2, inputs=n1,n2
endmodule
"""

print("\n" + "=" * 80)
print("FORMAT 1: ICCAD-2015 POSITIONAL")
print("=" * 80)
print("\nPort Argument Convention:")
print("  gate_type ( OUTPUT , INPUT1 , INPUT2 , ... );")
print("              ^^^^^^   ^^^^^^^^^^^^^^^^^^^^^")
print("              1st      2nd, 3rd, ... are inputs")
print("\nExamples:")
print("  and ( n3 , a , b );")
print("        ^^   ^^^^^")
print("        |    inputs")
print("        output")
print("\n  nand ( out1 , n3 , c );")
print("         ^^^^   ^^^^^^^")
print("         |      inputs")
print("         output")

# Test ICCAD format
with tempfile.NamedTemporaryFile(mode='w', suffix='.v', delete=False) as f:
    f.write(iccad_format)
    iccad_file = f.name

try:
    parser_iccad = VerilogParser(iccad_file)
    
    print("\nðŸ“Š ICCAD PARSING RESULTS:")
    print("-" * 80)
    print(f"Inputs:  {parser_iccad.inputs}")
    print(f"Outputs: {parser_iccad.outputs}")
    print(f"\nGates parsed:")
    for output, gate_type, inputs in parser_iccad.gates:
        print(f"  {gate_type:6s} : output={output:10s} inputs={inputs}")
    
    print("\nâœ… VERIFICATION:")
    # Check buf ( n1 , a )
    assert parser_iccad.gate_dict['n1'] == ('BUFF', ['a']), "buf gate failed"
    print("  âœ“ buf ( n1 , a ) -> output=n1, inputs=['a']")
    
    # Check and ( n3 , a , b )
    assert parser_iccad.gate_dict['n3'] == ('AND', ['a', 'b']), "and gate failed"
    print("  âœ“ and ( n3 , a , b ) -> output=n3, inputs=['a', 'b']")
    
    # Check nand ( out1 , n3 , c )
    assert parser_iccad.gate_dict['out1'] == ('NAND', ['n3', 'c']), "nand gate failed"
    print("  âœ“ nand ( out1 , n3 , c ) -> output=out1, inputs=['n3', 'c']")
    
finally:
    os.unlink(iccad_file)

# ============================================================================
# FORMAT 2: YOSYS NAMED PORT FORMAT
# ============================================================================

yosys_format = r"""
module yosys_example(\a , \b , \c , \out1 , \out2 );
  input \a , \b , \c ;
  output \out1 , \out2 ;
  wire n1, n2, n3;
  
  // Yosys Named Port Format: gate_type instance_name ( .PORT(wire), ... );
  //                                                     ^^^^^^^^^^^^^^^^
  //                                                     Named connections
  // Output ports: .Y(), .Q(), .OUT(), .Z(), .O()
  // Input ports:  .A(), .B(), .C(), .D(), .S(), etc.
  
  \$_BUF_ _1_ (
    .A(\a ),           // Input:  A
    .Y(n1)             // Output: Y
  );
  
  \$_NOT_ _2_ (
    .A(\b ),           // Input:  A
    .Y(n2)             // Output: Y
  );
  
  \$_AND_ _3_ (
    .A(\a ),           // Input:  A
    .B(\b ),           // Input:  B
    .Y(n3)             // Output: Y
  );
  
  \$_NAND_ _4_ (
    .A(n3),            // Input:  A
    .B(\c ),           // Input:  B
    .Y(\out1 )         // Output: Y
  );
  
  \$_OR_ _5_ (
    .A(n1),            // Input:  A
    .B(n2),            // Input:  B
    .Y(\out2 )         // Output: Y
  );
endmodule
"""

print("\n\n" + "=" * 80)
print("FORMAT 2: YOSYS NAMED PORT")
print("=" * 80)
print("\nPort Argument Convention:")
print("  gate_type instance ( .PORT_NAME(WIRE), ... );")
print("                       ^^^^^^^^^^^^^^^^^^^^")
print("                       Named connections")
print("\n  Output ports typically: .Y(), .Q(), .OUT(), .Z(), .O()")
print("  Input ports typically:  .A(), .B(), .C(), .D(), .S()")
print("\nExamples:")
print("  \\$_AND_ _3_ (")
print("    .A(\\a ),     // Input A")
print("    .B(\\b ),     // Input B")
print("    .Y(n3)       // Output Y")
print("  );")
print("\n  \\$_NAND_ _4_ (")
print("    .A(n3),      // Input A")
print("    .B(\\c ),     // Input B")
print("    .Y(\\out1 )   // Output Y")
print("  );")

# Test Yosys format
with tempfile.NamedTemporaryFile(mode='w', suffix='.v', delete=False) as f:
    f.write(yosys_format)
    yosys_file = f.name

try:
    parser_yosys = VerilogParser(yosys_file)
    
    print("\nðŸ“Š YOSYS PARSING RESULTS:")
    print("-" * 80)
    print(f"Inputs:  {parser_yosys.inputs}")
    print(f"Outputs: {parser_yosys.outputs}")
    print(f"\nGates parsed:")
    for output, gate_type, inputs in parser_yosys.gates:
        print(f"  {gate_type:6s} : output={output:10s} inputs={inputs}")
    
    print("\nâœ… VERIFICATION:")
    # Check $_BUF_ with .A(a), .Y(n1)
    assert parser_yosys.gate_dict['n1'] == ('BUFF', ['a']), "buf gate failed"
    print("  âœ“ \\$_BUF_ ( .A(\\a), .Y(n1) ) -> output=n1, inputs=['a']")
    
    # Check $_AND_ with .A(a), .B(b), .Y(n3)
    assert parser_yosys.gate_dict['n3'] == ('AND', ['a', 'b']), "and gate failed"
    print("  âœ“ \\$_AND_ ( .A(\\a), .B(\\b), .Y(n3) ) -> output=n3, inputs=['a', 'b']")
    
    # Check $_NAND_ with .A(n3), .B(c), .Y(out1)
    assert parser_yosys.gate_dict['out1'] == ('NAND', ['n3', 'c']), "nand gate failed"
    print("  âœ“ \\$_NAND_ ( .A(n3), .B(\\c), .Y(\\out1) ) -> output=out1, inputs=['n3', 'c']")
    
finally:
    os.unlink(yosys_file)

# ============================================================================
# SUMMARY TABLE
# ============================================================================

print("\n\n" + "=" * 80)
print("SUMMARY: WHERE ARE INPUTS AND OUTPUTS?")
print("=" * 80)

print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Format              â”‚ Output Position          â”‚ Input Position          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ICCAD Positional    â”‚ 1st argument             â”‚ 2nd, 3rd, ... arguments â”‚
â”‚                     â”‚ and ( OUT , in1 , in2 )  â”‚                         â”‚
â”‚                     â”‚       ^^^                â”‚                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Yosys Named Port    â”‚ .Y(), .Q(), .OUT(),      â”‚ .A(), .B(), .C(),       â”‚
â”‚                     â”‚ .Z(), .O()               â”‚ .D(), .S()              â”‚
â”‚                     â”‚ .Y(OUT)                  â”‚ .A(in1), .B(in2)        â”‚
â”‚                     â”‚    ^^^                   â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HOW THE PARSER DETERMINES OUTPUT:

1. ICCAD Format (Positional):
   ports = [w.strip() for w in port_list.split(',')]
   output = ports[0]        # First element
   inputs = ports[1:]       # Rest are inputs

2. Yosys Format (Named):
   output_names = ['Y', 'Q', 'OUT', 'Z', 'O']
   for name in output_names:
       if name in connections:
           output = connections[name]  # Port named Y/Q/OUT/Z/O
   
   skip_ports = ['CLK', 'CLOCK', 'RST', ...]
   inputs = [wire for port, wire in connections.items 
             if port not in output_names and port not in skip_ports]

RESULT: âœ… Parser correctly handles BOTH formats!
""")

print("=" * 80)
print("CONCLUSION")
print("=" * 80)
print("""
The VerilogParser_enhanced.py correctly handles:

âœ… ICCAD-2015 Positional Format
   - First argument = OUTPUT
   - Remaining arguments = INPUTS
   
âœ… Yosys Named Port Format
   - .Y/.Q/.OUT/.Z/.O ports = OUTPUT
   - .A/.B/.C/.D/.S ports = INPUTS
   - Automatically filters out CLK/RST/etc.
   
âœ… Escaped Identifiers
   - \\a[0], \\b[1], etc. are normalized to a[0], b[1]
   
âœ… Yosys Internal Gates
   - \\$_AND_, \\$_OR_, \\$_NAND_, etc. are recognized

Both ICCAD-2015 and Yosys output formats are fully supported!
""")
print("=" * 80)
===== test_yosys.py =====
#!/usr/bin/env python3
"""
Test what happens when Yosys processes a file with escaped identifiers
"""

import subprocess
import tempfile
import os

# Sample from the problematic file
test_verilog = """
module top ( 
    \\a[0] , \\a[1] , \\b[0] , \\b[1] , \\f[0] , \\f[1]  );
  input  \\a[0] , \\a[1] , \\b[0] , \\b[1] ;
  output \\f[0] , \\f[1] ;
  wire n1, n2, n3, n4;
  
  assign n1 = \\a[0]  & ~\\b[0] ;
  assign n2 = ~\\a[0]  & \\b[0] ;
  assign \\f[0]  = n1 | n2;
  assign n3 = \\a[0]  & \\b[0] ;
  assign n4 = ~\\a[1]  & ~\\b[1] ;
  assign \\f[1]  = n3 | n4;
endmodule
"""

print("=" * 80)
print("TESTING YOSYS WITH ESCAPED IDENTIFIERS")
print("=" * 80)

with tempfile.TemporaryDirectory() as tmpdir:
    # Write test file
    input_file = os.path.join(tmpdir, "test.v")
    with open(input_file, 'w') as f:
        f.write(test_verilog)
    
    output_file = os.path.join(tmpdir, "output.v")
    
    # Create synthesis script
    script = f"""
read_verilog {input_file}
hierarchy -check -auto-top
proc; opt; fsm; opt; memory; opt
flatten
techmap; opt
abc -g AND,NAND,OR,NOR,XOR,XNOR,ANDNOT,ORNOT
techmap; opt
opt_clean -purge
write_verilog -noattr -noexpr {output_file}
"""
    
    script_file = os.path.join(tmpdir, "script.ys")
    with open(script_file, 'w') as f:
        f.write(script)
    
    # Run Yosys
    print("\nðŸ”§ Running Yosys synthesis...")
    print("-" * 80)
    
    result = subprocess.run(
        ['yosys', '-s', script_file],
        capture_output=True,
        text=True,
        timeout=30
    )
    
    print("\nðŸ“Š RESULTS:")
    print("-" * 80)
    print(f"Return code: {result.returncode}")
    
    if result.returncode != 0:
        print("\nâŒ YOSYS FAILED!")
        print("\nðŸ”´ STDERR:")
        print(result.stderr)
        print("\nðŸ“ STDOUT:")
        print(result.stdout)
    else:
        print("\nâœ… YOSYS SUCCEEDED!")
        
        if os.path.exists(output_file):
            with open(output_file, 'r') as f:
                content = f.read()
            
            print(f"\nðŸ“„ Output file created ({len(content)} bytes)")
            print("\nFirst 50 lines:")
            print("-" * 80)
            for i, line in enumerate(content.split('\n')[:50], 1):
                print(f"{i:3d}: {line}")
        else:
            print("\nâŒ Output file NOT created!")

print("\n" + "=" * 80)
===== train_model.py =====
"""
GNN Training Script for Dual-Task Learning (Polarity + Importance)
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.nn import GATv2Conv
from torch_geometric.loader import DataLoader
import random
import numpy as np

# =============================================================================
# CONFIGS
# =============================================================================
DATASET_PATH = "dataset_hybrid_17feat.pt"
MODEL_PATH = "gnn_model_dual_task_17feat.pth"
EPOCHS = 20
BATCH_SIZE = 32
LEARNING_RATE = 0.001
SEED = 42

def set_global_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)

set_global_seed(SEED)

# =============================================================================
# MODEL DEFINITION
# =============================================================================

class CircuitGNN_DualTask(torch.nn.Module):
    """
    Dual-task GNN for circuit testability prediction.
    
    Predicts:
    1. Input Importance (regression)
    2. Input Polarity (binary classification)
    """
    def __init__(self, num_node_features=17, num_layers=20, hidden_dim=64, dropout=0.2):
        super(CircuitGNN_DualTask, self).__init__()
        self.dropout = dropout
        self.num_layers = num_layers
        
        self.convs = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()
        
        # Input layer
        self.convs.append(GATv2Conv(num_node_features, hidden_dim, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        
        # Hidden layers with residual connections
        for _ in range(num_layers - 2):
            self.convs.append(GATv2Conv(hidden_dim, hidden_dim, heads=2, concat=False))
            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        
        # Output layer
        self.convs.append(GATv2Conv(hidden_dim, 32, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(32))
        
        # Task-specific heads
        self.importance_head = torch.nn.Linear(32, 1)
        self.polarity_head = torch.nn.Linear(32, 1)
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        
        # First layer
        x = self.convs[0](x, edge_index)
        x = self.bns[0](x)
        x = torch.nn.functional.elu(x)
        
        # Middle layers with residual connections
        for i in range(1, self.num_layers - 1):
            identity = x
            x = self.convs[i](x, edge_index)
            x = self.bns[i](x)
            x = torch.nn.functional.elu(x)
            x = x + identity  # Residual connection
        
        # Final layer
        x = self.convs[-1](x, edge_index)
        x = self.bns[-1](x)
        x = torch.nn.functional.elu(x)
        
        # Task heads
        importance = self.importance_head(x)
        polarity = torch.sigmoid(self.polarity_head(x))
        
        return importance, polarity

# =============================================================================
# TRAINING FUNCTION
# =============================================================================

def train_model():
    print("=" * 80)
    print("DUAL-TASK GNN TRAINING")
    print("=" * 80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # Load dataset
    if not os.path.exists(DATASET_PATH):
        print(f"ERROR: Dataset not found at {DATASET_PATH}")
        print("Please run data generation first!")
        return
    
    print(f"Loading dataset from {DATASET_PATH}...")
    dataset = torch.load(DATASET_PATH, weights_only=False)
    print(f"Loaded {len(dataset)} samples")
    
    # Split dataset
    train_size = int(len(dataset) * 0.8)
    val_size = len(dataset) - train_size
    
    train_dataset = dataset[:train_size]
    val_dataset = dataset[train_size:]
    
    print(f"Train samples: {train_size}")
    print(f"Val samples: {val_size}")
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True,
        num_workers=0  # Set to 0 to avoid multiprocessing issues
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=False,
        num_workers=0
    )
    
    # Initialize model
    model = CircuitGNN_DualTask(num_node_features=17).to(device)
    
    # Check if pre-trained model exists
    if os.path.exists(MODEL_PATH):
        print(f"Loading existing model from {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    else:
        print("Training from scratch...")
    
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # Loss functions
    criterion_importance = nn.MSELoss(reduction='none')
    criterion_polarity = nn.BCELoss(reduction='none')
    
    print("\n" + "=" * 80)
    print("Starting Training")
    print("=" * 80)
    
    best_val_loss = float('inf')
    
    for epoch in range(EPOCHS):
        # Training
        model.train()
        train_loss = 0.0
        train_imp_loss = 0.0
        train_pol_loss = 0.0
        
        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            
            # Forward pass
            pred_importance, pred_polarity = model(batch)
            
            # Compute masked losses
            mask = batch.train_mask
            mask_sum = mask.sum().clamp(min=1)
            
            loss_imp = (criterion_importance(pred_importance, batch.y_importance) * mask).sum() / mask_sum
            loss_pol = (criterion_polarity(pred_polarity, batch.y_polarity) * mask).sum() / mask_sum
            
            total_loss = loss_imp + loss_pol
            
            # Backward pass
            total_loss.backward()
            optimizer.step()
            
            train_loss += total_loss.item()
            train_imp_loss += loss_imp.item()
            train_pol_loss += loss_pol.item()
        
        train_loss /= len(train_loader)
        train_imp_loss /= len(train_loader)
        train_pol_loss /= len(train_loader)
        
        # Validation
        model.eval()
        val_loss = 0.0
        val_imp_loss = 0.0
        val_pol_loss = 0.0
        
        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(device)
                pred_importance, pred_polarity = model(batch)
                
                mask = batch.train_mask
                mask_sum = mask.sum().clamp(min=1)
                
                loss_imp = (criterion_importance(pred_importance, batch.y_importance) * mask).sum() / mask_sum
                loss_pol = (criterion_polarity(pred_polarity, batch.y_polarity) * mask).sum() / mask_sum
                
                val_loss += (loss_imp + loss_pol).item()
                val_imp_loss += loss_imp.item()
                val_pol_loss += loss_pol.item()
        
        val_loss /= len(val_loader)
        val_imp_loss /= len(val_loader)
        val_pol_loss /= len(val_loader)
        
        # Print progress
        print(f"Epoch {epoch+1:2d}/{EPOCHS} | "
              f"Train Loss: {train_loss:.4f} (Imp: {train_imp_loss:.4f}, Pol: {train_pol_loss:.4f}) | "
              f"Val Loss: {val_loss:.4f} (Imp: {val_imp_loss:.4f}, Pol: {val_pol_loss:.4f})")
        
        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), MODEL_PATH)
            print(f"  â†’ Saved best model (val_loss: {val_loss:.4f})")
    
    print("\n" + "=" * 80)
    print("TRAINING COMPLETE")
    print("=" * 80)
    print(f"Best validation loss: {best_val_loss:.4f}")
    print(f"Model saved to: {MODEL_PATH}")

if __name__ == "__main__":
    train_model()
