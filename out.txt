===== BenchParser.py =====
"""
Shared Bench File Parser for DFT Analysis

This module provides a unified parser for .bench files that:
- Handles full-scan DFFs (Q output as PPI, D input as PPO)
- Tracks back edges for bidirectional graph traversal
- Provides a common data structure for both SAT and GNN models
"""

class BenchParser:
    """
    Unified parser for .bench format files with full-scan DFF support.
    
    For full-scan designs:
    - DFF outputs (Q) are treated as Pseudo Primary Inputs (PPIs)
    - DFF inputs (D) are treated as Pseudo Primary Outputs (PPOs)
    - The circuit is "broken" at flip-flops to eliminate cycles
    """
    
    def __init__(self, bench_file):
        self.bench_file = bench_file
        
        # Primary Inputs/Outputs
        self.inputs = []           # Primary Inputs (PIs)
        self.outputs = []          # Primary Outputs (POs)
        
        # Pseudo Inputs/Outputs (from DFFs)
        self.ppis = []             # Pseudo Primary Inputs (DFF Q outputs)
        self.ppos = []             # Pseudo Primary Outputs (DFF D inputs)
        
        # All inputs/outputs combined
        self.all_inputs = []       # PIs + PPIs
        self.all_outputs = []      # POs + PPOs
        
        # Gate structure
        self.gates = []            # List of (output, gate_type, inputs)
        self.gate_dict = {}        # Map: output_name -> (gate_type, inputs)
        
        # DFF tracking
        self.dffs = []             # List of (Q_output, D_input) tuples
        self.dff_map = {}          # Map: Q_output -> D_input
        
        # Back edges (for reverse traversal)
        self.back_edges = {}       # Map: input_wire -> [gates_it_drives]
        
        # Variable mapping (for SAT solver)
        self.var_map = {}          # Map: wire_name -> variable_id
        
        # Parse the file
        self._parse()
    
    def _parse(self):
        """Parse the bench file and populate all data structures."""
        with open(self.bench_file, 'r') as f:
            for line in f:
                line = line.strip()
                
                # Skip empty lines and comments
                if not line or line.startswith('#'):
                    continue
                
                # Parse INPUT declarations
                if line.startswith('INPUT'):
                    name = line[line.find('(')+1:line.find(')')]
                    self.inputs.append(name)
                    self.all_inputs.append(name)
                    
                # Parse OUTPUT declarations
                elif line.startswith('OUTPUT'):
                    name = line[line.find('(')+1:line.find(')')]
                    self.outputs.append(name)
                    self.all_outputs.append(name)
                    
                # Parse gate definitions
                elif '=' in line:
                    parts = line.split('=')
                    out = parts[0].strip()
                    rhs = parts[1].strip()
                    
                    # Extract gate type and inputs
                    g_type = rhs[:rhs.find('(')].strip().upper()
                    in_str = rhs[rhs.find('(')+1:-1]
                    inputs = [x.strip() for x in in_str.split(',')] if in_str else []
                    
                    # Handle DFFs specially (Full-Scan assumption)
                    if g_type == 'DFF':
                        # Q output (out) becomes a PPI
                        self.ppis.append(out)
                        self.all_inputs.append(out)
                        
                        # D input becomes a PPO
                        if len(inputs) > 0:
                            d_input = inputs[0]
                            self.ppos.append(d_input)
                            self.all_outputs.append(d_input)
                            
                            # Track the DFF relationship
                            self.dffs.append((out, d_input))
                            self.dff_map[out] = d_input
                        
                        # Note: DFFs are NOT added to self.gates
                        # This "breaks" the circuit at flip-flops
                    else:
                        # Regular combinational gate
                        self.gates.append((out, g_type, inputs))
                        self.gate_dict[out] = (g_type, inputs)
                        
                        # Build back edges for reverse traversal
                        for inp in inputs:
                            if inp not in self.back_edges:
                                self.back_edges[inp] = []
                            self.back_edges[inp].append(out)
        
        # Remove duplicates while preserving order
        self.all_inputs = list(dict.fromkeys(self.all_inputs))
        self.all_outputs = list(dict.fromkeys(self.all_outputs))
    
    def get_all_wires(self):
        """Get all wire names in the circuit (inputs, outputs, and internal)."""
        wires = set(self.all_inputs + self.all_outputs)
        for out, _, inputs in self.gates:
            wires.add(out)
            wires.update(inputs)
        return sorted(list(wires))
    
    def build_var_map(self):
        """Build variable mapping for SAT solver (1-indexed)."""
        if self.var_map:
            return self.var_map  # Already built
        
        next_var = 1
        for wire in self.get_all_wires():
            if wire not in self.var_map:
                self.var_map[wire] = next_var
                next_var += 1
        return self.var_map
    
    def get_fanout(self, wire_name):
        """Get all gates driven by a wire (forward edges)."""
        return self.back_edges.get(wire_name, [])
    
    def get_fanin(self, wire_name):
        """Get the gate driving a wire (backward edge)."""
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][1]  # Return inputs
        return []
    
    def is_pi(self, wire_name):
        """Check if wire is a Primary Input."""
        return wire_name in self.inputs
    
    def is_po(self, wire_name):
        """Check if wire is a Primary Output."""
        return wire_name in self.outputs
    
    def is_ppi(self, wire_name):
        """Check if wire is a Pseudo Primary Input (DFF Q)."""
        return wire_name in self.ppis
    
    def is_ppo(self, wire_name):
        """Check if wire is a Pseudo Primary Output (DFF D)."""
        return wire_name in self.ppos
    
    def is_dff_output(self, wire_name):
        """Check if wire is a DFF Q output."""
        return wire_name in self.dff_map
    
    def get_dff_input(self, q_output):
        """Get the D input for a DFF Q output."""
        return self.dff_map.get(q_output)
    
    def get_gate_type(self, wire_name):
        """Get the gate type that produces this wire."""
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][0]
        elif self.is_ppi(wire_name):
            return 'PPI'
        elif self.is_pi(wire_name):
            return 'INPUT'
        return None
    
    def __repr__(self):
        return (f"BenchParser({self.bench_file})\n"
                f"  PIs: {len(self.inputs)}, POs: {len(self.outputs)}\n"
                f"  PPIs: {len(self.ppis)}, PPOs: {len(self.ppos)}\n"
                f"  Gates: {len(self.gates)}, DFFs: {len(self.dffs)}")

===== WireFaultMiter.py =====
import os
from BenchParser import BenchParser

class WireFaultMiter:
    def __init__(self, bench_file):
        self.bench_file = bench_file
        
        # Use shared parser
        self.parser = BenchParser(bench_file)
        
        # Extract data from parser
        self.inputs = self.parser.all_inputs      # PIs + PPIs
        self.outputs = self.parser.all_outputs    # POs + PPOs
        self.gates = self.parser.gates
        
        # Build variable map (Deterministic from Parser)
        self.var_map = self.parser.build_var_map()
        self.next_var = len(self.var_map) + 1
        
        # Faulty circuit mapping
        self.faulty_map = {}
        
        self.scan_inputs = self.parser.ppis
        self.scan_outputs = self.parser.ppos

    def _get_var(self, name):
        if name not in self.var_map:
            self.var_map[name] = self.next_var
            self.next_var += 1
        return self.var_map[name]

    def build_miter(self, fault_wire, fault_type=None, force_diff=1):
        clauses = []
        
        # --- 1. Good Circuit ---
        # self.gates is a list (Deterministic order from file)
        for out, g_type, inputs in self.gates:
            self._add_gate_clauses(clauses, self.var_map[out], g_type, [self.var_map[i] for i in inputs])
            
        # --- 2. Faulty Circuit ---
        # Map inputs to same vars, but internal wires get new vars
        # Dict insertion order is preserved in Python 3.7+, but iterating input list is safer
        self.faulty_map = {}
        for name in self.inputs:
            self.faulty_map[name] = self.var_map[name]
            
        for out, _, _ in self.gates:
            if out not in self.faulty_map:
                self.faulty_map[out] = self.next_var
                self.next_var += 1
                
        # Inject Fault (Stuck-at)
        if fault_wire in self.faulty_map:
            fault_gate_var = self.faulty_map[fault_wire]
            if fault_type == 1: clauses.append([fault_gate_var])   # Stuck-at-1
            elif fault_type == 0: clauses.append([-fault_gate_var]) # Stuck-at-0

        for out, g_type, inputs in self.gates:
            # If gate drives the fault wire, disconnect it (fault overrides)
            if out == fault_wire:
                continue

            out_var = self.faulty_map[out]
            in_vars = [self.faulty_map.get(i) for i in inputs]
            if None in in_vars: continue 
            self._add_gate_clauses(clauses, out_var, g_type, in_vars)

        # --- 3. Miter Comparator (XOR Outputs) ---
        miter_out = self.next_var; self.next_var += 1
        diff_vars = []
        
        # --- FIX FOR DETERMINISM ---
        # Old: unique_outputs = list(set(self.outputs)) -> Random order!
        # New: Sorted list -> Fixed order
        unique_outputs = sorted(list(set(self.outputs)))
        
        for out in unique_outputs:
            if out not in self.var_map or out not in self.faulty_map: continue
            
            good = self.var_map[out]
            bad = self.faulty_map[out]
            diff = self.next_var; self.next_var += 1
            
            # XOR Logic: (Good != Bad) -> Diff
            # (-a -b -c), (a b -c), (-a b c), (a -b c)
            clauses.append([-good, -bad, -diff])
            clauses.append([good, bad, -diff])
            clauses.append([-good, bad, diff])
            clauses.append([good, -bad, diff])
            diff_vars.append(diff)
            
        # Big OR Gate (Any difference triggers Miter)
        big_or = [-miter_out]
        for d in diff_vars:
            clauses.append([-d, miter_out])
            big_or.append(d)
        clauses.append(big_or)
        clauses.append([miter_out]) # Force Miter = 1
        
        return clauses

    def _add_gate_clauses(self, clauses, out, g_type, inputs):
        if g_type == 'AND':
            for i in inputs: clauses.append([-out, i])
            clauses.append([out] + [-i for i in inputs])
        elif g_type == 'OR':
            for i in inputs: clauses.append([out, -i])
            clauses.append([-out] + inputs)
        elif g_type == 'NOT':
            clauses.append([-out, -inputs[0]])
            clauses.append([out, inputs[0]])
        elif g_type == 'NAND':
            for i in inputs: clauses.append([out, i])
            clauses.append([-out] + [-i for i in inputs])
        elif g_type == 'NOR':
            for i in inputs: clauses.append([-out, -i])
            clauses.append([out] + inputs)
        elif g_type == 'XOR':
            if len(inputs) == 2:
                a, b = inputs
                clauses.append([-out, -a, -b])
                clauses.append([-out, a, b])
                clauses.append([out, -a, b])
                clauses.append([out, a, -b])
        elif g_type == 'BUFF':
             clauses.append([-out, inputs[0]])
             clauses.append([out, -inputs[0]])
===== data_train_bench_mem_efficient.py =====
"""
Complete pipeline for Dual-Task GNN training with Parallel Generation & Polarity Guidance.
Uses 'set_phases' for soft solver constraints.
"""

import os
import sys

# Add local PySAT if needed
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'pysat'))

import time
import csv
import math
import torch
import torch.nn as nn
from torch_geometric.nn import GATv2Conv
import torch.optim as optim
import random
import numpy as np
from pysat.solvers import Glucose3, Minisat22
from pysat.formula import CNF
from tqdm import tqdm
from torch_geometric.loader import DataLoader
from torch_geometric.data import Dataset
from WireFaultMiter import WireFaultMiter
from BenchParser import BenchParser
from torch_geometric.data import Data
import torch.multiprocessing as mp

# IMPORT THE EXTRACTOR
from neuro_utils import VectorizedGraphExtractor

# =============================================================================
# CONFIGS
# =============================================================================
BENCHMARK_DIR = "../hdl-benchmarks/iscas85/bench/"
DATASET_PATH = "dataset_oracle_dual_16feat.pt"
SAMPLES_PER_FILE = 50
MODEL_PATH = "gnn_model_dual_task_16feat.pth"
EPOCHS = 20
BATCH_SIZE = 32
GENERATE_TRAIN_DATA_DIR = "../I99T"
SEED = 42

# =============================================================================
# 0. DETERMINISM SETUP
# =============================================================================
def set_global_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)

set_global_seed(SEED)

# =============================================================================
# PART 1: OPTIMIZED PARALLEL DATA GENERATION
# =============================================================================

# def get_target_files():
#     if not os.path.exists(BENCHMARK_DIR):
#         return []
#     return sorted([f for f in os.listdir(BENCHMARK_DIR) if f.endswith(".bench")])

def get_target_files(DIR):
    if not os.path.exists(DIR):
        return []
        
    file_list = []
    # os.walk recursively visits every subdirectory
    for root, dirs, files in os.walk(DIR):
        for f in files:
            if f.endswith(".bench"):
                # Get the full absolute path
                full_path = os.path.join(root, f)
                
                # Convert it to a path relative to BENCHMARK_DIR 
                # e.g., converts "/usr/bench/subdir/c17.bench" -> "subdir/c17.bench"
                # This ensures the os.path.join(DIR, filename) in your worker still works.
                rel_path = os.path.relpath(full_path, DIR)
                file_list.append(rel_path)
                
    return sorted(file_list)


def process_single_circuit(filename):
    """Worker with TIERED SAMPLING and GIANT SKIP."""
    set_global_seed(SEED + len(filename)) 
    
    filepath = os.path.join(GENERATE_TRAIN_DATA_DIR, filename)
    local_dataset = []

    try:
        # 1. Quick Gate Count Check (Avoid parsing massive files if possible)
        # (We parse it anyway here for simplicity, but in production you'd grep the file first)
        miter = WireFaultMiter(filepath)
        num_gates = len(miter.gates)
        if not miter.gates: return []
        
        # =========================================================
        # TIERED SAMPLING STRATEGY
        # =========================================================
        if num_gates > 20000:
            # TIER 3: GIANTS (b17, b18, b19) -> SKIP
            # These are too big for a single-threaded Python loop in a tutorial.
            print(f"[{filename}] SKIPPING Giant Circuit ({num_gates} gates).", flush=True)
            return []
            
        elif num_gates > 4000:
            # TIER 2: MEDIUM (b14, b15) -> REDUCED
            local_samples = 5   # Only 5 samples
            max_probes = 20     # Only probe 20 inputs
            probe_time_limit = 2.0
            print(f"[{filename}] Medium Circuit ({num_gates} gates). Reducing to {local_samples} samples.", flush=True)
            
        else:
            # TIER 1: SMALL -> FULL
            local_samples = SAMPLES_PER_FILE
            max_probes = 100
            probe_time_limit = 5.0

        extractor = VectorizedGraphExtractor(filepath, var_map=miter.var_map, device='cpu')
        input_list = sorted(list(miter.inputs))
        input_set = set(input_list)
        
        # Probe Sampling
        probe_list = input_list
        if len(input_list) > max_probes:
             probe_list = random.sample(input_list, max_probes)

        # Generate samples
        for i in range(local_samples):
            # VISUAL FEEDBACK
            if i % 5 == 0:
                print(f"[{filename}] Processing sample {i+1}/{local_samples}...", flush=True)

            all_gates = sorted(miter.gates, key=lambda x: x[0])
            target_gate = random.choice(all_gates)[0]
            
            clauses = miter.build_miter(target_gate, None, 1)
            cnf = CNF()
            cnf.extend(clauses)
            
            with Glucose3(bootstrap_with=cnf) as solver:
                solver.conf_budget(10000) 
                if not solver.solve(): continue
                    
                model = solver.get_model()
                if not model: continue
                
                with Glucose3(bootstrap_with=cnf) as probe_solver:
                    current_conflicts = probe_solver.accum_stats()['conflicts']
                    input_importance = {}
                    input_polarity = {} 
                    
                    start_probe_time = time.time()
                    
                    for input_name in probe_list:
                        # DYNAMIC TIMEOUT
                        if time.time() - start_probe_time > probe_time_limit: 
                            break

                        var_id = miter.var_map[input_name]
                        correct_val = var_id if var_id in model else -var_id
                        wrong_val = -correct_val
                        
                        probe_solver.conf_budget(1000)
                        result = probe_solver.solve(assumptions=[wrong_val])
                        
                        new_conflicts = probe_solver.accum_stats()['conflicts']
                        delta = new_conflicts - current_conflicts
                        current_conflicts = new_conflicts
                        
                        if result:
                            importance = delta 
                        else:
                            importance = 5000 
                        
                        input_importance[input_name] = importance
                        if importance > 0:
                            input_polarity[input_name] = 1.0 if var_id in model else 0.0
                        else:
                            input_polarity[input_name] = 0.5
                
                if not input_importance: continue 

                max_imp = max(input_importance.values()) if input_importance else 1
                data = extractor.get_data_for_fault(target_gate)
                y_polarity = torch.zeros(len(data.node_names), 1)
                y_importance = torch.zeros(len(data.node_names), 1)
                train_mask = torch.zeros(len(data.node_names), 1)
                
                for k, node_name in enumerate(data.node_names):
                    if node_name in input_set:
                        if node_name in input_importance:
                            y_polarity[k] = input_polarity.get(node_name, 0.5)
                            y_importance[k] = input_importance.get(node_name, 0) / max(max_imp, 1)
                            train_mask[k] = 1.0
                
                data.y_polarity = y_polarity
                data.y_importance = y_importance
                data.train_mask = train_mask
                local_dataset.append(data)
    
    except Exception as e:
        print(f"[{filename}] Error: {e}", flush=True)
        return []

    print(f"[{filename}] Finished. Generated {len(local_dataset)} samples.", flush=True)
    return local_dataset

def generate_dataset():
    print(f"--- MINING DUAL-TASK ORACLE DATA (PARALLEL) ---")
    if not os.path.exists(GENERATE_TRAIN_DATA_DIR):
        print(f"Error: {GENERATE_TRAIN_DATA_DIR} not found.")
        return

    files = get_target_files(GENERATE_TRAIN_DATA_DIR)
    num_workers = min(4, os.cpu_count())
    dataset = []

    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass

    with mp.Pool(processes=num_workers) as pool:
        results = list(tqdm(pool.imap_unordered(process_single_circuit, files), total=len(files)))
        for res in results:
            dataset.extend(res)

    torch.save(dataset, DATASET_PATH)


# =============================================================================
# PART 2 & 3: MODEL AND TRAINING (UNCHANGED)
# =============================================================================

class CircuitGNN_DualTask(torch.nn.Module):
    def __init__(self, num_node_features=16, num_layers=20, hidden_dim=64, dropout=0.2):
        super(CircuitGNN_DualTask, self).__init__()
        self.dropout = dropout
        self.num_layers = num_layers
        self.convs = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()
        self.convs.append(GATv2Conv(num_node_features, hidden_dim, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        for _ in range(num_layers - 2):
            self.convs.append(GATv2Conv(hidden_dim, hidden_dim, heads=2, concat=False))
            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        self.convs.append(GATv2Conv(hidden_dim, 32, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(32))
        self.importance_head = torch.nn.Linear(32, 1)
        self.polarity_head = torch.nn.Linear(32, 1)
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.convs[0](x, edge_index)
        x = self.bns[0](x)
        x = torch.nn.functional.elu(x)
        for i in range(1, self.num_layers - 1):
            identity = x
            x = self.convs[i](x, edge_index)
            x = self.bns[i](x)
            x = torch.nn.functional.elu(x)
            x = x + identity
        x = self.convs[-1](x, edge_index)
        x = self.bns[-1](x)
        x = torch.nn.functional.elu(x)
        return self.importance_head(x), torch.sigmoid(self.polarity_head(x))

def train_model():
    print("--- Training Dual-Task GNN ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # if not os.path.exists(DATASET_PATH): generate_dataset()
    if not os.path.exists(DATASET_PATH): print("Dataset not found. Please generate dataset first."); return
    
    dataset = torch.load(DATASET_PATH, weights_only=False)
    train_loader = DataLoader(dataset[:int(len(dataset)*0.8)], batch_size=BATCH_SIZE, shuffle=True)
    
    model = CircuitGNN_DualTask(num_node_features=16).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    crit_imp = nn.MSELoss(reduction='none')
    crit_pol = nn.BCELoss(reduction='none')
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            p_imp, p_pol = model(batch)
            mask = batch.train_mask
            mask_sum = mask.sum().clamp(min=1)
            l_imp = (crit_imp(p_imp, batch.y_importance) * mask).sum() / mask_sum
            l_pol = (crit_pol(p_pol, batch.y_polarity) * mask).sum() / mask_sum
            (l_imp + l_pol).backward()
            optimizer.step()
            total_loss += (l_imp + l_pol).item()
        print(f"Epoch {epoch+1}/{EPOCHS}: Loss={total_loss/len(train_loader):.4f}")
    
    torch.save(model.state_dict(), MODEL_PATH)


# =============================================================================
# PART 4: DETERMINISTIC BENCHMARKING (FIXED)
# =============================================================================

def solve_with_phases(cnf, hint_literals, solver_class=Minisat22):
    """
    Solve using set_phases for soft guidance.
    solver_class: Allows switching between Glucose3 and Minisat22
    """
    with solver_class(bootstrap_with=cnf) as solver:
        # 1. Deterministic Seeding (If supported by solver wrapper)
        # Most PySAT wrappers don't expose seed in init, but rely on deterministic behavior
        # given the same clause order.
        
        # 2. Apply Hints
        solver.set_phases(hint_literals)
        
        # 3. Solve
        result = solver.solve()
        conflicts = solver.accum_stats()['conflicts']
        
    return result, conflicts

def run_benchmark():
    print(f"--- BENCHMARKING WITH SET_PHASES (DETERMINISTIC) ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model = CircuitGNN_DualTask(num_node_features=16).to(device)
    if not os.path.exists(MODEL_PATH):
        print("Train model first.")
        return
        
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    
    results = []
    files = get_target_files(BENCHMARK_DIR)
    
    # 1. Sort files to ensure file processing order is fixed
    files.sort()
    
    # 2. Fix the sequence of faults we will test
    # We pre-generate the random seeds or indices if we want perfect repeatability across runs
    random.seed(SEED) 
    
    for filename in files:
        filepath = os.path.join(BENCHMARK_DIR, filename)
        print(f"\nProcessing {filename}...")
        
        try:
            miter = WireFaultMiter(filepath)
            if not miter.gates: continue
            
            extractor = VectorizedGraphExtractor(filepath, var_map=miter.var_map, device=device.type)
            
            # Deterministic: Sort input names
            input_names_list = sorted(list(miter.inputs))
            input_names_set = set(input_names_list)
            
            # Sort gates to ensure deterministic random choice
            all_gates = sorted(miter.gates, key=lambda x: x[0])
            
            # Run 20 faults
            for i in range(20): 
                # Pick target deterministically based on global seed state
                target_gate = random.choice(all_gates)[0]
                
                clauses = miter.build_miter(target_gate, None, 1)
                cnf = CNF()
                cnf.extend(clauses)
                
                # --- BASELINE (Minisat22) ---
                # Using Minisat22 as the "Weak Solver" to demonstrate GNN impact better
                # You can change this to Glucose3 if you prefer strong baseline
                SolverClass = Minisat22 
                
                t_start = time.time()
                with SolverClass(bootstrap_with=cnf) as s:
                    s.solve()
                    std_conflicts = s.accum_stats()['conflicts']
                std_time = time.time() - t_start
                
                # --- GNN INFERENCE ---
                t_gnn_start = time.time()
                data = extractor.get_data_for_fault(target_gate)
                data = data.to(device)
                
                with torch.no_grad():
                    imp_scores, pol_scores = model(data)
                
                # Extract Predictions
                candidates = []
                for idx, name in enumerate(data.node_names):
                    if name in input_names_set:
                        imp = imp_scores[idx].item()
                        prob = pol_scores[idx].item()
                        var_id = miter.var_map.get(name)
                        
                        if var_id:
                            signed_lit = var_id if prob > 0.5 else -var_id
                            candidates.append((signed_lit, imp, var_id)) # Add var_id for tie-breaking
                
                # --- CRITICAL FIX FOR DETERMINISM ---
                # Sort by: 
                # 1. Importance (Descending)
                # 2. Variable ID (Ascending) -> TIE BREAKER
                candidates.sort(key=lambda x: (-x[1], x[2]))
                
                hint_literals = [x[0] for x in candidates]
                
                # --- GUIDED SOLVE ---
                _, gnn_conflicts = solve_with_phases(cnf, hint_literals, solver_class=SolverClass)
                gnn_time = time.time() - t_gnn_start
                
                speedup = std_conflicts / max(gnn_conflicts, 1)
                print(f"  Fault {target_gate}: {std_conflicts} -> {gnn_conflicts} ({speedup:.2f}x)")
                
                results.append({
                    "Circuit": filename,
                    "Fault": target_gate,
                    "Speedup": speedup,
                    "Std_Conf": std_conflicts,
                    "GNN_Conf": gnn_conflicts
                })
                
        except Exception as e:
            print(f"Error: {e}")

    if results:
        with open("results_set_phases.csv", 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)
        print("Saved to results_set_phases.csv")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python importance_pipeline_16feat.py [generate|train|benchmark]")
    else:
        cmd = sys.argv[1]
        if cmd == "generate": generate_dataset()
        elif cmd == "train": train_model()
        elif cmd == "benchmark": run_benchmark()
===== neuro_utils.py =====
import torch
import math
from torch_geometric.data import Data
from BenchParser import BenchParser

class VectorizedGraphExtractor:
    """
    High-Performance SCOAP Extractor using Vectorized Tensor Operations.
    Generates 16-dimensional feature vectors including Observability.
    """
    
    # Gate Type Mapping
    TYPE_MAP = {
        'INPUT': 0, 'PPI': 0, 
        'BUFF': 1, 'NOT': 2,
        'AND': 3, 'NAND': 4,
        'OR': 5, 'NOR': 6,
        'XOR': 7, 'XNOR': 7
    }

    def __init__(self, bench_path, var_map=None, device='cpu'):
        self.parser = BenchParser(bench_path)
        self.device = device
        
        # 1. Build Name Mappings (Sync with Miter if var_map provided)
        if var_map:
            self.var_map = var_map
        else:
            self.var_map = self.parser.build_var_map()
            
        self.ordered_names = sorted(self.var_map.keys(), key=lambda k: self.var_map[k])
        self.name_to_idx = {name: i for i, name in enumerate(self.ordered_names)}
        self.num_nodes = len(self.ordered_names)
        
        # 2. Build Structural Tensors
        self.edges_list = []
        self.node_types = torch.zeros(self.num_nodes, dtype=torch.long, device=device)
        
        # Assign Gate Types
        for name, g_type, _ in self.parser.gates:
            if name in self.name_to_idx:
                idx = self.name_to_idx[name]
                self.node_types[idx] = self.TYPE_MAP.get(g_type, 1) # Default to BUFF
        
        # Overwrite Types for Inputs/PPIs
        for pi in self.parser.inputs:
            if pi in self.name_to_idx:
                self.node_types[self.name_to_idx[pi]] = self.TYPE_MAP['INPUT']
        for ppi in self.parser.ppis:
            if ppi in self.name_to_idx:
                self.node_types[self.name_to_idx[ppi]] = self.TYPE_MAP['INPUT']
        
        # Build Edge List (Source -> Dest)
        for out, _, inputs in self.parser.gates:
            if out in self.name_to_idx:
                dst = self.name_to_idx[out]
                for inp in inputs:
                    if inp in self.name_to_idx:
                        src = self.name_to_idx[inp]
                        self.edges_list.append([src, dst])
        
        # Create Edge Index Tensor
        if self.edges_list:
            self.edge_index = torch.tensor(self.edges_list, dtype=torch.long, device=device).t().contiguous()
        else:
            self.edge_index = torch.zeros((2, 0), dtype=torch.long, device=device)
            
        # Create Boolean Masks for Vectorized Logic
        self.masks = {}
        for t_name, t_id in self.TYPE_MAP.items():
            self.masks[t_name] = (self.node_types == t_id)

        # Pre-build Python adjacency for BFS traversals (Distance calculation)
        self.adj = [[] for _ in range(self.num_nodes)]      # Forward: src -> [dst]
        self.parents = [[] for _ in range(self.num_nodes)]  # Backward: dst -> [src]
        
        for src, dst in self.edges_list:
            self.adj[src].append(dst)
            self.parents[dst].append(src)
            
        # 3. Compute Metrics Immediately
        self.cc0, self.cc1, self.co = self._compute_scoap_vectorized()
        self.x_base = self._build_base_features()

    def _compute_scoap_vectorized(self):
        """Vectorized SCOAP: Forward Controllability & Backward Observability"""
        num_nodes = self.num_nodes
        src_idx, dst_idx = self.edge_index
        
        # --- Part A: Controllability (Forward) ---
        cc0 = torch.ones(num_nodes, device=self.device)
        cc1 = torch.ones(num_nodes, device=self.device)
        
        mask_and = self.masks['AND'] | self.masks['NAND']
        mask_or  = self.masks['OR'] | self.masks['NOR']
        mask_inv = self.masks['NAND'] | self.masks['NOR'] | self.masks['NOT']
        mask_xor = self.masks['XOR']
        mask_buf_not = self.masks['BUFF'] | self.masks['NOT']
        
        for _ in range(50): 
            cc0_prev, cc1_prev = cc0.clone(), cc1.clone()
            
            edge_cc0 = cc0[src_idx]
            edge_cc1 = cc1[src_idx]
            
            # Aggregate per Gate (Destination)
            min_cc0 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(
                0, dst_idx, edge_cc0, reduce='min', include_self=False)
            min_cc1 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(
                0, dst_idx, edge_cc1, reduce='min', include_self=False)
            
            sum_cc0 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc0)
            sum_cc1 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc1)
            
            # Apply Logic
            cc0[mask_and] = min_cc0[mask_and] + 1
            cc1[mask_and] = sum_cc1[mask_and] + 1
            
            cc0[mask_or] = sum_cc0[mask_or] + 1
            cc1[mask_or] = min_cc1[mask_or] + 1
            
            cc0[mask_buf_not] = min_cc0[mask_buf_not] + 1
            cc1[mask_buf_not] = min_cc1[mask_buf_not] + 1
            
            cc0[mask_xor] = torch.minimum(sum_cc0[mask_xor], sum_cc1[mask_xor]) + 1
            cc1[mask_xor] = torch.maximum(min_cc0[mask_xor], min_cc1[mask_xor]) + 1

            # Inversions
            temp_cc0 = cc0.clone()
            cc0[mask_inv] = cc1[mask_inv]
            cc1[mask_inv] = temp_cc0[mask_inv]
            
            # Reset Inputs
            mask_input = self.masks['INPUT']
            cc0[mask_input] = 1.0
            cc1[mask_input] = 1.0
            
            if torch.allclose(cc0, cc0_prev) and torch.allclose(cc1, cc1_prev):
                break

        # --- Part B: Observability (Backward) ---
        co = torch.full((num_nodes,), 1e6, device=self.device)
        
        output_indices = [self.name_to_idx[n] for n in self.parser.all_outputs if n in self.name_to_idx]
        if output_indices:
            co[torch.tensor(output_indices, device=self.device)] = 0.0

        gate_cc0_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc0[src_idx])
        gate_cc1_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc1[src_idx])
        gate_min_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(
            0, dst_idx, torch.minimum(cc0[src_idx], cc1[src_idx]))

        for _ in range(50):
            co_prev = co.clone()
            
            co_dst = co[dst_idx]
            dst_types = self.node_types[dst_idx]
            side_costs = torch.zeros_like(co_dst)
            
            # Side input logic
            is_and = (dst_types == self.TYPE_MAP['AND']) | (dst_types == self.TYPE_MAP['NAND'])
            side_costs[is_and] = gate_cc1_sum[dst_idx][is_and] - cc1[src_idx][is_and]
            
            is_or = (dst_types == self.TYPE_MAP['OR']) | (dst_types == self.TYPE_MAP['NOR'])
            side_costs[is_or] = gate_cc0_sum[dst_idx][is_or] - cc0[src_idx][is_or]
            
            is_xor = (dst_types == self.TYPE_MAP['XOR'])
            side_costs[is_xor] = gate_min_sum[dst_idx][is_xor] - torch.minimum(cc0[src_idx], cc1[src_idx])[is_xor]
            
            path_costs = co_dst + side_costs + 1
            
            new_co = torch.zeros_like(co).scatter_reduce_(
                0, src_idx, path_costs, reduce='min', include_self=False
            )
            
            co = torch.minimum(co, new_co)
            
            if torch.allclose(co, co_prev):
                break
                
        return cc0, cc1, co

    def _compute_depth_fast(self, reverse=False):
        """Vectorized Topological Depth"""
        d_vals = torch.zeros(self.num_nodes, device=self.device)
        src_idx, dst_idx = self.edge_index
        prop_src = dst_idx if reverse else src_idx
        prop_dst = src_idx if reverse else dst_idx
        
        for _ in range(50):
            changed = False
            src_depths = d_vals[prop_src]
            new_depths = torch.zeros(self.num_nodes, device=self.device).scatter_reduce_(
                0, prop_dst, src_depths, reduce='amax', include_self=True
            )
            new_depths = new_depths + 1
            if not torch.allclose(d_vals, new_depths):
                d_vals = new_depths
                changed = True
            if not changed: break
            
        max_d = d_vals.max() if d_vals.max() > 0 else 1.0
        return (d_vals / max_d).unsqueeze(1)

    def _build_base_features(self):
        """
        Builds 16-dimensional feature matrix
        [0-7]: Type, [8-9]: Depth, [10-11]: Fault, [12-14]: SCOAP, [15]: Output
        """
        x_type = torch.nn.functional.one_hot(self.node_types, num_classes=8).float()
        fwd_depth = self._compute_depth_fast(reverse=False)
        rev_depth = self._compute_depth_fast(reverse=True)
        
        f_cc0 = torch.log(self.cc0 + 1).unsqueeze(1) / 10.0
        f_cc1 = torch.log(self.cc1 + 1).unsqueeze(1) / 10.0
        f_co  = torch.log(self.co + 1).unsqueeze(1) / 10.0
        
        is_output = torch.zeros((self.num_nodes, 1), device=self.device)
        for name in self.parser.all_outputs:
            if name in self.name_to_idx:
                is_output[self.name_to_idx[name]] = 1.0
                
        zeros = torch.zeros((self.num_nodes, 2), device=self.device)
        
        return torch.cat([x_type, fwd_depth, rev_depth, zeros, f_cc0, f_cc1, f_co, is_output], dim=1)

    def get_data_for_fault(self, fault_name):
        """Generate Data object for a specific fault"""
        x = self.x_base.clone()
        tid = self.name_to_idx.get(fault_name)
        
        if tid is not None:
            x[tid, 10] = 1.0 # Mark target
            
            # BFS for Distance (Index 11)
            dist = torch.full((self.num_nodes,), -1.0, device=self.device)
            dist[tid] = 0.0
            queue = [tid]
            visited = {tid: 0}
            idx = 0
            
            while idx < len(queue):
                u = queue[idx]
                idx += 1
                d = visited[u]
                if d >= 10: continue
                
                neighbors = self.adj[u] + self.parents[u]
                for v in neighbors:
                    if v not in visited:
                        visited[v] = d + 1
                        dist[v] = d + 1
                        queue.append(v)
            
            mask_visited = (dist != -1)
            if mask_visited.any():
                max_d = dist.max()
                if max_d == 0: max_d = 1.0
                x[mask_visited, 11] = 1.0 - (dist[mask_visited] / max_d)
                
        return Data(x=x, edge_index=self.edge_index, node_names=self.ordered_names)
===== BenchParser.py =====
"""
Shared Bench File Parser for DFT Analysis

This module provides a unified parser for .bench files that:
- Handles full-scan DFFs (Q output as PPI, D input as PPO)
- Tracks back edges for bidirectional graph traversal
- Provides a common data structure for both SAT and GNN models
"""

class BenchParser:
    """
    Unified parser for .bench format files with full-scan DFF support.
    
    For full-scan designs:
    - DFF outputs (Q) are treated as Pseudo Primary Inputs (PPIs)
    - DFF inputs (D) are treated as Pseudo Primary Outputs (PPOs)
    - The circuit is "broken" at flip-flops to eliminate cycles
    """
    
    def __init__(self, bench_file):
        self.bench_file = bench_file
        
        # Primary Inputs/Outputs
        self.inputs = []           # Primary Inputs (PIs)
        self.outputs = []          # Primary Outputs (POs)
        
        # Pseudo Inputs/Outputs (from DFFs)
        self.ppis = []             # Pseudo Primary Inputs (DFF Q outputs)
        self.ppos = []             # Pseudo Primary Outputs (DFF D inputs)
        
        # All inputs/outputs combined
        self.all_inputs = []       # PIs + PPIs
        self.all_outputs = []      # POs + PPOs
        
        # Gate structure
        self.gates = []            # List of (output, gate_type, inputs)
        self.gate_dict = {}        # Map: output_name -> (gate_type, inputs)
        
        # DFF tracking
        self.dffs = []             # List of (Q_output, D_input) tuples
        self.dff_map = {}          # Map: Q_output -> D_input
        
        # Back edges (for reverse traversal)
        self.back_edges = {}       # Map: input_wire -> [gates_it_drives]
        
        # Variable mapping (for SAT solver)
        self.var_map = {}          # Map: wire_name -> variable_id
        
        # Parse the file
        self._parse()
    
    def _parse(self):
        """Parse the bench file and populate all data structures."""
        with open(self.bench_file, 'r') as f:
            for line in f:
                line = line.strip()
                
                # Skip empty lines and comments
                if not line or line.startswith('#'):
                    continue
                
                # Parse INPUT declarations
                if line.startswith('INPUT'):
                    name = line[line.find('(')+1:line.find(')')]
                    self.inputs.append(name)
                    self.all_inputs.append(name)
                    
                # Parse OUTPUT declarations
                elif line.startswith('OUTPUT'):
                    name = line[line.find('(')+1:line.find(')')]
                    self.outputs.append(name)
                    self.all_outputs.append(name)
                    
                # Parse gate definitions
                elif '=' in line:
                    parts = line.split('=')
                    out = parts[0].strip()
                    rhs = parts[1].strip()
                    
                    # Extract gate type and inputs
                    g_type = rhs[:rhs.find('(')].strip().upper()
                    in_str = rhs[rhs.find('(')+1:-1]
                    inputs = [x.strip() for x in in_str.split(',')] if in_str else []
                    
                    # Handle DFFs specially (Full-Scan assumption)
                    if g_type == 'DFF':
                        # Q output (out) becomes a PPI
                        self.ppis.append(out)
                        self.all_inputs.append(out)
                        
                        # D input becomes a PPO
                        if len(inputs) > 0:
                            d_input = inputs[0]
                            self.ppos.append(d_input)
                            self.all_outputs.append(d_input)
                            
                            # Track the DFF relationship
                            self.dffs.append((out, d_input))
                            self.dff_map[out] = d_input
                        
                        # Note: DFFs are NOT added to self.gates
                        # This "breaks" the circuit at flip-flops
                    else:
                        # Regular combinational gate
                        self.gates.append((out, g_type, inputs))
                        self.gate_dict[out] = (g_type, inputs)
                        
                        # Build back edges for reverse traversal
                        for inp in inputs:
                            if inp not in self.back_edges:
                                self.back_edges[inp] = []
                            self.back_edges[inp].append(out)
        
        # Remove duplicates while preserving order
        self.all_inputs = list(dict.fromkeys(self.all_inputs))
        self.all_outputs = list(dict.fromkeys(self.all_outputs))
    
    def get_all_wires(self):
        """Get all wire names in the circuit (inputs, outputs, and internal)."""
        wires = set(self.all_inputs + self.all_outputs)
        for out, _, inputs in self.gates:
            wires.add(out)
            wires.update(inputs)
        return sorted(list(wires))
    
    def build_var_map(self):
        """Build variable mapping for SAT solver (1-indexed)."""
        if self.var_map:
            return self.var_map  # Already built
        
        next_var = 1
        for wire in self.get_all_wires():
            if wire not in self.var_map:
                self.var_map[wire] = next_var
                next_var += 1
        return self.var_map
    
    def get_fanout(self, wire_name):
        """Get all gates driven by a wire (forward edges)."""
        return self.back_edges.get(wire_name, [])
    
    def get_fanin(self, wire_name):
        """Get the gate driving a wire (backward edge)."""
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][1]  # Return inputs
        return []
    
    def is_pi(self, wire_name):
        """Check if wire is a Primary Input."""
        return wire_name in self.inputs
    
    def is_po(self, wire_name):
        """Check if wire is a Primary Output."""
        return wire_name in self.outputs
    
    def is_ppi(self, wire_name):
        """Check if wire is a Pseudo Primary Input (DFF Q)."""
        return wire_name in self.ppis
    
    def is_ppo(self, wire_name):
        """Check if wire is a Pseudo Primary Output (DFF D)."""
        return wire_name in self.ppos
    
    def is_dff_output(self, wire_name):
        """Check if wire is a DFF Q output."""
        return wire_name in self.dff_map
    
    def get_dff_input(self, q_output):
        """Get the D input for a DFF Q output."""
        return self.dff_map.get(q_output)
    
    def get_gate_type(self, wire_name):
        """Get the gate type that produces this wire."""
        if wire_name in self.gate_dict:
            return self.gate_dict[wire_name][0]
        elif self.is_ppi(wire_name):
            return 'PPI'
        elif self.is_pi(wire_name):
            return 'INPUT'
        return None
    
    def __repr__(self):
        return (f"BenchParser({self.bench_file})\n"
                f"  PIs: {len(self.inputs)}, POs: {len(self.outputs)}\n"
                f"  PPIs: {len(self.ppis)}, PPOs: {len(self.ppos)}\n"
                f"  Gates: {len(self.gates)}, DFFs: {len(self.dffs)}")
===== WireFaultMiter.py =====
import os
import random
from collections import deque
from pysat.solvers import Glucose3, Minisat22
from BenchParser import BenchParser

class WireFaultMiter:
    def __init__(self, bench_file):
        self.bench_file = bench_file
        self.parser = BenchParser(bench_file)
        self.inputs = self.parser.all_inputs      
        self.outputs = self.parser.all_outputs    
        self.gates = self.parser.gates
        self.var_map = self.parser.build_var_map()
        self.next_var = len(self.var_map) + 1
        self.faulty_map = {}
        self.scan_inputs = self.parser.ppis
        self.scan_outputs = self.parser.ppos

    def _get_var(self, name):
        if name not in self.var_map:
            self.var_map[name] = self.next_var
            self.next_var += 1
        return self.var_map[name]

    # --- OPTIMIZATION METHODS ---

    def get_reachable_outputs(self, fault_wire):
        reachable_pos = set()
        queue = deque([fault_wire])
        visited = {fault_wire}
        while queue:
            wire = queue.popleft()
            if wire in self.outputs: reachable_pos.add(wire)
            if wire in self.parser.back_edges:
                for gate_out in self.parser.back_edges[wire]:
                    if gate_out not in visited:
                        visited.add(gate_out)
                        queue.append(gate_out)
        return list(reachable_pos)

    def get_logic_cone(self, target_outputs, fault_wire):
        relevant_gates = set()
        queue = deque(list(target_outputs) + [fault_wire])
        visited = set(list(target_outputs) + [fault_wire])
        while queue:
            wire = queue.popleft()
            if wire in self.parser.gate_dict:
                g_type, inputs = self.parser.gate_dict[wire]
                relevant_gates.add((wire, g_type, tuple(inputs)))
                for inp in inputs:
                    if inp not in visited:
                        visited.add(inp)
                        queue.append(inp)
        return sorted(list(relevant_gates), key=lambda x: x[0])

    def get_branchless_implications(self, fault_wire, target_val):
        forced = {}
        curr, val = fault_wire, target_val
        while True:
            if curr in self.inputs:
                forced[curr] = val
                break
            if curr not in self.parser.gate_dict: break
            
            g_type, inputs = self.parser.gate_dict[curr]
            
            if g_type == 'NOT': n_val = 1 - val
            elif g_type == 'BUFF': n_val = val
            elif g_type == 'AND' and val == 1: 
                for i in inputs: forced[i] = 1; 
                break
            elif g_type == 'OR' and val == 0:
                for i in inputs: forced[i] = 0
                break
            elif g_type == 'NAND' and val == 0:
                for i in inputs: forced[i] = 1
                break
            elif g_type == 'NOR' and val == 1:
                for i in inputs: forced[i] = 0
                break
            else: break
            
            forced[inputs[0]] = n_val
            curr, val = inputs[0], n_val
        return forced

    def solve_fault_specific_cones(self, fault_wire, fault_type=1, gnn_hints=None):
        """
        Solves using Split-Cone + Heuristic Sorting + Branchless + GNN Hints.
        """
        possible_outputs = self.get_reachable_outputs(fault_wire)
        if not possible_outputs: return None, 0
        
        # Sample outputs to avoid hang on large circuits
        sample_size = min(20, len(possible_outputs))
        sampled_subset = random.sample(possible_outputs, sample_size)
        remaining_subset = list(set(possible_outputs) - set(sampled_subset))
        
        candidates = []
        for po in sampled_subset:
            cone = self.get_logic_cone([po], fault_wire)
            candidates.append((len(cone), po))
        
        candidates.sort(key=lambda x: x[0])
        sorted_outputs = [x[1] for x in candidates] + remaining_subset

        orig_gates, orig_outputs = self.gates, self.outputs
        
        activation_val = 1 if fault_type == 0 else 0
        forced_map = self.get_branchless_implications(fault_wire, activation_val)
        
        final_assignment = None
        total_conflicts = 0
        
        for target_po in sorted_outputs:
            cone_gates = self.get_logic_cone([target_po], fault_wire)
            self.gates, self.outputs = cone_gates, [target_po]
            
            # 1. Reset Vars (Inputs must keep original IDs for hints)
            self.var_map = {name: i+1 for i, name in enumerate(self.parser.all_inputs)}
            self.next_var = len(self.var_map) + 1
            
            # 2. POPULATE VAR_MAP FOR INTERNAL WIRES (The Missing Fix) 
            for out, _, inputs in self.gates:
                if out not in self.var_map:
                    self.var_map[out] = self.next_var
                    self.next_var += 1
                for inp in inputs:
                    if inp not in self.var_map:
                        self.var_map[inp] = self.next_var
                        self.next_var += 1
            
            assumptions = []
            for name, val in forced_map.items():
                if name in self.var_map:
                    lit = self.var_map[name]
                    assumptions.append(lit if val == 1 else -lit)

            phases = []
            if gnn_hints:
                for name, prob in gnn_hints.items():
                    if name in self.var_map:
                        lit = self.var_map[name]
                        phases.append(lit if prob > 0.5 else -lit)

            clauses = self.build_miter(fault_wire, fault_type)
            with Minisat22(bootstrap_with=clauses) as solver:
                if phases: solver.set_phases(phases)
                
                # Timeout for bad cones
                solver.conf_budget(2000) 
                
                result = solver.solve(assumptions=assumptions)
                total_conflicts += solver.accum_stats()['conflicts']
                
                if result:
                    final_assignment = self._extract_assignment(solver.get_model())
                    break 
        
        self.gates, self.outputs = orig_gates, orig_outputs
        return final_assignment, total_conflicts

    def _extract_assignment(self, model):
        assign = {}
        model_set = set(model)
        for inp in self.inputs:
            if inp in self.var_map:
                vid = self.var_map[inp]
                if vid in model_set: assign[inp] = 1
                elif -vid in model_set: assign[inp] = 0
                else: assign[inp] = 'X'
        return assign

    # --- STANDARD BUILDER ---
    def build_miter(self, fault_wire, fault_type=None, force_diff=1):
        clauses = []
        for out, g_type, inputs in self.gates:
            self._add_gate_clauses(clauses, self.var_map[out], g_type, [self.var_map[i] for i in inputs])
            
        self.faulty_map = {name: self.var_map[name] for name in self.inputs}
        for out, _, _ in self.gates:
            if out not in self.faulty_map:
                self.faulty_map[out] = self.next_var
                self.next_var += 1
                
        if fault_wire in self.faulty_map:
            f_var = self.faulty_map[fault_wire]
            if fault_type == 1: clauses.append([f_var])
            elif fault_type == 0: clauses.append([-f_var])

        for out, g_type, inputs in self.gates:
            if out == fault_wire: continue
            out_var = self.faulty_map[out]
            in_vars = [self.faulty_map.get(i) for i in inputs]
            if None in in_vars: continue 
            self._add_gate_clauses(clauses, out_var, g_type, in_vars)

        miter_out = self.next_var; self.next_var += 1
        diff_vars = []
        unique_outputs = sorted(list(set(self.outputs)))
        
        for out in unique_outputs:
            if out not in self.var_map or out not in self.faulty_map: continue
            good = self.var_map[out]
            bad = self.faulty_map[out]
            diff = self.next_var; self.next_var += 1
            clauses.extend([[-good, -bad, -diff], [good, bad, -diff], [-good, bad, diff], [good, -bad, diff]])
            diff_vars.append(diff)
            
        big_or = [-miter_out]
        for d in diff_vars:
            clauses.append([-d, miter_out])
            big_or.append(d)
        clauses.append(big_or)
        clauses.append([miter_out]) 
        return clauses

    def _add_gate_clauses(self, clauses, out, g_type, inputs):
        if g_type == 'AND':
            for i in inputs: clauses.append([-out, i])
            clauses.append([out] + [-i for i in inputs])
        elif g_type == 'OR':
            for i in inputs: clauses.append([out, -i])
            clauses.append([-out] + inputs)
        elif g_type == 'NOT':
            clauses.append([-out, -inputs[0]])
            clauses.append([out, inputs[0]])
        elif g_type == 'NAND':
            for i in inputs: clauses.append([out, i])
            clauses.append([-out] + [-i for i in inputs])
        elif g_type == 'NOR':
            for i in inputs: clauses.append([-out, -i])
            clauses.append([out] + inputs)
        elif g_type == 'XOR':
            if len(inputs) == 2:
                a, b = inputs
                clauses.append([-out, -a, -b])
                clauses.append([-out, a, b])
                clauses.append([out, -a, b])
                clauses.append([out, a, -b])
        elif g_type == 'BUFF':
             clauses.append([-out, inputs[0]])
             clauses.append([out, -inputs[0]])
===== data_train_bench_mem_efficient.py =====
"""
Complete pipeline for Dual-Task GNN training with Parallel Generation & Polarity Guidance.
Uses Cone Splitting and Branch-less Stem Optimization for Large Circuits.
"""

import os
import sys

# Add local PySAT if needed
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'pysat'))

import time
import csv
import math
import torch
import torch.nn as nn
from torch_geometric.nn import GATv2Conv
import torch.optim as optim
import random
import numpy as np
from pysat.solvers import Glucose3, Minisat22
from pysat.formula import CNF
from tqdm import tqdm
from torch_geometric.loader import DataLoader
from torch_geometric.data import Dataset
from WireFaultMiter import WireFaultMiter
from BenchParser import BenchParser
from torch_geometric.data import Data
import torch.multiprocessing as mp

# IMPORT THE EXTRACTOR
from neuro_utils import VectorizedGraphExtractor

# =============================================================================
# CONFIGS
# =============================================================================
BENCHMARK_DIR = "../hdl-benchmarks/iscas85/bench/"
DATASET_PATH = "dataset_oracle_dual_17feat.pt"
SAMPLES_PER_FILE = 50
MODEL_PATH = "gnn_model_dual_task_17feat.pth"
EPOCHS = 20
BATCH_SIZE = 32
GENERATE_TRAIN_DATA_DIR = "../I99T"
SEED = 42

# =============================================================================
# 0. DETERMINISM SETUP
# =============================================================================
def set_global_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)

set_global_seed(SEED)

# =============================================================================
# PART 1: OPTIMIZED PARALLEL DATA GENERATION
# =============================================================================

def get_target_files(DIR):
    if not os.path.exists(DIR): return []
    file_list = []
    for root, dirs, files in os.walk(DIR):
        for f in files:
            if f.endswith(".bench"):
                full_path = os.path.join(root, f)
                rel_path = os.path.relpath(full_path, DIR)
                file_list.append(rel_path)
    return sorted(file_list, key=lambda x: os.path.getsize(os.path.join(DIR, x)), reverse=True)

def process_single_circuit(filename):
    """Worker passing fault_type=1 to extractor"""
    set_global_seed(SEED + len(filename)) 
    filepath = os.path.join(GENERATE_TRAIN_DATA_DIR, filename)
    local_dataset = []
    try:
        miter = WireFaultMiter(filepath)
        if not miter.gates: return []
        local_samples = 10 if len(miter.gates) > 5000 else SAMPLES_PER_FILE
        print(f"[{filename}] Mining {local_samples} samples (Cone Optimized)...", flush=True)
        extractor = VectorizedGraphExtractor(filepath, var_map=miter.parser.build_var_map(), device='cpu')
        
        for i in range(local_samples):
            # 1. Pick a Fault
            target_gate = random.choice(miter.gates)[0]
            outs = miter.get_reachable_outputs(target_gate)
            if not outs: continue
            
            # --- FIX FOR HANG ON B17/B19 ---
            # Don't check all outputs. Sample 20 random ones.
            sample_outs = random.sample(outs, min(20, len(outs)))
            candidates = [(len(miter.get_logic_cone([o], target_gate)), o) for o in sample_outs]
            candidates.sort()
            target_out = candidates[0][1]
            
            cone_gates = miter.get_logic_cone([target_out], target_gate)
            
            orig_gates = miter.gates; miter.gates = cone_gates
            
            # --- FAULT TYPE IS 1 (SA1) ---
            FAULT_TYPE = 1 
            clauses = miter.build_miter(target_gate, None, FAULT_TYPE) 
            miter.gates = orig_gates
            
            with Glucose3(bootstrap_with=clauses) as solver:
                solver.conf_budget(5000)
                if solver.solve():
                    model = solver.get_model()
                    cone_inputs = set([i for _,_,inps in cone_gates for i in inps if i in miter.inputs])
                    probe_list = list(cone_inputs)[:50]
                    
                    with Glucose3(bootstrap_with=clauses) as probe:
                        base = probe.accum_stats()['conflicts']
                        input_importance = {}
                        input_polarity = {} 
                        for inp in probe_list:
                            if inp not in miter.var_map: continue
                            vid = miter.var_map[inp]
                            val = vid if vid in model else -vid
                            probe.conf_budget(500)
                            res = probe.solve(assumptions=[-val])
                            new_c = probe.accum_stats()['conflicts']
                            input_importance[inp] = (new_c - base) if res else 2000
                            input_polarity[inp] = 1.0 if vid in model else 0.0
                            base = new_c
                    
                    if input_importance:
                        # --- PASS FAULT TYPE HERE ---
                        data = extractor.get_data_for_fault(target_gate, fault_type=FAULT_TYPE)
                        max_imp = max(input_importance.values())
                        y_pol = torch.zeros(len(data.node_names), 1)
                        y_imp = torch.zeros(len(data.node_names), 1)
                        mask = torch.zeros(len(data.node_names), 1)
                        for k, n in enumerate(data.node_names):
                            if n in input_importance:
                                y_pol[k] = input_polarity[n]
                                y_imp[k] = input_importance[n] / max(max_imp, 1)
                                mask[k] = 1.0
                        data.y_polarity = y_pol
                        data.y_importance = y_imp
                        data.train_mask = mask
                        local_dataset.append(data)
                        
                        # Progress print
                        if i % 2 == 0: 
                            print(f"[{filename}] {i+1}/{local_samples} done.", flush=True)

    except Exception as e:
        print(f"[{filename}] Error: {e}")
    return local_dataset

def generate_dataset():
    files = get_target_files(GENERATE_TRAIN_DATA_DIR)
    dataset = []
    try: mp.set_start_method('spawn', force=True)
    except: pass
    with mp.Pool(min(4, os.cpu_count())) as pool:
        results = list(tqdm(pool.imap_unordered(process_single_circuit, files), total=len(files)))
        for res in results: dataset.extend(res)
    torch.save(dataset, DATASET_PATH)

# =============================================================================
# PART 2 & 3: MODEL AND TRAINING
# =============================================================================

class CircuitGNN_DualTask(torch.nn.Module):
    # --- UPDATED INPUT DIM TO 17 ---
    def __init__(self, num_node_features=17, num_layers=20, hidden_dim=64, dropout=0.2):
        super(CircuitGNN_DualTask, self).__init__()
        self.convs = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()
        self.convs.append(GATv2Conv(num_node_features, hidden_dim, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        for _ in range(num_layers - 2):
            self.convs.append(GATv2Conv(hidden_dim, hidden_dim, heads=2, concat=False))
            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        self.convs.append(GATv2Conv(hidden_dim, 32, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(32))
        self.importance_head = torch.nn.Linear(32, 1)
        self.polarity_head = torch.nn.Linear(32, 1)
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = torch.relu(self.bns[0](self.convs[0](x, edge_index)))
        for i in range(1, len(self.convs) - 1):
            x = x + torch.relu(self.bns[i](self.convs[i](x, edge_index)))
        x = torch.relu(self.bns[-1](self.convs[-1](x, edge_index)))
        return self.importance_head(x), torch.sigmoid(self.polarity_head(x))

def train_model():
    print("--- Training Dual-Task GNN ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    if not os.path.exists(DATASET_PATH):
        generate_dataset()

    dataset = torch.load(DATASET_PATH, weights_only=False)
    loader = DataLoader(dataset[:int(len(dataset)*0.8)],
                        batch_size=BATCH_SIZE,
                        shuffle=True)

    model = CircuitGNN_DualTask(num_node_features=17).to(device)

    # LOAD MODEL IF IT EXISTS
    if os.path.exists(MODEL_PATH):
        print(f"Loading existing model from {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    else:
        print("No existing model found. Training from scratch.")

    optimizer = optim.Adam(model.parameters(), lr=0.001)
    crit_imp = nn.MSELoss(reduction='none')
    crit_pol = nn.BCELoss(reduction='none')

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0.0

        for batch in loader:
            batch = batch.to(device)
            optimizer.zero_grad()

            p_imp, p_pol = model(batch)
            mask = batch.train_mask
            mask_sum = mask.sum().clamp(min=1)

            l_imp = (crit_imp(p_imp, batch.y_importance) * mask).sum() / mask_sum
            l_pol = (crit_pol(p_pol, batch.y_polarity) * mask).sum() / mask_sum

            (l_imp + l_pol).backward()
            optimizer.step()
            total_loss += (l_imp + l_pol).item()

        print(f"Epoch {epoch+1}: {total_loss/len(loader):.4f}")

    torch.save(model.state_dict(), MODEL_PATH)
    print(f"Model saved to {MODEL_PATH}")


# =============================================================================
# PART 4: DETERMINISTIC BENCHMARKING (FIXED WITH OPTIMIZATION)
# =============================================================================

def run_benchmark():
    print(f"--- BENCHMARKING (Cone Split + Branchless + GNN) ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # --- UPDATE MODEL CALL TO 17 ---
    model = CircuitGNN_DualTask(num_node_features=17).to(device)
    if os.path.exists(MODEL_PATH): model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    
    results = []
    files = get_target_files(BENCHMARK_DIR)
    random.seed(SEED) 
    
    for filename in files:
        filepath = os.path.join(BENCHMARK_DIR, filename)
        print(f"\nProcessing {filename}...")
        try:
            miter = WireFaultMiter(filepath)
            if not miter.gates: continue
            extractor = VectorizedGraphExtractor(filepath, var_map=miter.var_map, device=device.type)
            
            all_gates = sorted(miter.gates, key=lambda x: x[0])
            for i in range(10): 
                target_gate = random.choice(all_gates)[0]
                FAULT_TYPE = 1 # SA1
                
                # --- GNN INFERENCE ---
                t0 = time.time()
                # Pass fault type 1
                data = extractor.get_data_for_fault(target_gate, fault_type=FAULT_TYPE).to(device)
                with torch.no_grad(): imp, pol = model(data)
                
                hints = {}
                for idx, name in enumerate(data.node_names):
                    if name in miter.inputs: hints[name] = pol[idx].item()
                
                # --- 1. BASELINE RUN (No Hints) ---
                _, conf_std = miter.solve_fault_specific_cones(target_gate, FAULT_TYPE, gnn_hints=None)
                
                # --- 2. GNN RUN (With Hints) ---
                assign_gnn, conf_gnn = miter.solve_fault_specific_cones(target_gate, FAULT_TYPE, gnn_hints=hints)
                dur = time.time() - t0
                
                spd = conf_std / max(conf_gnn, 1)
                status = "DETECTED" if assign_gnn else "UNSAT"
                
                if assign_gnn:
                    vec_str = "".join([str(assign_gnn.get(k, 'X')) for k in sorted(miter.inputs)])
                    print(f"  {target_gate}: {status} | Conf: {conf_std} -> {conf_gnn} ({spd:.2f}x)")
                    print(f"  Vector: {vec_str[:30]}...")
                else:
                    vec_str = "UNSAT"
                    print(f"  {target_gate}: {status} | Conf: {conf_std} -> {conf_gnn} ({spd:.2f}x)")
                
                results.append({
                    "Circuit": filename, "Fault": target_gate, "Speedup": spd,
                    "Std_Conf": conf_std, "GNN_Conf": conf_gnn, "Status": status,
                    "Vector": vec_str
                })
        except Exception as e:
            print(f"Error: {e}")

    if results:
        with open("results_optimized.csv", 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)

if __name__ == "__main__":
    if len(sys.argv) < 2: print("Usage: python data_train_bench_mem_efficient.py [generate|train|benchmark]")
    else:
        cmd = sys.argv[1]
        if cmd == "generate": generate_dataset()
        elif cmd == "train": train_model()
        elif cmd == "benchmark": run_benchmark()
===== neuro_utils.py =====
import torch
import math
from torch_geometric.data import Data
from BenchParser import BenchParser

class VectorizedGraphExtractor:
    """
    High-Performance SCOAP Extractor using Vectorized Tensor Operations.
    Generates 17-dimensional feature vectors (16 Base + 1 Target Value).
    """
    
    TYPE_MAP = {
        'INPUT': 0, 'PPI': 0, 
        'BUFF': 1, 'NOT': 2,
        'AND': 3, 'NAND': 4,
        'OR': 5, 'NOR': 6,
        'XOR': 7, 'XNOR': 7
    }

    def __init__(self, bench_path, var_map=None, device='cpu'):
        self.parser = BenchParser(bench_path)
        self.device = device
        
        if var_map:
            self.var_map = var_map
        else:
            self.var_map = self.parser.build_var_map()
            
        self.ordered_names = sorted(self.var_map.keys(), key=lambda k: self.var_map[k])
        self.name_to_idx = {name: i for i, name in enumerate(self.ordered_names)}
        self.num_nodes = len(self.ordered_names)
        
        # Structure
        self.edges_list = []
        self.node_types = torch.zeros(self.num_nodes, dtype=torch.long, device=device)
        
        for name, g_type, _ in self.parser.gates:
            if name in self.name_to_idx:
                self.node_types[self.name_to_idx[name]] = self.TYPE_MAP.get(g_type, 1)
        for pi in self.parser.inputs:
            if pi in self.name_to_idx: self.node_types[self.name_to_idx[pi]] = self.TYPE_MAP['INPUT']
        for ppi in self.parser.ppis:
            if ppi in self.name_to_idx: self.node_types[self.name_to_idx[ppi]] = self.TYPE_MAP['INPUT']
        
        for out, _, inputs in self.parser.gates:
            if out in self.name_to_idx:
                dst = self.name_to_idx[out]
                for inp in inputs:
                    if inp in self.name_to_idx:
                        src = self.name_to_idx[inp]
                        self.edges_list.append([src, dst])
        
        if self.edges_list:
            self.edge_index = torch.tensor(self.edges_list, dtype=torch.long, device=device).t().contiguous()
        else:
            self.edge_index = torch.zeros((2, 0), dtype=torch.long, device=device)
            
        self.masks = {}
        for t_name, t_id in self.TYPE_MAP.items():
            self.masks[t_name] = (self.node_types == t_id)

        self.adj = [[] for _ in range(self.num_nodes)]
        self.parents = [[] for _ in range(self.num_nodes)]
        for src, dst in self.edges_list:
            self.adj[src].append(dst)
            self.parents[dst].append(src)
            
        self.cc0, self.cc1, self.co = self._compute_scoap_vectorized()
        self.x_base = self._build_base_features()

    def _compute_scoap_vectorized(self):
        num_nodes = self.num_nodes
        src_idx, dst_idx = self.edge_index
        
        cc0 = torch.ones(num_nodes, device=self.device)
        cc1 = torch.ones(num_nodes, device=self.device)
        
        mask_and = self.masks['AND'] | self.masks['NAND']
        mask_or  = self.masks['OR'] | self.masks['NOR']
        mask_inv = self.masks['NAND'] | self.masks['NOR'] | self.masks['NOT']
        mask_xor = self.masks['XOR']
        mask_buf_not = self.masks['BUFF'] | self.masks['NOT']
        
        for _ in range(50): 
            cc0_prev, cc1_prev = cc0.clone(), cc1.clone()
            edge_cc0 = cc0[src_idx]; edge_cc1 = cc1[src_idx]
            min_cc0 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(0, dst_idx, edge_cc0, reduce='min', include_self=False)
            min_cc1 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(0, dst_idx, edge_cc1, reduce='min', include_self=False)
            sum_cc0 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc0)
            sum_cc1 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc1)
            
            cc0[mask_and] = min_cc0[mask_and] + 1; cc1[mask_and] = sum_cc1[mask_and] + 1
            cc0[mask_or] = sum_cc0[mask_or] + 1; cc1[mask_or] = min_cc1[mask_or] + 1
            cc0[mask_buf_not] = min_cc0[mask_buf_not] + 1; cc1[mask_buf_not] = min_cc1[mask_buf_not] + 1
            cc0[mask_xor] = torch.minimum(sum_cc0[mask_xor], sum_cc1[mask_xor]) + 1
            cc1[mask_xor] = torch.maximum(min_cc0[mask_xor], min_cc1[mask_xor]) + 1
            
            temp_cc0 = cc0.clone()
            cc0[mask_inv] = cc1[mask_inv]; cc1[mask_inv] = temp_cc0[mask_inv]
            mask_input = self.masks['INPUT']; cc0[mask_input] = 1.0; cc1[mask_input] = 1.0
            if torch.allclose(cc0, cc0_prev) and torch.allclose(cc1, cc1_prev): break

        co = torch.full((num_nodes,), 1e6, device=self.device)
        output_indices = [self.name_to_idx[n] for n in self.parser.all_outputs if n in self.name_to_idx]
        if output_indices: co[torch.tensor(output_indices, device=self.device)] = 0.0
        
        gate_cc0_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc0[src_idx])
        gate_cc1_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc1[src_idx])
        gate_min_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, torch.minimum(cc0[src_idx], cc1[src_idx]))

        for _ in range(50):
            co_prev = co.clone()
            co_dst = co[dst_idx]
            dst_types = self.node_types[dst_idx]
            side_costs = torch.zeros_like(co_dst)
            
            is_and = (dst_types == self.TYPE_MAP['AND']) | (dst_types == self.TYPE_MAP['NAND'])
            side_costs[is_and] = gate_cc1_sum[dst_idx][is_and] - cc1[src_idx][is_and]
            is_or = (dst_types == self.TYPE_MAP['OR']) | (dst_types == self.TYPE_MAP['NOR'])
            side_costs[is_or] = gate_cc0_sum[dst_idx][is_or] - cc0[src_idx][is_or]
            is_xor = (dst_types == self.TYPE_MAP['XOR'])
            side_costs[is_xor] = gate_min_sum[dst_idx][is_xor] - torch.minimum(cc0[src_idx], cc1[src_idx])[is_xor]
            
            path_costs = co_dst + side_costs + 1
            new_co = torch.zeros_like(co).scatter_reduce_(0, src_idx, path_costs, reduce='min', include_self=False)
            co = torch.minimum(co, new_co)
            if torch.allclose(co, co_prev): break
            
        return cc0, cc1, co

    def _compute_depth_fast(self, reverse=False):
        d_vals = torch.zeros(self.num_nodes, device=self.device)
        src_idx, dst_idx = self.edge_index
        prop_src = dst_idx if reverse else src_idx
        prop_dst = src_idx if reverse else dst_idx
        
        for _ in range(50):
            changed = False
            src_depths = d_vals[prop_src]
            new_depths = torch.zeros(self.num_nodes, device=self.device).scatter_reduce_(
                0, prop_dst, src_depths, reduce='amax', include_self=True)
            new_depths = new_depths + 1
            if not torch.allclose(d_vals, new_depths): d_vals = new_depths; changed = True
            if not changed: break
        max_d = d_vals.max() if d_vals.max() > 0 else 1.0
        return (d_vals / max_d).unsqueeze(1)

    def _build_base_features(self):
        """Builds 16 Base Features"""
        x_type = torch.nn.functional.one_hot(self.node_types, num_classes=8).float()
        fwd_depth = self._compute_depth_fast(reverse=False)
        rev_depth = self._compute_depth_fast(reverse=True)
        
        f_cc0 = torch.log(self.cc0 + 1).unsqueeze(1) / 10.0
        f_cc1 = torch.log(self.cc1 + 1).unsqueeze(1) / 10.0
        f_co  = torch.log(self.co + 1).unsqueeze(1) / 10.0
        
        is_output = torch.zeros((self.num_nodes, 1), device=self.device)
        for name in self.parser.all_outputs:
            if name in self.name_to_idx: is_output[self.name_to_idx[name]] = 1.0
        
        zeros = torch.zeros((self.num_nodes, 2), device=self.device)
        
        return torch.cat([x_type, fwd_depth, rev_depth, zeros, f_cc0, f_cc1, f_co, is_output], dim=1)

    def get_data_for_fault(self, fault_name, fault_type=1):
        """
        Generate Data object with 17 features.
        fault_type: 1 for SA1 (Target=0), 0 for SA0 (Target=1).
        """
        x = self.x_base.clone()
        tid = self.name_to_idx.get(fault_name)
        
        # 17th Feature: Target Value
        target_feat = torch.full((self.num_nodes, 1), 0.5, device=self.device)
        
        if tid is not None:
            x[tid, 10] = 1.0 # Fault Location
            target_feat[tid] = 0.0 if fault_type == 1 else 1.0 # Target Value
            
            # BFS Distance
            dist = torch.full((self.num_nodes,), -1.0, device=self.device)
            dist[tid] = 0.0
            queue = [tid]; visited = {tid: 0}; idx = 0
            while idx < len(queue):
                u = queue[idx]; idx += 1
                d = visited[u]
                if d >= 10: continue
                neighbors = self.adj[u] + self.parents[u]
                for v in neighbors:
                    if v not in visited: visited[v] = d + 1; dist[v] = d + 1; queue.append(v)
            
            mask_visited = (dist != -1)
            if mask_visited.any():
                max_d = dist.max()
                if max_d == 0: max_d = 1.0
                x[mask_visited, 11] = 1.0 - (dist[mask_visited] / max_d)
        
        x = torch.cat([x, target_feat], dim=1)
        return Data(x=x, edge_index=self.edge_index, node_names=self.ordered_names)
===== pipeline.py =====
"""
Complete pipeline for Dual-Task GNN training with Parallel Generation & Polarity Guidance.
Uses Cone Splitting and Branch-less Stem Optimization for Large Circuits.
"""

import os
import sys

# Add local PySAT if needed
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'pysat'))

import time
import csv
import math
import torch
import torch.nn as nn
from torch_geometric.nn import GATv2Conv
import torch.optim as optim
import random
import numpy as np
from pysat.solvers import Glucose3, Minisat22
from pysat.formula import CNF
from tqdm import tqdm
from torch_geometric.loader import DataLoader
from torch_geometric.data import Dataset
from WireFaultMiter import WireFaultMiter
from BenchParser import BenchParser
from torch_geometric.data import Data
import torch.multiprocessing as mp

# =============================================================================
# EXTRACTOR (Defined here to ensure completeness)
# =============================================================================
class VectorizedGraphExtractor:
    """
    High-Performance SCOAP Extractor using Vectorized Tensor Operations.
    Generates 16-dimensional feature vectors including Observability.
    """
    
    # Gate Type Mapping
    TYPE_MAP = {
        'INPUT': 0, 'PPI': 0, 
        'BUFF': 1, 'NOT': 2,
        'AND': 3, 'NAND': 4,
        'OR': 5, 'NOR': 6,
        'XOR': 7, 'XNOR': 7
    }

    def __init__(self, bench_path, var_map=None, device='cpu'):
        self.parser = BenchParser(bench_path)
        self.device = device
        
        # 1. Build Name Mappings (Sync with Miter if var_map provided)
        if var_map:
            self.var_map = var_map
        else:
            self.var_map = self.parser.build_var_map()
            
        self.ordered_names = sorted(self.var_map.keys(), key=lambda k: self.var_map[k])
        self.name_to_idx = {name: i for i, name in enumerate(self.ordered_names)}
        self.num_nodes = len(self.ordered_names)
        
        # 2. Build Structural Tensors
        self.edges_list = []
        self.node_types = torch.zeros(self.num_nodes, dtype=torch.long, device=device)
        
        # Assign Gate Types
        for name, g_type, _ in self.parser.gates:
            if name in self.name_to_idx:
                idx = self.name_to_idx[name]
                self.node_types[idx] = self.TYPE_MAP.get(g_type, 1) # Default to BUFF
        
        # Overwrite Types for Inputs/PPIs
        for pi in self.parser.inputs:
            if pi in self.name_to_idx:
                self.node_types[self.name_to_idx[pi]] = self.TYPE_MAP['INPUT']
        for ppi in self.parser.ppis:
            if ppi in self.name_to_idx:
                self.node_types[self.name_to_idx[ppi]] = self.TYPE_MAP['INPUT']
        
        # Build Edge List (Source -> Dest)
        for out, _, inputs in self.parser.gates:
            if out in self.name_to_idx:
                dst = self.name_to_idx[out]
                for inp in inputs:
                    if inp in self.name_to_idx:
                        src = self.name_to_idx[inp]
                        self.edges_list.append([src, dst])
        
        # Create Edge Index Tensor
        if self.edges_list:
            self.edge_index = torch.tensor(self.edges_list, dtype=torch.long, device=device).t().contiguous()
        else:
            self.edge_index = torch.zeros((2, 0), dtype=torch.long, device=device)
            
        # Create Boolean Masks for Vectorized Logic
        self.masks = {}
        for t_name, t_id in self.TYPE_MAP.items():
            self.masks[t_name] = (self.node_types == t_id)

        # Pre-build Python adjacency for BFS traversals (Distance calculation)
        self.adj = [[] for _ in range(self.num_nodes)]       # Forward: src -> [dst]
        self.parents = [[] for _ in range(self.num_nodes)]  # Backward: dst -> [src]
        
        for src, dst in self.edges_list:
            self.adj[src].append(dst)
            self.parents[dst].append(src)
            
        # 3. Compute Metrics Immediately
        self.cc0, self.cc1, self.co = self._compute_scoap_vectorized()
        self.x_base = self._build_base_features()

    def _compute_scoap_vectorized(self):
        """Vectorized SCOAP: Forward Controllability & Backward Observability"""
        num_nodes = self.num_nodes
        src_idx, dst_idx = self.edge_index
        
        # --- Part A: Controllability (Forward) ---
        cc0 = torch.ones(num_nodes, device=self.device)
        cc1 = torch.ones(num_nodes, device=self.device)
        
        mask_and = self.masks['AND'] | self.masks['NAND']
        mask_or  = self.masks['OR'] | self.masks['NOR']
        mask_inv = self.masks['NAND'] | self.masks['NOR'] | self.masks['NOT']
        mask_xor = self.masks['XOR']
        mask_buf_not = self.masks['BUFF'] | self.masks['NOT']
        
        for _ in range(50): 
            cc0_prev, cc1_prev = cc0.clone(), cc1.clone()
            
            edge_cc0 = cc0[src_idx]
            edge_cc1 = cc1[src_idx]
            
            # Aggregate per Gate (Destination)
            min_cc0 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(
                0, dst_idx, edge_cc0, reduce='min', include_self=False)
            min_cc1 = torch.zeros(num_nodes, device=self.device).scatter_reduce_(
                0, dst_idx, edge_cc1, reduce='min', include_self=False)
            
            sum_cc0 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc0)
            sum_cc1 = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, edge_cc1)
            
            # Apply Logic
            cc0[mask_and] = min_cc0[mask_and] + 1
            cc1[mask_and] = sum_cc1[mask_and] + 1
            
            cc0[mask_or] = sum_cc0[mask_or] + 1
            cc1[mask_or] = min_cc1[mask_or] + 1
            
            cc0[mask_buf_not] = min_cc0[mask_buf_not] + 1
            cc1[mask_buf_not] = min_cc1[mask_buf_not] + 1
            
            cc0[mask_xor] = torch.minimum(sum_cc0[mask_xor], sum_cc1[mask_xor]) + 1
            cc1[mask_xor] = torch.maximum(min_cc0[mask_xor], min_cc1[mask_xor]) + 1

            # Inversions
            temp_cc0 = cc0.clone()
            cc0[mask_inv] = cc1[mask_inv]
            cc1[mask_inv] = temp_cc0[mask_inv]
            
            # Reset Inputs
            mask_input = self.masks['INPUT']
            cc0[mask_input] = 1.0
            cc1[mask_input] = 1.0
            
            if torch.allclose(cc0, cc0_prev) and torch.allclose(cc1, cc1_prev):
                break

        # --- Part B: Observability (Backward) ---
        co = torch.full((num_nodes,), 1e6, device=self.device)
        
        output_indices = [self.name_to_idx[n] for n in self.parser.all_outputs if n in self.name_to_idx]
        if output_indices:
            co[torch.tensor(output_indices, device=self.device)] = 0.0

        gate_cc0_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc0[src_idx])
        gate_cc1_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(0, dst_idx, cc1[src_idx])
        gate_min_sum = torch.zeros(num_nodes, device=self.device).scatter_add_(
            0, dst_idx, torch.minimum(cc0[src_idx], cc1[src_idx]))

        for _ in range(50):
            co_prev = co.clone()
            
            co_dst = co[dst_idx]
            dst_types = self.node_types[dst_idx]
            side_costs = torch.zeros_like(co_dst)
            
            # Side input logic
            is_and = (dst_types == self.TYPE_MAP['AND']) | (dst_types == self.TYPE_MAP['NAND'])
            side_costs[is_and] = gate_cc1_sum[dst_idx][is_and] - cc1[src_idx][is_and]
            
            is_or = (dst_types == self.TYPE_MAP['OR']) | (dst_types == self.TYPE_MAP['NOR'])
            side_costs[is_or] = gate_cc0_sum[dst_idx][is_or] - cc0[src_idx][is_or]
            
            is_xor = (dst_types == self.TYPE_MAP['XOR'])
            side_costs[is_xor] = gate_min_sum[dst_idx][is_xor] - torch.minimum(cc0[src_idx], cc1[src_idx])[is_xor]
            
            path_costs = co_dst + side_costs + 1
            
            new_co = torch.zeros_like(co).scatter_reduce_(
                0, src_idx, path_costs, reduce='min', include_self=False
            )
            
            co = torch.minimum(co, new_co)
            
            if torch.allclose(co, co_prev):
                break
                
        return cc0, cc1, co

    def _compute_depth_fast(self, reverse=False):
        """Vectorized Topological Depth"""
        d_vals = torch.zeros(self.num_nodes, device=self.device)
        src_idx, dst_idx = self.edge_index
        prop_src = dst_idx if reverse else src_idx
        prop_dst = src_idx if reverse else dst_idx
        
        for _ in range(50):
            changed = False
            src_depths = d_vals[prop_src]
            new_depths = torch.zeros(self.num_nodes, device=self.device).scatter_reduce_(
                0, prop_dst, src_depths, reduce='amax', include_self=True
            )
            new_depths = new_depths + 1
            if not torch.allclose(d_vals, new_depths):
                d_vals = new_depths
                changed = True
            if not changed: break
            
        max_d = d_vals.max() if d_vals.max() > 0 else 1.0
        return (d_vals / max_d).unsqueeze(1)

    def _build_base_features(self):
        """
        Builds 16-dimensional feature matrix
        [0-7]: Type, [8-9]: Depth, [10-11]: Fault, [12-14]: SCOAP, [15]: Output
        """
        x_type = torch.nn.functional.one_hot(self.node_types, num_classes=8).float()
        fwd_depth = self._compute_depth_fast(reverse=False)
        rev_depth = self._compute_depth_fast(reverse=True)
        
        f_cc0 = torch.log(self.cc0 + 1).unsqueeze(1) / 10.0
        f_cc1 = torch.log(self.cc1 + 1).unsqueeze(1) / 10.0
        f_co  = torch.log(self.co + 1).unsqueeze(1) / 10.0
        
        is_output = torch.zeros((self.num_nodes, 1), device=self.device)
        for name in self.parser.all_outputs:
            if name in self.name_to_idx:
                is_output[self.name_to_idx[name]] = 1.0
                
        zeros = torch.zeros((self.num_nodes, 2), device=self.device)
        
        return torch.cat([x_type, fwd_depth, rev_depth, zeros, f_cc0, f_cc1, f_co, is_output], dim=1)

    def get_data_for_fault(self, fault_name):
        """Generate Data object for a specific fault"""
        x = self.x_base.clone()
        tid = self.name_to_idx.get(fault_name)
        
        if tid is not None:
            x[tid, 10] = 1.0 # Mark target
            
            # BFS for Distance (Index 11)
            dist = torch.full((self.num_nodes,), -1.0, device=self.device)
            dist[tid] = 0.0
            queue = [tid]
            visited = {tid: 0}
            idx = 0
            
            while idx < len(queue):
                u = queue[idx]
                idx += 1
                d = visited[u]
                if d >= 10: continue
                
                neighbors = self.adj[u] + self.parents[u]
                for v in neighbors:
                    if v not in visited:
                        visited[v] = d + 1
                        dist[v] = d + 1
                        queue.append(v)
            
            mask_visited = (dist != -1)
            if mask_visited.any():
                max_d = dist.max()
                if max_d == 0: max_d = 1.0
                x[mask_visited, 11] = 1.0 - (dist[mask_visited] / max_d)
                
        return Data(x=x, edge_index=self.edge_index, node_names=self.ordered_names)

# =============================================================================
# CONFIGS
# =============================================================================
BENCHMARK_DIR = "../hdl-benchmarks/iscas85/bench/"
DATASET_PATH = "dataset_oracle_dual_16feat.pt"
SAMPLES_PER_FILE = 50
MODEL_PATH = "gnn_model_dual_task_16feat.pth"
EPOCHS = 20
BATCH_SIZE = 32
GENERATE_TRAIN_DATA_DIR = "../I99T"
SEED = 42

# =============================================================================
# 0. DETERMINISM SETUP
# =============================================================================
def set_global_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)

set_global_seed(SEED)

# =============================================================================
# PART 1: OPTIMIZED PARALLEL DATA GENERATION (CONE BASED)
# =============================================================================

def get_target_files(DIR):
    if not os.path.exists(DIR): return []
    file_list = []
    for root, dirs, files in os.walk(DIR):
        for f in files:
            if f.endswith(".bench"):
                full_path = os.path.join(root, f)
                rel_path = os.path.relpath(full_path, DIR)
                file_list.append(rel_path)
    # Sort largest first for better scheduling
    return sorted(file_list, key=lambda x: os.path.getsize(os.path.join(DIR, x)), reverse=True)

def process_single_circuit(filename):
    """Worker using CONE SPLITTING to handle Large Circuits."""
    set_global_seed(SEED + len(filename)) 
    filepath = os.path.join(GENERATE_TRAIN_DATA_DIR, filename)
    local_dataset = []

    try:
        miter = WireFaultMiter(filepath)
        if not miter.gates: return []
        
        # Adjust samples based on size (Giant circuits get fewer samples, but higher quality)
        num_gates = len(miter.gates)
        local_samples = 10 if num_gates > 5000 else SAMPLES_PER_FILE
        
        extractor = VectorizedGraphExtractor(filepath, var_map=miter.parser.build_var_map(), device='cpu')
        
        print(f"[{filename}] Mining {local_samples} samples (Cone Optimized)...", flush=True)

        for i in range(local_samples):
            # 1. Pick a Fault
            target_gate = random.choice(miter.gates)[0]
            
            # 2. Extract a Valid Cone (Instead of solving whole circuit)
            outs = miter.get_reachable_outputs(target_gate)
            if not outs: continue
            
            target_out = random.choice(outs)
            cone_gates = miter.get_logic_cone([target_out], target_gate)
            
            # 3. Swap Gates & Build Miter
            orig_gates = miter.gates
            miter.gates = cone_gates
            
            clauses = miter.build_miter(target_gate, None, 1) # SA1
            miter.gates = orig_gates # Restore immediately
            
            with Glucose3(bootstrap_with=clauses) as solver:
                solver.conf_budget(5000)
                if not solver.solve(): continue
                
                model = solver.get_model()
                if not model: continue
                
                # 4. Probe Inputs (Only those in the Cone!)
                cone_inputs = set()
                for _, _, inps in cone_gates:
                    for inp in inps:
                        if inp in miter.inputs: cone_inputs.add(inp)
                
                probe_list = list(cone_inputs)
                if len(probe_list) > 50: probe_list = random.sample(probe_list, 50)
                
                with Glucose3(bootstrap_with=clauses) as probe:
                    base_conf = probe.accum_stats()['conflicts']
                    input_importance = {}
                    input_polarity = {}
                    
                    for input_name in probe_list:
                        if input_name not in miter.var_map: continue
                        var_id = miter.var_map[input_name]
                        
                        correct = var_id if var_id in model else -var_id
                        probe.conf_budget(500)
                        res = probe.solve(assumptions=[-correct])
                        new_conf = probe.accum_stats()['conflicts']
                        
                        imp = new_conf - base_conf
                        base_conf = new_conf
                        
                        input_importance[input_name] = imp if res else 2000
                        input_polarity[input_name] = 1.0 if var_id in model else 0.0

                    # 5. Build Data Object
                    if input_importance:
                        data = extractor.get_data_for_fault(target_gate)
                        max_imp = max(input_importance.values())
                        
                        y_pol = torch.zeros(len(data.node_names), 1)
                        y_imp = torch.zeros(len(data.node_names), 1)
                        mask = torch.zeros(len(data.node_names), 1)
                        
                        for k, node in enumerate(data.node_names):
                            if node in input_importance:
                                y_pol[k] = input_polarity[node]
                                y_imp[k] = input_importance[node] / max(max_imp, 1)
                                mask[k] = 1.0
                        
                        data.y_polarity = y_pol
                        data.y_importance = y_imp
                        data.train_mask = mask
                        local_dataset.append(data)

    except Exception as e:
        print(f"[{filename}] Error: {e}", flush=True)
        return []

    return local_dataset

def generate_dataset():
    print(f"--- MINING DATA (CONE OPTIMIZED) ---")
    if not os.path.exists(GENERATE_TRAIN_DATA_DIR): return

    files = get_target_files(GENERATE_TRAIN_DATA_DIR)
    dataset = []
    
    try: mp.set_start_method('spawn', force=True)
    except: pass

    with mp.Pool(min(4, os.cpu_count())) as pool:
        results = list(tqdm(pool.imap_unordered(process_single_circuit, files), total=len(files)))
        for res in results:
            dataset.extend(res)

    torch.save(dataset, DATASET_PATH)


# =============================================================================
# PART 2 & 3: MODEL AND TRAINING (UNCHANGED)
# =============================================================================

class CircuitGNN_DualTask(torch.nn.Module):
    def __init__(self, num_node_features=16, num_layers=20, hidden_dim=64, dropout=0.2):
        super(CircuitGNN_DualTask, self).__init__()
        self.dropout = dropout
        self.num_layers = num_layers
        self.convs = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()
        self.convs.append(GATv2Conv(num_node_features, hidden_dim, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        for _ in range(num_layers - 2):
            self.convs.append(GATv2Conv(hidden_dim, hidden_dim, heads=2, concat=False))
            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))
        self.convs.append(GATv2Conv(hidden_dim, 32, heads=2, concat=False))
        self.bns.append(torch.nn.BatchNorm1d(32))
        self.importance_head = torch.nn.Linear(32, 1)
        self.polarity_head = torch.nn.Linear(32, 1)
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.convs[0](x, edge_index)
        x = self.bns[0](x)
        x = torch.nn.functional.elu(x)
        for i in range(1, self.num_layers - 1):
            identity = x
            x = self.convs[i](x, edge_index)
            x = self.bns[i](x)
            x = torch.nn.functional.elu(x)
            x = x + identity
        x = self.convs[-1](x, edge_index)
        x = self.bns[-1](x)
        x = torch.nn.functional.elu(x)
        return self.importance_head(x), torch.sigmoid(self.polarity_head(x))

def train_model():
    print("--- Training Dual-Task GNN ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if not os.path.exists(DATASET_PATH): 
        print("Dataset not found. Generating...")
        generate_dataset()
    
    dataset = torch.load(DATASET_PATH, weights_only=False)
    train_loader = DataLoader(dataset[:int(len(dataset)*0.8)], batch_size=BATCH_SIZE, shuffle=True)
    
    model = CircuitGNN_DualTask(num_node_features=16).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    crit_imp = nn.MSELoss(reduction='none')
    crit_pol = nn.BCELoss(reduction='none')
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            p_imp, p_pol = model(batch)
            mask = batch.train_mask
            mask_sum = mask.sum().clamp(min=1)
            l_imp = (crit_imp(p_imp, batch.y_importance) * mask).sum() / mask_sum
            l_pol = (crit_pol(p_pol, batch.y_polarity) * mask).sum() / mask_sum
            (l_imp + l_pol).backward()
            optimizer.step()
            total_loss += (l_imp + l_pol).item()
        print(f"Epoch {epoch+1}/{EPOCHS}: Loss={total_loss/len(train_loader):.4f}")
    
    torch.save(model.state_dict(), MODEL_PATH)


# =============================================================================
# PART 4: DETERMINISTIC BENCHMARKING (OPTIMIZED)
# =============================================================================

def run_benchmark():
    print(f"--- BENCHMARKING (GNN + CONE SPLIT + BRANCHLESS) ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model = CircuitGNN_DualTask(num_node_features=16).to(device)
    if not os.path.exists(MODEL_PATH):
        print("Train model first.")
        return
        
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    
    results = []
    files = get_target_files(BENCHMARK_DIR)
    
    random.seed(SEED) 
    
    for filename in files:
        filepath = os.path.join(BENCHMARK_DIR, filename)
        print(f"\nProcessing {filename}...")
        
        try:
            miter = WireFaultMiter(filepath)
            if not miter.gates: continue
            
            extractor = VectorizedGraphExtractor(filepath, var_map=miter.var_map, device=device.type)
            all_gates = sorted(miter.gates, key=lambda x: x[0])
            
            # Run 10 faults
            for i in range(10): 
                target_gate = random.choice(all_gates)[0]
                
                # 1. Inference
                t_gnn_start = time.time()
                data = extractor.get_data_for_fault(target_gate)
                data = data.to(device)
                
                with torch.no_grad():
                    imp_scores, pol_scores = model(data)
                
                # 2. Extract Hints
                hints = []
                for idx, name in enumerate(data.node_names):
                    if name in miter.inputs:
                        imp = imp_scores[idx].item()
                        prob = pol_scores[idx].item()
                        var_id = miter.var_map.get(name)
                        if var_id:
                            val = 1 if prob > 0.5 else 0
                            # Map 0 -> -Lit, 1 -> Lit
                            hints.append((var_id if val else -var_id, imp))
                            
                hints.sort(key=lambda x: -x[1])
                hint_lits = [h[0] for h in hints]
                
                # 3. Solve using Optimized Cone Strategy with Hints
                assignment = miter.solve_fault_specific_cones(target_gate, gnn_hints=hint_lits)
                
                dur = time.time() - t_gnn_start
                status = "DETECTED" if assignment else "UNSAT/UNDETECTED"
                print(f"  Fault {target_gate}: {status} in {dur:.4f}s")
                if assignment:
                    # Print Compact Vector
                    vec = "".join([str(assignment.get(k, 'X')) for k in sorted(miter.inputs)])
                    print(f"  Vector: {vec[:50]}...")
                
                results.append({
                    "Circuit": filename,
                    "Fault": target_gate,
                    "Status": status,
                    "Time": dur
                })
                
        except Exception as e:
            print(f"Error: {e}")

    if results:
        with open("results_optimized.csv", 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)
        print("Saved to results_optimized.csv")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python data_train_bench_mem_efficient.py [generate|train|benchmark]")
    else:
        cmd = sys.argv[1]
        if cmd == "generate": generate_dataset()
        elif cmd == "train": train_model()
        elif cmd == "benchmark": run_benchmark()
